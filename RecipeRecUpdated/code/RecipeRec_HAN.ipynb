{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fiCm-HpOSauP"
      },
      "outputs": [],
      "source": [
        "!cp -r /content/drive/MyDrive/data_mlp_project/ /content/data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2icc72sVqJb",
        "outputId": "83b2211d-d73d-4e2e-869a-96d83ee1857b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Mar 25 14:11:28 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   44C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install torch 2.4.0 first\n",
        "!pip install torch==2.4.0\n",
        "\n",
        "# Install torchvision compatible with torch 2.4.0\n",
        "!pip install torchvision==0.15.0\n",
        "\n",
        "# Install torchaudio compatible with torch 2.4.0\n",
        "!pip install torchaudio==2.4.0\n",
        "\n",
        "# Now install dgl\n",
        "!pip install  dgl -f https://data.dgl.ai/wheels/torch-2.4/cu124/repo.html"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ER8rb5fPBpOY",
        "outputId": "f3180785-c3e7-4b76-82c6-729aae7576b9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==2.4.0\n",
            "  Downloading torch-2.4.0-cp311-cp311-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.4.0)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.4.0)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.4.0)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.4.0)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.4.0)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.4.0)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.4.0)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.4.0)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.4.0)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch==2.4.0)\n",
            "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.4.0)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==3.0.0 (from torch==2.4.0)\n",
            "  Downloading triton-3.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.4.0) (12.5.82)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.4.0) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.4.0) (1.3.0)\n",
            "Downloading torch-2.4.0-cp311-cp311-manylinux1_x86_64.whl (797.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.3/797.3 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m82.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m71.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.2.0\n",
            "    Uninstalling triton-3.2.0:\n",
            "      Successfully uninstalled triton-3.2.0\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.4.127\n",
            "    Uninstalling nvidia-nvtx-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.21.5\n",
            "    Uninstalling nvidia-nccl-cu12-2.21.5:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.6.0+cu124\n",
            "    Uninstalling torch-2.6.0+cu124:\n",
            "      Successfully uninstalled torch-2.6.0+cu124\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.4.0 which is incompatible.\n",
            "torchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 2.4.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvtx-cu12-12.1.105 torch-2.4.0 triton-3.0.0\n",
            "Collecting torchvision==0.15.0\n",
            "  Downloading torchvision-0.15.0-cp311-cp311-manylinux1_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision==0.15.0) (2.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchvision==0.15.0) (2.32.3)\n",
            "INFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement torch==2.0.0+cu117 (from torchvision) (from versions: 1.13.0, 1.13.1, 2.0.0, 2.0.1, 2.1.0, 2.1.1, 2.1.2, 2.2.0, 2.2.1, 2.2.2, 2.3.0, 2.3.1, 2.4.0, 2.4.1, 2.5.0, 2.5.1, 2.6.0)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for torch==2.0.0+cu117\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting torchaudio==2.4.0\n",
            "  Downloading torchaudio-2.4.0-cp311-cp311-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: torch==2.4.0 in /usr/local/lib/python3.11/dist-packages (from torchaudio==2.4.0) (2.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0->torchaudio==2.4.0) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0->torchaudio==2.4.0) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0->torchaudio==2.4.0) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0->torchaudio==2.4.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0->torchaudio==2.4.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0->torchaudio==2.4.0) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0->torchaudio==2.4.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0->torchaudio==2.4.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0->torchaudio==2.4.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0->torchaudio==2.4.0) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0->torchaudio==2.4.0) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0->torchaudio==2.4.0) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0->torchaudio==2.4.0) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0->torchaudio==2.4.0) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0->torchaudio==2.4.0) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0->torchaudio==2.4.0) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0->torchaudio==2.4.0) (12.1.105)\n",
            "Requirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0->torchaudio==2.4.0) (3.0.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.4.0->torchaudio==2.4.0) (12.5.82)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.4.0->torchaudio==2.4.0) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.4.0->torchaudio==2.4.0) (1.3.0)\n",
            "Downloading torchaudio-2.4.0-cp311-cp311-manylinux1_x86_64.whl (3.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torchaudio\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 2.6.0+cu124\n",
            "    Uninstalling torchaudio-2.6.0+cu124:\n",
            "      Successfully uninstalled torchaudio-2.6.0+cu124\n",
            "Successfully installed torchaudio-2.4.0\n",
            "Looking in links: https://data.dgl.ai/wheels/torch-2.4/cu124/repo.html\n",
            "Collecting dgl\n",
            "  Downloading https://data.dgl.ai/wheels/torch-2.4/cu124/dgl-2.4.0%2Bcu124-cp311-cp311-manylinux1_x86_64.whl (347.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m347.8/347.8 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.11/dist-packages (from dgl) (3.4.2)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from dgl) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from dgl) (24.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from dgl) (2.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from dgl) (5.9.5)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.11/dist-packages (from dgl) (2.10.6)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from dgl) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from dgl) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from dgl) (1.14.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from dgl) (4.67.1)\n",
            "Requirement already satisfied: torch<=2.4.0 in /usr/local/lib/python3.11/dist-packages (from dgl) (2.4.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->dgl) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->dgl) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->dgl) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->dgl) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->dgl) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->dgl) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->dgl) (2025.1.31)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<=2.4.0->dgl) (3.18.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch<=2.4.0->dgl) (1.13.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<=2.4.0->dgl) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<=2.4.0->dgl) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch<=2.4.0->dgl) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch<=2.4.0->dgl) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch<=2.4.0->dgl) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<=2.4.0->dgl) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch<=2.4.0->dgl) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch<=2.4.0->dgl) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch<=2.4.0->dgl) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch<=2.4.0->dgl) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch<=2.4.0->dgl) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.11/dist-packages (from torch<=2.4.0->dgl) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch<=2.4.0->dgl) (12.1.105)\n",
            "Requirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.11/dist-packages (from torch<=2.4.0->dgl) (3.0.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch<=2.4.0->dgl) (12.5.82)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->dgl) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->dgl) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->dgl) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->dgl) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<=2.4.0->dgl) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch<=2.4.0->dgl) (1.3.0)\n",
            "Installing collected packages: dgl\n",
            "Successfully installed dgl-2.4.0+cu124\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.__version__)\n",
        "print(torch.version.cuda)\n",
        "import platform\n",
        "print(platform.platform())\n",
        "print(platform.system(), platform.release())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MqQEqQ2wZz-P",
        "outputId": "10694238-1730-4ceb-cf0d-92c480931e24"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.4.0+cu121\n",
            "12.1\n",
            "Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "Linux 6.1.85+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchfile"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_4ExK9NVuiS",
        "outputId": "88a51a05-dd36-4d29-9e7e-efdb2ef079a7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchfile\n",
            "  Downloading torchfile-0.1.0.tar.gz (5.2 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: torchfile\n",
            "  Building wheel for torchfile (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchfile: filename=torchfile-0.1.0-py3-none-any.whl size=5693 sha256=2168e584774ba3b83fb52b08fafe90ec4ae38fec0b0f98c8a88075469ef59198\n",
            "  Stored in directory: /root/.cache/pip/wheels/2d/0d/6c/cc20d113923479bc37343fc2268b9b369a1b25a4cb97296b3a\n",
            "Successfully built torchfile\n",
            "Installing collected packages: torchfile\n",
            "Successfully installed torchfile-0.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lmdb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ma0inGrVvwr",
        "outputId": "41c4ea00-8788-4b18-9531-3f72ca94c362"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lmdb\n",
            "  Downloading lmdb-1.6.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)\n",
            "Downloading lmdb-1.6.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (297 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/297.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.6/297.8 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.8/297.8 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: lmdb\n",
            "Successfully installed lmdb-1.6.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pickle\n",
        "import re\n",
        "import nltk\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "import random\n",
        "import heapq\n",
        "import csv\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import numpy as np\n",
        "import time\n",
        "import math\n",
        "import lmdb\n",
        "#import gensim\n",
        "import heapq\n",
        "os.environ[\"DGL_GRAPHBOLT_DISABLE\"] = \"1\"\n",
        "\n",
        "import dgl\n",
        "import dgl.nn as dglnn\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from dgl.nn.functional import edge_softmax\n",
        "import torchfile\n",
        "from torch.nn import init\n",
        "import dgl.function as fn\n",
        "from dgl.utils import expand_as_pair\n",
        "from dgl.nn import EdgeWeightNorm\n",
        "from dgl.distributed import DistDataLoader\n",
        "from dgl.distributed.dist_dataloader import EdgeCollator\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import roc_auc_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m2apTWXyVxNR",
        "outputId": "974f676b-aad7-4015-af2f-d972d442be44"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DGL backend not selected or invalid.  Assuming PyTorch for now.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/dgl/graphbolt/__init__.py:114: GBWarning: \n",
            "An experimental feature for CUDA allocations is turned on for better allocation\n",
            "pattern resulting in better memory usage for minibatch GNN training workloads.\n",
            "See https://pytorch.org/docs/stable/notes/cuda.html#optimizing-memory-usage-with-pytorch-cuda-alloc-conf,\n",
            "and set the environment variable `PYTORCH_CUDA_ALLOC_CONF=expandable_segments:False`\n",
            "if you want to disable it and set it True to acknowledge and disable the warning.\n",
            "\n",
            "  gb_warning(WARNING_STR_TO_BE_SHOWN)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = (\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print('device: ', device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9PLKTAB9V4hq",
        "outputId": "4696186c-9cce-4577-a1aa-453e3f2b64b2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device:  cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# parameters\n",
        "Ks = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] # top@K performance\n",
        "n_test_negs = 100 # number of negative recipes for each test user\n",
        "dataset_folder = '/content/data'"
      ],
      "metadata": {
        "id": "anPOhtwzV6nw"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build graph"
      ],
      "metadata": {
        "id": "e5AYCP45V-do"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_graph():\n",
        "    print('generating graph ...')\n",
        "    edge_src, edge_dst, r_i_edge_weight = torch.load(dataset_folder+'/edge_r2i_src_dst_weight.pt')\n",
        "    recipe_edge_src, recipe_edge_dst, recipe_edge_weight = torch.load(dataset_folder+'/edge_r2r_src_and_dst_and_weight.pt')\n",
        "    ingre_edge_src, ingre_edge_dst, ingre_edge_weight = torch.load(dataset_folder+'/edge_i2i_src_and_dst_and_weight.pt')\n",
        "    all_u2r_src_dst_weight, train_u2r_src_dst_weight, val_u2r_src_dst_weight, test_u2r_src_dst_weight = torch.load(dataset_folder+'/all_train_val_test_edge_u_rate_r_src_and_dst_and_weight.pt')\n",
        "    u_rate_r_edge_src, u_rate_r_edge_dst, u_rate_r_edge_weight = all_u2r_src_dst_weight\n",
        "\n",
        "    # nodes and edges\n",
        "    graph = dgl.heterograph({\n",
        "        ('recipe', 'r-i', 'ingredient'): (edge_src, edge_dst),\n",
        "        ('ingredient', 'i-r', 'recipe'): (edge_dst, edge_src),\n",
        "        ('recipe', 'r-r', 'recipe'): (recipe_edge_src, recipe_edge_dst),\n",
        "        ('ingredient', 'i-i', 'ingredient'): (ingre_edge_src, ingre_edge_dst),\n",
        "        ('user', 'u-r', 'recipe'): (u_rate_r_edge_src, u_rate_r_edge_dst),\n",
        "        ('recipe', 'r-u', 'user'): (u_rate_r_edge_dst, u_rate_r_edge_src)\n",
        "    })\n",
        "\n",
        "    # Modify your metapaths dictionary to organize by node type\n",
        "    metapaths = {\n",
        "        'user': [\n",
        "            # User-Recipe-User metapath\n",
        "            [('user', 'u-r', 'recipe'), ('recipe', 'r-u', 'user')],\n",
        "            # User-Recipe-Recipe-User metapath (connecting through similar recipes)\n",
        "            [('user', 'u-r', 'recipe'), ('recipe', 'r-r', 'recipe'), ('recipe', 'r-u', 'user')]\n",
        "        ],\n",
        "        'recipe': [\n",
        "            # Recipe-Ingredient-Recipe metapath\n",
        "            [('recipe', 'r-i', 'ingredient'), ('ingredient', 'i-r', 'recipe')],\n",
        "            # Recipe-Recipe direct connection\n",
        "            [('recipe', 'r-r', 'recipe')],\n",
        "            # Recipe-User-Recipe metapath\n",
        "            [('recipe', 'r-u', 'user'), ('user', 'u-r', 'recipe')]\n",
        "        ],\n",
        "        'ingredient': [\n",
        "            # Ingredient-Recipe-Ingredient metapath\n",
        "            [('ingredient', 'i-r', 'recipe'), ('recipe', 'r-i', 'ingredient')],\n",
        "            # Ingredient-Ingredient direct connection\n",
        "            [('ingredient', 'i-i', 'ingredient')]\n",
        "        ]\n",
        "    }\n",
        "    graph.metapaths = metapaths\n",
        "\n",
        "    # edge weight\n",
        "    graph.edges['r-i'].data['weight'] = torch.FloatTensor(r_i_edge_weight)\n",
        "    graph.edges['i-r'].data['weight'] = torch.FloatTensor(r_i_edge_weight)\n",
        "    graph.edges['r-r'].data['weight'] = torch.FloatTensor(recipe_edge_weight)\n",
        "    graph.edges['i-i'].data['weight'] = torch.FloatTensor(ingre_edge_weight)\n",
        "    graph.edges['u-r'].data['weight'] = torch.FloatTensor(u_rate_r_edge_weight)\n",
        "    graph.edges['r-u'].data['weight'] = torch.FloatTensor(u_rate_r_edge_weight)\n",
        "\n",
        "    # node features\n",
        "    recipe_nodes_avg_instruction_features = torch.load(dataset_folder+'/recipe_nodes_avg_instruction_features.pt')\n",
        "    ingredient_nodes_nutrient_features_minus1 = torch.load(dataset_folder+'/ingredient_nodes_nutrient_features.pt')\n",
        "    graph.nodes['recipe'].data['avg_instr_feature'] = recipe_nodes_avg_instruction_features\n",
        "    graph.nodes['ingredient'].data['nutrient_feature'] = ingredient_nodes_nutrient_features_minus1\n",
        "    graph.nodes['user'].data['random_feature'] = torch.nn.init.xavier_normal_(torch.ones(7959, 300))\n",
        "    graph.nodes['recipe'].data['random_feature'] = torch.nn.init.xavier_normal_(torch.ones(68794, 1024))\n",
        "\n",
        "    return graph\n",
        "\n",
        "graph = get_graph()\n",
        "print('graph: ', graph)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YGnKjEYHV-vp",
        "outputId": "b291a7fa-ca44-4fb4-fd63-f25e6dab1174"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "generating graph ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-25a2b6790825>:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  edge_src, edge_dst, r_i_edge_weight = torch.load(dataset_folder+'/edge_r2i_src_dst_weight.pt')\n",
            "<ipython-input-10-25a2b6790825>:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  recipe_edge_src, recipe_edge_dst, recipe_edge_weight = torch.load(dataset_folder+'/edge_r2r_src_and_dst_and_weight.pt')\n",
            "<ipython-input-10-25a2b6790825>:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  ingre_edge_src, ingre_edge_dst, ingre_edge_weight = torch.load(dataset_folder+'/edge_i2i_src_and_dst_and_weight.pt')\n",
            "<ipython-input-10-25a2b6790825>:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  all_u2r_src_dst_weight, train_u2r_src_dst_weight, val_u2r_src_dst_weight, test_u2r_src_dst_weight = torch.load(dataset_folder+'/all_train_val_test_edge_u_rate_r_src_and_dst_and_weight.pt')\n",
            "<ipython-input-10-25a2b6790825>:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  recipe_nodes_avg_instruction_features = torch.load(dataset_folder+'/recipe_nodes_avg_instruction_features.pt')\n",
            "<ipython-input-10-25a2b6790825>:54: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  ingredient_nodes_nutrient_features_minus1 = torch.load(dataset_folder+'/ingredient_nodes_nutrient_features.pt')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "graph:  Graph(num_nodes={'ingredient': 8847, 'recipe': 68794, 'user': 7959},\n",
            "      num_edges={('ingredient', 'i-i', 'ingredient'): 146188, ('ingredient', 'i-r', 'recipe'): 463485, ('recipe', 'r-i', 'ingredient'): 463485, ('recipe', 'r-r', 'recipe'): 647146, ('recipe', 'r-u', 'user'): 135353, ('user', 'u-r', 'recipe'): 135353},\n",
            "      metagraph=[('ingredient', 'ingredient', 'i-i'), ('ingredient', 'recipe', 'i-r'), ('recipe', 'ingredient', 'r-i'), ('recipe', 'recipe', 'r-r'), ('recipe', 'user', 'r-u'), ('user', 'recipe', 'u-r')])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split train/val/test"
      ],
      "metadata": {
        "id": "MYnaYfvYWB-1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_src_dst_weight, train_src_dst_weight, val_src_dst_weight, test_src_dst_weight = torch.load(dataset_folder+'/all_train_val_test_edge_u_rate_r_src_and_dst_and_weight.pt')\n",
        "all_src, all_dst, all_weight = all_src_dst_weight\n",
        "train_src, train_dst, train_weight = train_src_dst_weight\n",
        "val_src, val_dst, val_weight = val_src_dst_weight\n",
        "test_src, test_dst, test_weight = test_src_dst_weight\n",
        "\n",
        "train_eids = graph.edge_ids(train_src, train_dst, etype='u-r')\n",
        "val_eids = graph.edge_ids(val_src, val_dst, etype='u-r')\n",
        "test_eids = graph.edge_ids(test_src, test_dst, etype='u-r')\n",
        "val_eids_r2u = graph.edge_ids(val_dst, val_src, etype='r-u')\n",
        "test_eids_r2u = graph.edge_ids(test_dst, test_src, etype='r-u')\n",
        "print('length of all_src: ', len(all_src))\n",
        "print('length of train_eids: ', len(train_eids))\n",
        "print('length of val_eids: ', len(val_eids))\n",
        "print('length of test_eids: ', len(test_eids))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQXpLDTqWAtT",
        "outputId": "6087a26c-5510-4bb7-9c7a-d02d0e02ca5d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of all_src:  135353\n",
            "length of train_eids:  119435\n",
            "length of val_eids:  7959\n",
            "length of test_eids:  7959\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-63b0a4d482de>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  all_src_dst_weight, train_src_dst_weight, val_src_dst_weight, test_src_dst_weight = torch.load(dataset_folder+'/all_train_val_test_edge_u_rate_r_src_and_dst_and_weight.pt')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get train_graph and val_graph\n",
        "train_graph = graph.clone()\n",
        "train_graph.remove_edges(torch.cat([val_eids, test_eids]), etype='u-r')\n",
        "train_graph.remove_edges(torch.cat([val_eids_r2u, test_eids_r2u]), etype='r-u')\n",
        "print('training graph: ')\n",
        "print(train_graph)\n",
        "print()\n",
        "\n",
        "val_graph = graph.clone()\n",
        "val_graph.remove_edges(test_eids, etype='u-r')\n",
        "val_graph.remove_edges(test_eids, etype='r-u')\n",
        "print('val graph: ')\n",
        "print(val_graph)\n",
        "\n",
        "for g in [train_graph, val_graph, graph]:\n",
        "    if not hasattr(g, '_use_graphbolt'):\n",
        "        g._use_graphbolt = False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdh0vDeCWFR5",
        "outputId": "9b118225-966d-4fa7-d5d0-97fcb9a27b62"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training graph: \n",
            "Graph(num_nodes={'ingredient': 8847, 'recipe': 68794, 'user': 7959},\n",
            "      num_edges={('ingredient', 'i-i', 'ingredient'): 146188, ('ingredient', 'i-r', 'recipe'): 463485, ('recipe', 'r-i', 'ingredient'): 463485, ('recipe', 'r-r', 'recipe'): 647146, ('recipe', 'r-u', 'user'): 119435, ('user', 'u-r', 'recipe'): 119435},\n",
            "      metagraph=[('ingredient', 'ingredient', 'i-i'), ('ingredient', 'recipe', 'i-r'), ('recipe', 'ingredient', 'r-i'), ('recipe', 'recipe', 'r-r'), ('recipe', 'user', 'r-u'), ('user', 'recipe', 'u-r')])\n",
            "\n",
            "val graph: \n",
            "Graph(num_nodes={'ingredient': 8847, 'recipe': 68794, 'user': 7959},\n",
            "      num_edges={('ingredient', 'i-i', 'ingredient'): 146188, ('ingredient', 'i-r', 'recipe'): 463485, ('recipe', 'r-i', 'ingredient'): 463485, ('recipe', 'r-r', 'recipe'): 647146, ('recipe', 'r-u', 'user'): 127394, ('user', 'u-r', 'recipe'): 127394},\n",
            "      metagraph=[('ingredient', 'ingredient', 'i-i'), ('ingredient', 'recipe', 'i-r'), ('recipe', 'ingredient', 'r-i'), ('recipe', 'recipe', 'r-r'), ('recipe', 'user', 'r-u'), ('user', 'recipe', 'u-r')])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# edge dataloaders\n",
        "sampler = dgl.dataloading.MultiLayerNeighborSampler([20, 20])\n",
        "neg_sampler = dgl.dataloading.negative_sampler.Uniform(5)\n",
        "\n",
        "class test_NegativeSampler(object):\n",
        "    def __init__(self, g, k):\n",
        "        # get the negatives\n",
        "        self.user2negs_100_dict = {}\n",
        "        filename = dataset_folder+'/test_negatives_100.txt'\n",
        "        with open(filename, \"r\") as f:\n",
        "            lines = f.readlines()\n",
        "            for line in tqdm(lines):\n",
        "                if line == None or line == \"\":\n",
        "                    continue\n",
        "                line = line[:-1] # remove \\n\n",
        "                user = int(line.split('\\t')[0].split(',')[0][1:])\n",
        "                negs = [int(neg) for neg in line.split('\\t')[1:]]\n",
        "                self.user2negs_100_dict[user] = negs\n",
        "\n",
        "        self.k = k\n",
        "\n",
        "    def __call__(self, g, eids_dict):\n",
        "        result_dict = {}\n",
        "        for etype, eids in eids_dict.items():\n",
        "            src, _ = g.find_edges(eids, etype=etype)\n",
        "            dst = []\n",
        "            for each_src in src:\n",
        "                dst.extend(self.user2negs_100_dict[int(each_src)][:self.k])\n",
        "            dst = torch.tensor(dst)\n",
        "            src = src.repeat_interleave(self.k)\n",
        "            result_dict[etype] = (src, dst)\n",
        "        return result_dict\n",
        "\n",
        "test_neg_sampler = test_NegativeSampler(graph, n_test_negs)\n",
        "test_train_neg_sampler = test_NegativeSampler(graph, n_test_negs)\n",
        "\n",
        "train_collator = EdgeCollator(\n",
        "    train_graph, {'u-r': train_graph.edge_ids(train_src, train_dst, etype='u-r')}, sampler,\n",
        "    exclude='reverse_types',\n",
        "    reverse_etypes={'u-r': 'r-u', 'r-u': 'u-r'},\n",
        "    negative_sampler=neg_sampler)\n",
        "val_collator = EdgeCollator(\n",
        "    val_graph, {'u-r': val_graph.edge_ids(val_src, val_dst, etype='u-r')}, sampler,\n",
        "    exclude='reverse_types',\n",
        "    reverse_etypes={'u-r': 'r-u', 'r-u': 'u-r'},\n",
        "    negative_sampler=neg_sampler)\n",
        "test_collator = EdgeCollator(\n",
        "    graph, {('user', 'u-r', 'recipe'): test_eids}, sampler,\n",
        "    exclude='reverse_types',\n",
        "    reverse_etypes={'u-r': 'r-u', 'r-u': 'u-r'},\n",
        "    negative_sampler=test_neg_sampler)\n",
        "\n",
        "train_edgeloader = torch.utils.data.DataLoader(\n",
        "    train_collator.dataset, collate_fn=train_collator.collate,\n",
        "    batch_size=1024, shuffle=True, drop_last=False, num_workers=0)\n",
        "val_edgeloader = torch.utils.data.DataLoader(\n",
        "    val_collator.dataset, collate_fn=val_collator.collate,\n",
        "    batch_size=128, shuffle=False, drop_last=False, num_workers=0)\n",
        "test_edgeloader = torch.utils.data.DataLoader(\n",
        "    test_collator.dataset, collate_fn=test_collator.collate,\n",
        "    batch_size=128, shuffle=False, drop_last=False, num_workers=0)\n",
        "\n",
        "print('# of batches in train_edgeloader: ', len(train_edgeloader))\n",
        "print('# of batches in val_edgeloader: ', len(val_edgeloader))\n",
        "print('# of batches in test_edgeloader: ', len(test_edgeloader))\n",
        "print()\n",
        "\n",
        "for input_nodes, pos_pair_graph, neg_pair_graph, blocks in train_edgeloader:\n",
        "    print('blocks: ', blocks)\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wy_Y68RVWHnO",
        "outputId": "6721e3cd-1862-4cae-de8c-5daa7630b64e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 7959/7959 [00:00<00:00, 42545.16it/s]\n",
            "100%|██████████| 7959/7959 [00:00<00:00, 40442.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# of batches in train_edgeloader:  117\n",
            "# of batches in val_edgeloader:  63\n",
            "# of batches in test_edgeloader:  63\n",
            "\n",
            "blocks:  [Block(num_src_nodes={'ingredient': 6508, 'recipe': 61733, 'user': 5835},\n",
            "      num_dst_nodes={'ingredient': 3667, 'recipe': 27135, 'user': 3044},\n",
            "      num_edges={('ingredient', 'i-i', 'ingredient'): 46844, ('ingredient', 'i-r', 'recipe'): 177442, ('recipe', 'r-i', 'ingredient'): 50600, ('recipe', 'r-r', 'recipe'): 252098, ('recipe', 'r-u', 'user'): 35060, ('user', 'u-r', 'recipe'): 52412},\n",
            "      metagraph=[('ingredient', 'ingredient', 'i-i'), ('ingredient', 'recipe', 'i-r'), ('recipe', 'ingredient', 'r-i'), ('recipe', 'recipe', 'r-r'), ('recipe', 'user', 'r-u'), ('user', 'recipe', 'u-r')]), Block(num_src_nodes={'ingredient': 3667, 'recipe': 27135, 'user': 3044},\n",
            "      num_dst_nodes={'ingredient': 0, 'recipe': 5884, 'user': 669},\n",
            "      num_edges={('ingredient', 'i-i', 'ingredient'): 0, ('ingredient', 'i-r', 'recipe'): 39122, ('recipe', 'r-i', 'ingredient'): 0, ('recipe', 'r-r', 'recipe'): 31421, ('recipe', 'r-u', 'user'): 10170, ('user', 'u-r', 'recipe'): 9867},\n",
            "      metagraph=[('ingredient', 'ingredient', 'i-i'), ('ingredient', 'recipe', 'i-r'), ('recipe', 'ingredient', 'r-i'), ('recipe', 'recipe', 'r-r'), ('recipe', 'user', 'r-u'), ('user', 'recipe', 'u-r')])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set Transformer"
      ],
      "metadata": {
        "id": "4AIrpW8fWIiC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get ingre neighbors for each recipe nodes\n",
        "def get_recipe2ingreNeighbor_dict():\n",
        "    max_length = 33\n",
        "    out = {}\n",
        "    neighbor_list = []\n",
        "    ingre_length_list = []\n",
        "    total_length_index_list = []\n",
        "    total_ingre_neighbor_list = []\n",
        "    total_length_index = 0\n",
        "    total_length_index_list.append(total_length_index)\n",
        "    for recipeNodeID in tqdm(range(graph.number_of_nodes('recipe'))):\n",
        "        _, succs = graph.out_edges(recipeNodeID, etype='r-i')\n",
        "        succs_list = list(set(succs.tolist()))\n",
        "        total_ingre_neighbor_list.extend(succs_list)\n",
        "        cur_length = len(succs_list)\n",
        "        ingre_length_list.append(cur_length)\n",
        "\n",
        "        total_length_index += cur_length\n",
        "        total_length_index_list.append(total_length_index)\n",
        "        while len(succs_list) < max_length:\n",
        "            succs_list.append(77733)\n",
        "        neighbor_list.append(succs_list)\n",
        "\n",
        "    ingre_neighbor_tensor = torch.tensor(neighbor_list)\n",
        "    ingre_length_tensor = torch.tensor(ingre_length_list)\n",
        "    total_ingre_neighbor_tensor = torch.tensor(total_ingre_neighbor_list)\n",
        "    return ingre_neighbor_tensor, ingre_length_tensor, total_length_index_list, total_ingre_neighbor_tensor\n",
        "\n",
        "ingre_neighbor_tensor, ingre_length_tensor, total_length_index_list, total_ingre_neighbor_tensor = get_recipe2ingreNeighbor_dict()\n",
        "print('ingre_neighbor_tensor: ', ingre_neighbor_tensor.shape)\n",
        "print('ingre_length_tensor: ', ingre_length_tensor.shape)\n",
        "print('total_length_index_list: ', len(total_length_index_list))\n",
        "print('total_ingre_neighbor_tensor: ', total_ingre_neighbor_tensor.shape)\n",
        "\n",
        "def find(tensor, values):\n",
        "    return torch.nonzero(tensor[..., None] == values)\n",
        "\n",
        "# example of find()\n",
        "# a = torch.tensor([0, 10, 20, 30])\n",
        "# b = torch.tensor([[ 0, 30, 20,  10, 77733],[ 0, 30, 20,  10, 77733]])\n",
        "# find(b, a)[:, 2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2CK-Q87xo_c",
        "outputId": "897ac0c6-5269-450c-8dc6-b4f61bb27f91"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 68794/68794 [00:12<00:00, 5383.86it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ingre_neighbor_tensor:  torch.Size([68794, 33])\n",
            "ingre_length_tensor:  torch.Size([68794])\n",
            "total_length_index_list:  68795\n",
            "total_ingre_neighbor_tensor:  torch.Size([454941])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_ingredient_neighbors_all_embeddings(blocks, output_nodes, secondToLast_ingre):\n",
        "    ingreNodeIDs = blocks[1].srcdata['_ID']['ingredient']\n",
        "    recipeNodeIDs = output_nodes\n",
        "    batch_ingre_neighbors = ingre_neighbor_tensor[recipeNodeIDs.cpu()].to(device)\n",
        "    batch_ingre_length = ingre_length_tensor[recipeNodeIDs.cpu()].to(device)\n",
        "    valid_batch_ingre_neighbors = find(batch_ingre_neighbors, ingreNodeIDs)[:, 2]\n",
        "\n",
        "    # based on valid_batch_ingre_neighbors each row index\n",
        "    _, valid_batch_ingre_length = torch.unique(find(batch_ingre_neighbors, ingreNodeIDs)[:, 0], return_counts=True)\n",
        "    batch_sum_ingre_length = np.cumsum(valid_batch_ingre_length.cpu())\n",
        "\n",
        "    total_ingre_emb = None\n",
        "    for i in range(len(recipeNodeIDs)):\n",
        "        if i == 0:\n",
        "            recipeNode_ingres = valid_batch_ingre_neighbors[0:batch_sum_ingre_length[i]]\n",
        "            a = secondToLast_ingre[recipeNode_ingres]\n",
        "        else:\n",
        "            recipeNode_ingres = valid_batch_ingre_neighbors[batch_sum_ingre_length[i-1]:batch_sum_ingre_length[i]]\n",
        "            a = secondToLast_ingre[recipeNode_ingres]\n",
        "\n",
        "        # all ingre instead of average\n",
        "        a_rows = a.shape[0]\n",
        "        a_columns = a.shape[1]\n",
        "        max_rows = 5\n",
        "        if a_rows < max_rows:\n",
        "            a = torch.cat([a, torch.zeros(max_rows-a_rows, a_columns).cuda()])\n",
        "        else:\n",
        "            a = a[:max_rows, :]\n",
        "\n",
        "        if total_ingre_emb == None:\n",
        "            total_ingre_emb = a.unsqueeze(0)\n",
        "        else:\n",
        "            total_ingre_emb = torch.cat([total_ingre_emb,a.unsqueeze(0)], dim = 0)\n",
        "            if torch.isnan(total_ingre_emb).any():\n",
        "                print('Error!')\n",
        "\n",
        "    return total_ingre_emb"
      ],
      "metadata": {
        "id": "KzjTLh-Exp4q"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(nn.Module):\n",
        "    \"\"\"Scaled Dot-Product Attention.\"\"\"\n",
        "\n",
        "    def __init__(self, temperature):\n",
        "        super().__init__()\n",
        "\n",
        "        self.temperature = temperature\n",
        "        self.softmax = nn.Softmax(dim=2)\n",
        "\n",
        "    def forward(self, queries, keys, values):\n",
        "        \"\"\"\n",
        "        It is equivariant to permutations\n",
        "        of the batch dimension (`b`).\n",
        "\n",
        "        It is equivariant to permutations of the\n",
        "        second dimension of the queries (`n`).\n",
        "\n",
        "        It is invariant to permutations of the\n",
        "        second dimension of keys and values (`m`).\n",
        "\n",
        "        Arguments:\n",
        "            queries: a float tensor with shape [b, n, d].\n",
        "            keys: a float tensor with shape [b, m, d].\n",
        "            values: a float tensor with shape [b, m, d'].\n",
        "        Returns:\n",
        "            a float tensor with shape [b, n, d'].\n",
        "        \"\"\"\n",
        "\n",
        "        attention = torch.bmm(queries, keys.transpose(1, 2))\n",
        "        attention = self.softmax(attention / self.temperature)\n",
        "        # it has shape [b, n, m]\n",
        "\n",
        "        return torch.bmm(attention, values)\n",
        "\n",
        "\n",
        "class MultiheadAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, d, h):\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            d: an integer, dimension of queries and values.\n",
        "                It is assumed that input and\n",
        "                output dimensions are the same.\n",
        "            h: an integer, number of heads.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        assert d % h == 0\n",
        "        self.h = h\n",
        "\n",
        "        # everything is projected to this dimension\n",
        "        p = d // h\n",
        "\n",
        "        self.project_queries = nn.Linear(d, d)\n",
        "        self.project_keys = nn.Linear(d, d)\n",
        "        self.project_values = nn.Linear(d, d)\n",
        "        self.concatenation = nn.Linear(d, d)\n",
        "        self.attention = Attention(temperature=p**0.5)\n",
        "\n",
        "    def forward(self, queries, keys, values):\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            queries: a float tensor with shape [b, n, d].\n",
        "            keys: a float tensor with shape [b, m, d].\n",
        "            values: a float tensor with shape [b, m, d].\n",
        "        Returns:\n",
        "            a float tensor with shape [b, n, d].\n",
        "        \"\"\"\n",
        "\n",
        "        h = self.h\n",
        "        b, n, d = queries.size()\n",
        "        _, m, _ = keys.size()\n",
        "        p = d // h\n",
        "\n",
        "        queries = self.project_queries(queries)  # shape [b, n, d]\n",
        "        keys = self.project_keys(keys)  # shape [b, m, d]\n",
        "        values = self.project_values(values)  # shape [b, m, d]\n",
        "\n",
        "        queries = queries.view(b, n, h, p)\n",
        "        keys = keys.view(b, m, h, p)\n",
        "        values = values.view(b, m, h, p)\n",
        "\n",
        "        queries = queries.permute(2, 0, 1, 3).contiguous().view(h * b, n, p)\n",
        "        keys = keys.permute(2, 0, 1, 3).contiguous().view(h * b, m, p)\n",
        "        values = values.permute(2, 0, 1, 3).contiguous().view(h * b, m, p)\n",
        "\n",
        "        output = self.attention(queries, keys, values)  # shape [h * b, n, p]\n",
        "        output = output.view(h, b, n, p)\n",
        "        output = output.permute(1, 2, 0, 3).contiguous().view(b, n, d)\n",
        "        output = self.concatenation(output)  # shape [b, n, d]\n",
        "\n",
        "        return output\n",
        "\n",
        "class RFF(nn.Module):\n",
        "    \"\"\"\n",
        "    Row-wise FeedForward layers.\n",
        "    \"\"\"\n",
        "    def __init__(self, d):\n",
        "        super().__init__()\n",
        "\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(d, d), nn.ReLU(inplace=True),\n",
        "            nn.Linear(d, d), nn.ReLU(inplace=True),\n",
        "            nn.Linear(d, d), nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            x: a float tensor with shape [b, n, d].\n",
        "        Returns:\n",
        "            a float tensor with shape [b, n, d].\n",
        "        \"\"\"\n",
        "        return self.layers(x)\n",
        "\n",
        "class MultiheadAttentionBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, d, h, rff):\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            d: an integer, input dimension.\n",
        "            h: an integer, number of heads.\n",
        "            rff: a module, row-wise feedforward layers.\n",
        "                It takes a float tensor with shape [b, n, d] and\n",
        "                returns a float tensor with the same shape.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.multihead = MultiheadAttention(d, h)\n",
        "        self.layer_norm1 = nn.LayerNorm(d)\n",
        "        self.layer_norm2 = nn.LayerNorm(d)\n",
        "        self.rff = rff\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        \"\"\"\n",
        "        It is equivariant to permutations of the\n",
        "        second dimension of tensor x (`n`).\n",
        "\n",
        "        It is invariant to permutations of the\n",
        "        second dimension of tensor y (`m`).\n",
        "\n",
        "        Arguments:\n",
        "            x: a float tensor with shape [b, n, d].\n",
        "            y: a float tensor with shape [b, m, d].\n",
        "        Returns:\n",
        "            a float tensor with shape [b, n, d].\n",
        "        \"\"\"\n",
        "        h = self.layer_norm1(x + self.multihead(x, y, y))\n",
        "        return self.layer_norm2(h + self.rff(h))\n",
        "\n",
        "class SetAttentionBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, d, h, rff):\n",
        "        super().__init__()\n",
        "        self.mab = MultiheadAttentionBlock(d, h, rff)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            x: a float tensor with shape [b, n, d].\n",
        "        Returns:\n",
        "            a float tensor with shape [b, n, d].\n",
        "        \"\"\"\n",
        "        return self.mab(x, x)\n",
        "\n",
        "class InducedSetAttentionBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, d, m, h, rff1, rff2):\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            d: an integer, input dimension.\n",
        "            m: an integer, number of inducing points.\n",
        "            h: an integer, number of heads.\n",
        "            rff1, rff2: modules, row-wise feedforward layers.\n",
        "                It takes a float tensor with shape [b, n, d] and\n",
        "                returns a float tensor with the same shape.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.mab1 = MultiheadAttentionBlock(d, h, rff1)\n",
        "        self.mab2 = MultiheadAttentionBlock(d, h, rff2)\n",
        "        self.inducing_points = nn.Parameter(torch.randn(1, m, d))\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            x: a float tensor with shape [b, n, d].\n",
        "        Returns:\n",
        "            a float tensor with shape [b, n, d].\n",
        "        \"\"\"\n",
        "        b = x.size(0)\n",
        "        p = self.inducing_points\n",
        "        p = p.repeat([b, 1, 1])  # shape [b, m, d]\n",
        "        h = self.mab1(p, x)  # shape [b, m, d]\n",
        "        return self.mab2(x, h)\n",
        "\n",
        "class PoolingMultiheadAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, d, k, h, rff):\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            d: an integer, input dimension.\n",
        "            k: an integer, number of seed vectors.\n",
        "            h: an integer, number of heads.\n",
        "            rff: a module, row-wise feedforward layers.\n",
        "                It takes a float tensor with shape [b, n, d] and\n",
        "                returns a float tensor with the same shape.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.mab = MultiheadAttentionBlock(d, h, rff)\n",
        "        self.seed_vectors = nn.Parameter(torch.randn(1, k, d))\n",
        "\n",
        "    def forward(self, z):\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            z: a float tensor with shape [b, n, d].\n",
        "        Returns:\n",
        "            a float tensor with shape [b, k, d].\n",
        "        \"\"\"\n",
        "        b = z.size(0)\n",
        "        s = self.seed_vectors\n",
        "        s = s.repeat([b, 1, 1])  # random seed vector: shape [b, k, d]\n",
        "\n",
        "        output = self.mab(s, z)\n",
        "        # print('PoolingMultiheadAttention', output.shape)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "dpAq5QwSxsPH"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set transformer for ingredient representation\n",
        "class SetTransformer(nn.Module):\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            in_dimension: an integer.  # 2\n",
        "            out_dimension: an integer. # 5 * K\n",
        "        \"\"\"\n",
        "        super(SetTransformer, self).__init__()\n",
        "        in_dimension = 46 # 300\n",
        "        out_dimension = 128 # 600\n",
        "\n",
        "        d = in_dimension\n",
        "        m = 46  # number of inducing points\n",
        "        h = 2  # 4 # number of heads\n",
        "        k = 4  # number of seed vectors\n",
        "\n",
        "        self.encoder = nn.Sequential(\n",
        "            InducedSetAttentionBlock(d, m, h, RFF(d), RFF(d)),\n",
        "            InducedSetAttentionBlock(d, m, h, RFF(d), RFF(d))\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            PoolingMultiheadAttention(d, k, h, RFF(d)),\n",
        "            SetAttentionBlock(d, h, RFF(d))\n",
        "        )\n",
        "\n",
        "        self.decoder_2 = nn.Sequential(\n",
        "            PoolingMultiheadAttention(d, k, h, RFF(d))\n",
        "        )\n",
        "        self.decoder_3 = nn.Sequential(\n",
        "            SetAttentionBlock(d, h, RFF(d))\n",
        "        )\n",
        "\n",
        "        self.predictor = nn.Sequential(\n",
        "            nn.Linear(k * d, out_dimension),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            x: a float tensor with shape [batch, n, in_dimension].\n",
        "        Returns:\n",
        "            a float tensor with shape [batch, out_dimension].\n",
        "        \"\"\"\n",
        "        x = self.encoder(x) # x = self.encoder(cut_x) # shape [batch, batch_max_len, d]\n",
        "        x = self.dropout(x)\n",
        "        x = self.decoder(x)  # shape [batch, k, d]\n",
        "\n",
        "        b, k, d = x.shape\n",
        "        x = x.view(b, k * d)\n",
        "\n",
        "        y = self.predictor(x)\n",
        "        return y"
      ],
      "metadata": {
        "id": "yvYTdowNxuD3"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GNN"
      ],
      "metadata": {
        "id": "gSid1gjfxz0V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class custom_HeteroGraphConv(nn.Module):\n",
        "    def __init__(self, mods, aggregate='sum'):\n",
        "        super(custom_HeteroGraphConv, self).__init__()\n",
        "        self.mods = nn.ModuleDict(mods)\n",
        "        # Do not break if graph has 0-in-degree nodes.\n",
        "        # Because there is no general rule to add self-loop for heterograph.\n",
        "        for _, v in self.mods.items():\n",
        "            set_allow_zero_in_degree_fn = getattr(v, 'set_allow_zero_in_degree', None)\n",
        "            if callable(set_allow_zero_in_degree_fn):\n",
        "                set_allow_zero_in_degree_fn(True)\n",
        "        if isinstance(aggregate, str):\n",
        "            self.agg_fn = get_aggregate_fn(aggregate)\n",
        "        else:\n",
        "            self.agg_fn = aggregate\n",
        "\n",
        "    def forward(self, g, inputs, mod_args=None, mod_kwargs=None):\n",
        "        if mod_args is None:\n",
        "            mod_args = {}\n",
        "        if mod_kwargs is None:\n",
        "            mod_kwargs = {}\n",
        "        outputs = {nty : [] for nty in g.dsttypes}\n",
        "        if isinstance(inputs, tuple) or g.is_block:\n",
        "            if isinstance(inputs, tuple):\n",
        "                src_inputs, dst_inputs = inputs\n",
        "            else:\n",
        "                src_inputs = inputs\n",
        "                dst_inputs = {k: v[:g.number_of_dst_nodes(k)] for k, v in inputs.items()}\n",
        "\n",
        "            for stype, etype, dtype in g.canonical_etypes:\n",
        "                rel_graph = g[stype, etype, dtype]\n",
        "                if rel_graph.number_of_edges() == 0:\n",
        "                    continue\n",
        "                if stype not in src_inputs or dtype not in dst_inputs:\n",
        "                    continue\n",
        "                dstdata = self.mods[etype](\n",
        "                    rel_graph,\n",
        "                    (src_inputs[stype], dst_inputs[dtype], mod_kwargs),\n",
        "                    *mod_args.get(etype, ())\n",
        "                    )\n",
        "                outputs[dtype].append(dstdata)\n",
        "        else:\n",
        "            for stype, etype, dtype in g.canonical_etypes:\n",
        "                rel_graph = g[stype, etype, dtype]\n",
        "                if rel_graph.number_of_edges() == 0:\n",
        "                    continue\n",
        "                if stype not in inputs:\n",
        "                    continue\n",
        "                dstdata = self.mods[etype](\n",
        "                    rel_graph,\n",
        "                    (inputs[stype], inputs[dtype], mod_kwargs),\n",
        "                    *mod_args.get(etype, ())\n",
        "                    )\n",
        "                outputs[dtype].append(dstdata)\n",
        "        rsts = {}\n",
        "        for nty, alist in outputs.items():\n",
        "            if len(alist) != 0:\n",
        "                rsts[nty] = self.agg_fn(alist, nty)\n",
        "        return rsts\n",
        "\n",
        "def _max_reduce_func(inputs, dim):\n",
        "    return torch.max(inputs, dim=dim)[0]\n",
        "\n",
        "def _min_reduce_func(inputs, dim):\n",
        "    return torch.min(inputs, dim=dim)[0]\n",
        "\n",
        "def _sum_reduce_func(inputs, dim):\n",
        "    return torch.sum(inputs, dim=dim)\n",
        "\n",
        "def _mean_reduce_func(inputs, dim):\n",
        "    return torch.mean(inputs, dim=dim)\n",
        "\n",
        "def _stack_agg_func(inputs, dsttype):\n",
        "    if len(inputs) == 0:\n",
        "        return None\n",
        "    return torch.stack(inputs, dim=1)\n",
        "\n",
        "def _agg_func(inputs, dsttype, fn):\n",
        "    if len(inputs) == 0:\n",
        "        return None\n",
        "    stacked = torch.stack(inputs, dim=0)\n",
        "    return fn(stacked, dim=0)\n",
        "\n",
        "def get_aggregate_fn(agg):\n",
        "    if agg == 'sum':\n",
        "        fn = _sum_reduce_func\n",
        "    elif agg == 'max':\n",
        "        fn = _max_reduce_func\n",
        "    elif agg == 'min':\n",
        "        fn = _min_reduce_func\n",
        "    elif agg == 'mean':\n",
        "        fn = _mean_reduce_func\n",
        "    elif agg == 'stack':\n",
        "        fn = None  # will not be called\n",
        "    else:\n",
        "        raise DGLError('Invalid cross type aggregator. Must be one of '\n",
        "                       '\"sum\", \"max\", \"min\", \"mean\" or \"stack\". But got \"%s\"' % agg)\n",
        "    if agg == 'stack':\n",
        "        return _stack_agg_func\n",
        "    else:\n",
        "        return partial(_agg_func, fn=fn)"
      ],
      "metadata": {
        "id": "Mta2Thmrx0mX"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ScorePredictor(nn.Module):\n",
        "    def forward(self, edge_subgraph, x):\n",
        "        with edge_subgraph.local_scope():\n",
        "            edge_subgraph.ndata['x'] = x\n",
        "            edge_subgraph.apply_edges(dgl.function.u_dot_v('x', 'x', 'score'), etype='u-r')\n",
        "            return edge_subgraph.edata['score'][('user', 'u-r', 'recipe')].squeeze()\n",
        "\n",
        "\n",
        "class RelationAttention(nn.Module):\n",
        "    def __init__(self, in_size, hidden_size=16):\n",
        "        super(RelationAttention, self).__init__()\n",
        "\n",
        "        self.project = nn.Sequential(\n",
        "            nn.Linear(in_size, hidden_size),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_size, 1, bias=False)\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        w = self.project(z).mean(0)                    # (M, 1)\n",
        "        beta = torch.softmax(w, dim=0)                 # (M, 1)\n",
        "        beta = beta.expand((z.shape[0],) + beta.shape) # (N, M, 1)\n",
        "        out = (beta * z).sum(1)                        # (N, D * K)\n",
        "        return out\n",
        "\n",
        "def node_drop(feats, drop_rate, training):\n",
        "    n = feats.shape[0]\n",
        "    drop_rates = torch.FloatTensor(np.ones(n) * drop_rate)\n",
        "\n",
        "    if training:\n",
        "        masks = torch.bernoulli(1. - drop_rates).unsqueeze(1)\n",
        "        feats = masks.to(feats.device) * feats / (1. - drop_rate)\n",
        "    else:\n",
        "        feats = feats\n",
        "    return feats\n",
        "\n",
        "\n",
        "class custom_GATConv(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_feats,\n",
        "                 out_feats,\n",
        "                 num_heads,\n",
        "                 feat_drop=0.1,\n",
        "                 attn_drop=0.,\n",
        "                 negative_slope=0.2,\n",
        "                 edge_drop=0.1,\n",
        "                 residual=False,\n",
        "                 activation=None,\n",
        "                 allow_zero_in_degree=False,\n",
        "                 bias=True):\n",
        "        super(custom_GATConv, self).__init__()\n",
        "        self._num_heads = num_heads\n",
        "        self._in_src_feats, self._in_dst_feats = expand_as_pair(in_feats)\n",
        "        self._out_feats = out_feats\n",
        "        self._allow_zero_in_degree = allow_zero_in_degree\n",
        "        if isinstance(in_feats, tuple):\n",
        "            self.fc_src = nn.Linear(\n",
        "                self._in_src_feats, out_feats * num_heads, bias=False)\n",
        "            self.fc_dst = nn.Linear(\n",
        "                self._in_dst_feats, out_feats * num_heads, bias=False)\n",
        "            self.fc_src2 = nn.Linear(\n",
        "                self._in_src_feats, out_feats * num_heads, bias=False)\n",
        "            self.fc_dst2 = nn.Linear(\n",
        "                self._in_dst_feats, out_feats * num_heads, bias=False)\n",
        "        else:\n",
        "            self.fc = nn.Linear(\n",
        "                self._in_src_feats, out_feats * num_heads, bias=False)\n",
        "            self.fc2 = nn.Linear(\n",
        "                self._in_src_feats, out_feats * num_heads, bias=False)\n",
        "        self.attn_l = nn.Parameter(torch.FloatTensor(size=(1, num_heads, out_feats)))\n",
        "        self.attn_r = nn.Parameter(torch.FloatTensor(size=(1, num_heads, out_feats)))\n",
        "        self.attn_l2 = nn.Parameter(torch.FloatTensor(size=(1, num_heads, out_feats)))\n",
        "        self.attn_r2 = nn.Parameter(torch.FloatTensor(size=(1, num_heads, out_feats)))\n",
        "        self.feat_drop = nn.Dropout(feat_drop)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.leaky_relu = nn.LeakyReLU(negative_slope)\n",
        "        if bias:\n",
        "            self.bias = nn.Parameter(torch.FloatTensor(size=(num_heads * out_feats,)))\n",
        "        else:\n",
        "            self.register_buffer('bias', None)\n",
        "        if residual:\n",
        "            if self._in_dst_feats != out_feats:\n",
        "                self.res_fc = nn.Linear(\n",
        "                    self._in_dst_feats, num_heads * out_feats, bias=False)\n",
        "            else:\n",
        "                self.res_fc = Identity()\n",
        "        else:\n",
        "            self.register_buffer('res_fc', None)\n",
        "        self.reset_parameters()\n",
        "        self.activation = activation\n",
        "\n",
        "        self.edge_drop = edge_drop\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        gain = nn.init.calculate_gain('relu')\n",
        "        if hasattr(self, 'fc'):\n",
        "            nn.init.xavier_normal_(self.fc.weight, gain=gain)\n",
        "        else:\n",
        "            nn.init.xavier_normal_(self.fc_src.weight, gain=gain)\n",
        "            nn.init.xavier_normal_(self.fc_dst.weight, gain=gain)\n",
        "        nn.init.xavier_normal_(self.attn_l, gain=gain)\n",
        "        nn.init.xavier_normal_(self.attn_r, gain=gain)\n",
        "        nn.init.xavier_normal_(self.attn_l2, gain=gain)\n",
        "        nn.init.xavier_normal_(self.attn_r2, gain=gain)\n",
        "        nn.init.constant_(self.bias, 0)\n",
        "        if isinstance(self.res_fc, nn.Linear):\n",
        "            nn.init.xavier_normal_(self.res_fc.weight, gain=gain)\n",
        "\n",
        "    def set_allow_zero_in_degree(self, set_value):\n",
        "        self._allow_zero_in_degree = set_value\n",
        "\n",
        "    def forward(self, graph, feat, get_attention=False):\n",
        "      with graph.local_scope():\n",
        "          if not self._allow_zero_in_degree:\n",
        "              if (graph.in_degrees() == 0).any():\n",
        "                  raise DGLError('There are 0-in-degree nodes in the graph, '\n",
        "                                'output for those nodes will be invalid. '\n",
        "                                'This is harmful for some applications, '\n",
        "                                'causing silent performance regression. '\n",
        "                                'Adding self-loop on the input graph by '\n",
        "                                'calling `g = dgl.add_self_loop(g)` will resolve '\n",
        "                                'the issue. Setting ``allow_zero_in_degree`` '\n",
        "                                'to be `True` when constructing this module will '\n",
        "                                'suppress the check and let the code run.')\n",
        "          if isinstance(feat, tuple):\n",
        "              do_edge_drop = feat[2]\n",
        "              h_src = self.feat_drop(feat[0])\n",
        "              h_dst = self.feat_drop(feat[1])\n",
        "              h_src2 = h_src.clone()\n",
        "              h_dst2 = h_dst.clone()\n",
        "              if not hasattr(self, 'fc_src'):\n",
        "                  feat_src = self.fc(h_src).view(-1, self._num_heads, self._out_feats)\n",
        "                  feat_dst = self.fc(h_dst).view(-1, self._num_heads, self._out_feats)\n",
        "                  feat_src2 = self.fc2(h_src2).view(-1, self._num_heads, self._out_feats)\n",
        "                  feat_dst2 = self.fc2(h_dst2).view(-1, self._num_heads, self._out_feats)\n",
        "              else:\n",
        "                  feat_src = self.fc_src(h_src).view(-1, self._num_heads, self._out_feats)\n",
        "                  feat_dst = self.fc_dst(h_dst).view(-1, self._num_heads, self._out_feats)\n",
        "                  feat_src2 = self.fc_src2(h_src2).view(-1, self._num_heads, self._out_feats)\n",
        "                  feat_dst2 = self.fc_dst2(h_dst2).view(-1, self._num_heads, self._out_feats)\n",
        "          else:\n",
        "              h_src = h_dst = self.feat_drop(feat)\n",
        "              h_src2 = h_dst2 = h_src.clone()\n",
        "              feat_src = feat_dst = self.fc(h_src).view(-1, self._num_heads, self._out_feats)\n",
        "              feat_src2 = feat_dst2 = self.fc(h_src).view(-1, self._num_heads, self._out_feats)\n",
        "              if graph.is_block:\n",
        "                  feat_dst = feat_src[:graph.number_of_dst_nodes()]\n",
        "                  feat_dst2 = feat_src2[:graph.number_of_dst_nodes()]\n",
        "              do_edge_drop = False  # Set default when not provided\n",
        "\n",
        "          el = (feat_src * self.attn_l).sum(dim=-1).unsqueeze(-1)\n",
        "          er = (feat_dst * self.attn_r).sum(dim=-1).unsqueeze(-1)\n",
        "\n",
        "          graph.srcdata.update({'ft': feat_src, 'el': el, 'feat_src2': feat_src2})\n",
        "          graph.dstdata.update({'er': er, 'feat_dst2': feat_dst2})\n",
        "          graph.apply_edges(fn.u_add_v('el', 'er', 'e'))\n",
        "          e = self.leaky_relu(graph.edata.pop('e'))\n",
        "\n",
        "          # Use do_edge_drop here\n",
        "          if self.training and do_edge_drop and self.edge_drop > 0:\n",
        "              perm = torch.randperm(graph.number_of_edges(), device=e.device)\n",
        "              bound = int(graph.number_of_edges() * self.edge_drop)\n",
        "              eids = perm[bound:]\n",
        "              graph.edata[\"a\"] = torch.zeros_like(e)\n",
        "              graph.edata[\"a\"][eids] = self.attn_drop(edge_softmax(graph, e[eids], eids=eids))\n",
        "          else:\n",
        "              graph.edata['a'] = self.attn_drop(edge_softmax(graph, e))\n",
        "\n",
        "          # message passing\n",
        "          graph.update_all(fn.u_mul_e('ft', 'a', 'm'),\n",
        "                          fn.sum('m', 'initial_ft'))\n",
        "          graph.update_all(fn.u_mul_v('feat_src2', 'feat_dst2', 'm2'),\n",
        "                          fn.sum('m2', 'add_ft'))\n",
        "          rst = graph.dstdata['initial_ft'] + graph.dstdata['add_ft']\n",
        "\n",
        "          # residual\n",
        "          if self.res_fc is not None:\n",
        "              resval = self.res_fc(h_dst).view(h_dst.shape[0], self._num_heads, self._out_feats)\n",
        "              rst = rst + resval\n",
        "          # bias\n",
        "          if self.bias is not None:\n",
        "              rst = rst + self.bias.view(1, self._num_heads, self._out_feats)\n",
        "          # activation\n",
        "          if self.activation:\n",
        "              rst = self.activation(rst)\n",
        "\n",
        "          if get_attention:\n",
        "              return rst, graph.edata['a']\n",
        "          else:\n",
        "              return rst\n",
        "\n",
        "\n",
        "class GNN(nn.Module):\n",
        "    def __init__(self, in_feats, hid_feats, out_feats, rel_names):\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_heads = 4 # 8\n",
        "        self.hid_feats = int(hid_feats/self.num_heads)\n",
        "        self.out_feats = int(out_feats/self.num_heads)\n",
        "        self.relation_attention = RelationAttention(hid_feats)\n",
        "\n",
        "        self.gatconv1 = custom_HeteroGraphConv({ # dglnn.HeteroGraphConv\n",
        "            'i-r': custom_GATConv(in_feats, self.hid_feats, num_heads=self.num_heads),\n",
        "            'r-i': custom_GATConv(in_feats, self.hid_feats, num_heads=self.num_heads),\n",
        "            'r-r': custom_GATConv(in_feats, self.hid_feats, num_heads=self.num_heads),\n",
        "            'i-i': custom_GATConv(in_feats, self.hid_feats, num_heads=self.num_heads),\n",
        "            'u-r': custom_GATConv(in_feats, self.hid_feats, num_heads=self.num_heads),\n",
        "            'r-u': custom_GATConv(in_feats, self.hid_feats, num_heads=self.num_heads),\n",
        "            }, aggregate='stack')\n",
        "\n",
        "        self.gatconv2 = custom_HeteroGraphConv({ # dglnn.HeteroGraphConv\n",
        "            'i-r': custom_GATConv(self.hid_feats*self.num_heads, self.out_feats, num_heads=self.num_heads),\n",
        "            'r-i': custom_GATConv(self.hid_feats*self.num_heads, self.out_feats, num_heads=self.num_heads),\n",
        "            'r-r': custom_GATConv(self.hid_feats*self.num_heads, self.out_feats, num_heads=self.num_heads),\n",
        "            'i-i': custom_GATConv(self.hid_feats*self.num_heads, self.out_feats, num_heads=self.num_heads),\n",
        "            'u-r': custom_GATConv(self.hid_feats*self.num_heads, self.out_feats, num_heads=self.num_heads),\n",
        "            'r-u': custom_GATConv(self.hid_feats*self.num_heads, self.out_feats, num_heads=self.num_heads),\n",
        "            }, aggregate='stack')\n",
        "\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "\n",
        "    def forward(self, blocks, inputs, do_edge_drop):\n",
        "        edge_weight_0 = blocks[0].edata['weight']\n",
        "        edge_weight_1 = blocks[1].edata['weight']\n",
        "\n",
        "        num_users = blocks[-1].dstdata[dgl.NID]['user'].shape[0]\n",
        "        num_recipes = blocks[-1].dstdata[dgl.NID]['recipe'].shape[0]\n",
        "\n",
        "        h = self.gatconv1(blocks[0], inputs, edge_weight_0, do_edge_drop)\n",
        "        h = {k: F.relu(v).flatten(2) for k, v in h.items()}\n",
        "        h = {k: self.relation_attention(v) for k, v in h.items()}\n",
        "\n",
        "        first_layer_output = {}\n",
        "        first_layer_output['user'] = h['user'][:num_users]\n",
        "        first_layer_output['recipe'] = h['recipe'][:num_recipes]\n",
        "\n",
        "        h = {key: self.dropout(value) for key, value in h.items()}\n",
        "        h = self.gatconv2(blocks[-1], h, edge_weight_1, do_edge_drop)\n",
        "        last_ingre_and_instr = h['recipe'].flatten(2)\n",
        "        h = {k: self.relation_attention(v.flatten(2)) for k, v in h.items()}\n",
        "\n",
        "        return h\n",
        "\n",
        "#         # combine several layer embs as the final emb\n",
        "#         combined_output = {}\n",
        "#         combined_output['user'] = torch.cat([h['user'], first_layer_output['user']], dim=1)\n",
        "#         combined_output['recipe'] = torch.cat([h['recipe'], first_layer_output['recipe']], dim=1)\n",
        "#         combined_output['user'] = torch.add(h['user'], first_layer_output['user'])\n",
        "#         combined_output['recipe'] = torch.add(h['recipe'], first_layer_output['recipe'])\n",
        "#         return combined_output"
      ],
      "metadata": {
        "id": "9lrcoiFRx2OI"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def norm(input, p=1, dim=1, eps=1e-12):\n",
        "    return input / input.norm(p, dim, keepdim=True).clamp(min=eps).expand_as(input)\n",
        "\n",
        "def get_recommendation_loss(pos_score, neg_score):\n",
        "    n = pos_score.shape[0]\n",
        "    return (neg_score.view(n, -1) - pos_score.view(n, -1) + 1).clamp(min=0).mean()\n",
        "\n",
        "def get_contrastive_loss(x1, x2):\n",
        "    temperature = 0.07\n",
        "\n",
        "    # users\n",
        "    x1_user, x2_user = F.normalize(x1['user']), F.normalize(x2['user'])\n",
        "    pos_score_user = torch.mul(x1_user, x2_user).sum(dim=1)\n",
        "    pos_score_user = torch.exp(pos_score_user/temperature)\n",
        "\n",
        "    x2_user_neg = torch.flipud(x2_user)\n",
        "    ttl_score_user = torch.mul(x1_user, x2_user_neg).sum(dim=1)\n",
        "    ttl_score_user = pos_score_user + torch.exp(ttl_score_user/temperature)\n",
        "\n",
        "    contrastive_loss_user = - torch.log(pos_score_user/ttl_score_user).mean()\n",
        "    # print('contrastive_loss_user: ', contrastive_loss_user)\n",
        "    assert not math.isnan(contrastive_loss_user)\n",
        "\n",
        "\n",
        "    # recipes\n",
        "    x1_recipe, x2_recipe = F.normalize(x1['recipe']), F.normalize(x2['recipe'])\n",
        "    pos_score_recipe = torch.mul(x1_recipe, x2_recipe).sum(dim=1)\n",
        "    pos_score_recipe = torch.exp(pos_score_recipe/temperature)\n",
        "\n",
        "    x2_recipe_neg = torch.flipud(x2_recipe)\n",
        "    ttl_score_recipe = torch.mul(x1_recipe, x2_recipe_neg).sum(dim=1)\n",
        "    ttl_score_recipe = pos_score_recipe + torch.exp(ttl_score_recipe/temperature) #.sum(dim=1)\n",
        "\n",
        "    contrastive_loss_recipe = - torch.log(pos_score_recipe/ttl_score_recipe).mean()\n",
        "    # print('contrastive_loss_recipe: ', contrastive_loss_recipe)\n",
        "\n",
        "    return contrastive_loss_user + contrastive_loss_recipe\n",
        "\n",
        "def get_emb_loss(*params):\n",
        "    out = None\n",
        "    for param in params:\n",
        "        for k,v in param.items():\n",
        "            if out == None:\n",
        "                out = (v**2/2).mean()\n",
        "            else:\n",
        "                out += (v**2/2).mean()\n",
        "    return out"
      ],
      "metadata": {
        "id": "3psARw6mx4AJ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## New GNN (HAN)"
      ],
      "metadata": {
        "id": "CVZCY7dkThZS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import dgl\n",
        "import dgl.function as fn\n",
        "from dgl.nn.pytorch import GATConv\n",
        "\n",
        "\n",
        "class SemanticAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Semantic attention layer for HAN.\n",
        "    This layer computes attention weights across different meta-path based neighborhoods.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_size, hidden_size=128):\n",
        "        super(SemanticAttention, self).__init__()\n",
        "        self.project = nn.Sequential(\n",
        "            nn.Linear(in_size, hidden_size),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_size, 1, bias=False)\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            z: Meta-path based node embeddings, shape [num_nodes, num_metapaths, in_size]\n",
        "        Returns:\n",
        "            Attentional node embeddings, shape [num_nodes, in_size]\n",
        "        \"\"\"\n",
        "        w = self.project(z)  # [num_nodes, num_metapaths, 1]\n",
        "        beta = torch.softmax(w, dim=1)  # [num_nodes, num_metapaths, 1]\n",
        "        return (beta * z).sum(1)  # [num_nodes, in_size]\n",
        "\n",
        "\n",
        "class HANLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    HAN layer for a specific node type.\n",
        "    This layer computes node embeddings for a specific node type using\n",
        "    meta-path based neighbors and attention mechanism.\n",
        "    \"\"\"\n",
        "    def __init__(self, meta_paths, in_size, out_size, layer_num_heads, dropout):\n",
        "        super(HANLayer, self).__init__()\n",
        "\n",
        "        # Meta-path GNN for each meta path\n",
        "        self.gat_layers = nn.ModuleList()\n",
        "        for i in range(len(meta_paths)):\n",
        "            self.gat_layers.append(GATConv(\n",
        "                in_size, out_size // layer_num_heads,\n",
        "                layer_num_heads, dropout, dropout, activation=F.elu,\n",
        "                allow_zero_in_degree=True))\n",
        "\n",
        "        # Semantic attention layer\n",
        "        self.semantic_attention = SemanticAttention(out_size)\n",
        "        self.meta_paths = meta_paths\n",
        "        self.in_size = in_size\n",
        "        self.out_size = out_size\n",
        "\n",
        "    def forward(self, g, h):\n",
        "        semantic_embeddings = []\n",
        "\n",
        "        # For each meta-path\n",
        "        for i, meta_path in enumerate(self.meta_paths):\n",
        "            # Get the subgraph for this meta-path\n",
        "            meta_path_g = dgl.metapath_reachable_graph(g, meta_path)\n",
        "            # Apply GAT on the subgraph\n",
        "            h_gat = self.gat_layers[i](meta_path_g, h).flatten(1)\n",
        "            semantic_embeddings.append(h_gat)\n",
        "\n",
        "        # Stack embeddings from all meta-paths\n",
        "        semantic_embeddings = torch.stack(semantic_embeddings, dim=1)  # [num_nodes, num_metapaths, out_size]\n",
        "\n",
        "        # Apply semantic-level attention\n",
        "        return self.semantic_attention(semantic_embeddings)  # [num_nodes, out_size]\n",
        "\n",
        "\n",
        "class HAN(nn.Module):\n",
        "    \"\"\"\n",
        "    Heterogeneous Attention Network (HAN)\n",
        "    \"\"\"\n",
        "    def __init__(self, g, node_features, meta_paths_dict, hidden_size, out_size, num_heads, dropout=0.1):\n",
        "        super(HAN, self).__init__()\n",
        "\n",
        "        self.g = g\n",
        "\n",
        "        # Feature projection for each node type\n",
        "        self.projection = nn.ModuleDict({\n",
        "            ntype: nn.Linear(features.shape[1], hidden_size)\n",
        "            for ntype, features in node_features.items()\n",
        "        })\n",
        "\n",
        "        # Node type specific HAN layers\n",
        "        self.layers = nn.ModuleDict({\n",
        "            ntype: HANLayer(meta_paths, hidden_size, out_size, num_heads, dropout)\n",
        "            for ntype, meta_paths in meta_paths_dict.items()\n",
        "        })\n",
        "\n",
        "        # Output normalization\n",
        "        self.norms = nn.ModuleDict({\n",
        "            ntype: nn.LayerNorm(out_size)\n",
        "            for ntype in meta_paths_dict.keys()\n",
        "        })\n",
        "\n",
        "    def forward(self, h_dict=None):\n",
        "        if h_dict is None:\n",
        "            # Use node features from the graph if not provided\n",
        "            h_dict = {ntype: self.g.nodes[ntype].data['feat'] for ntype in self.g.ntypes}\n",
        "\n",
        "        # Feature projection\n",
        "        for ntype in h_dict:\n",
        "            if ntype in self.projection:\n",
        "                h_dict[ntype] = self.projection[ntype](h_dict[ntype])\n",
        "\n",
        "        # Apply HAN layers for each node type\n",
        "        out_dict = {}\n",
        "        for ntype, layer in self.layers.items():\n",
        "            out_dict[ntype] = self.norms[ntype](layer(self.g, h_dict[ntype]))\n",
        "\n",
        "        return out_dict\n",
        "\n",
        "\n",
        "class SimpleHAN(nn.Module):\n",
        "    \"\"\"\n",
        "    A simplified version of HAN that doesn't use the full DGL metapath machinery,\n",
        "    but instead handles the mini-batch processing with blocks correctly.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_sizes, hidden_size=128, out_size=128, num_heads=4, dropout=0.1):\n",
        "        super(SimpleHAN, self).__init__()\n",
        "\n",
        "        # Feature projection for each node type\n",
        "        self.projections = nn.ModuleDict({\n",
        "            'user': nn.Linear(in_sizes['user'], hidden_size),\n",
        "            'recipe': nn.Linear(in_sizes['recipe'], hidden_size),\n",
        "            'ingredient': nn.Linear(in_sizes['ingredient'], hidden_size)\n",
        "        })\n",
        "\n",
        "        # Node-level attention for each relation type\n",
        "        self.node_attentions = nn.ModuleDict({\n",
        "            'u-r': GATConv(hidden_size, hidden_size // num_heads, num_heads, dropout, dropout, activation=F.elu, allow_zero_in_degree=True),\n",
        "            'r-u': GATConv(hidden_size, hidden_size // num_heads, num_heads, dropout, dropout, activation=F.elu, allow_zero_in_degree=True),\n",
        "            'r-i': GATConv(hidden_size, hidden_size // num_heads, num_heads, dropout, dropout, activation=F.elu, allow_zero_in_degree=True),\n",
        "            'i-r': GATConv(hidden_size, hidden_size // num_heads, num_heads, dropout, dropout, activation=F.elu, allow_zero_in_degree=True),\n",
        "            'r-r': GATConv(hidden_size, hidden_size // num_heads, num_heads, dropout, dropout, activation=F.elu, allow_zero_in_degree=True),\n",
        "            'i-i': GATConv(hidden_size, hidden_size // num_heads, num_heads, dropout, dropout, activation=F.elu, allow_zero_in_degree=True)\n",
        "        })\n",
        "\n",
        "        # Semantic attention for each node type\n",
        "        self.semantic_attentions = nn.ModuleDict({\n",
        "            'user': SemanticAttention(hidden_size),\n",
        "            'recipe': SemanticAttention(hidden_size),\n",
        "            'ingredient': SemanticAttention(hidden_size)\n",
        "        })\n",
        "\n",
        "        # Output projections\n",
        "        self.output_projections = nn.ModuleDict({\n",
        "            'user': nn.Linear(hidden_size, out_size),\n",
        "            'recipe': nn.Linear(hidden_size, out_size),\n",
        "            'ingredient': nn.Linear(hidden_size, out_size)\n",
        "        })\n",
        "\n",
        "        # Normalization layers\n",
        "        self.norms = nn.ModuleDict({\n",
        "            'user': nn.LayerNorm(out_size),\n",
        "            'recipe': nn.LayerNorm(out_size),\n",
        "            'ingredient': nn.LayerNorm(out_size)\n",
        "        })\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, blocks, features_dict, do_edge_drop=False):\n",
        "        \"\"\"\n",
        "        Apply SimpleHAN to input blocks and features.\n",
        "\n",
        "        Args:\n",
        "            blocks: List of DGL blocks (sampled subgraphs for mini-batch training)\n",
        "            features_dict: Dictionary of node features for each node type\n",
        "            do_edge_drop: Whether to perform edge dropout during training\n",
        "\n",
        "        Returns:\n",
        "            Dictionary of output embeddings for each node type\n",
        "        \"\"\"\n",
        "        # Project input features\n",
        "        h_dict = {}\n",
        "        for ntype, features in features_dict.items():\n",
        "            h_dict[ntype] = F.relu(self.projections[ntype](features))\n",
        "            h_dict[ntype] = self.dropout(h_dict[ntype])\n",
        "\n",
        "        # First block processing - prepare dictionary that maps relation types to embeddings\n",
        "        first_block = blocks[0]\n",
        "        rel_embeddings = {ntype: [] for ntype in first_block.dsttypes}\n",
        "\n",
        "        # Process each block and each relation type\n",
        "        dst_node_dict = {}\n",
        "        for stype, etype, dtype in first_block.canonical_etypes:\n",
        "            if first_block.number_of_edges((stype, etype, dtype)) > 0:\n",
        "                # Extract subgraph for this relation\n",
        "                rel_g = first_block[stype, etype, dtype]\n",
        "\n",
        "                # Get source features relevant to this relation\n",
        "                src_feat = h_dict[stype]\n",
        "                if rel_g.srcdata[dgl.NID].shape[0] < src_feat.shape[0]:\n",
        "                    # If the block has fewer source nodes than features, index the features\n",
        "                    src_indices = rel_g.srcdata[dgl.NID]\n",
        "                    src_feat = src_feat[src_indices]\n",
        "\n",
        "                # Process with attention\n",
        "                try:\n",
        "                    h_rel = self.node_attentions[etype](rel_g, src_feat).flatten(1)\n",
        "                    if dtype not in rel_embeddings:\n",
        "                        rel_embeddings[dtype] = []\n",
        "                    rel_embeddings[dtype].append(h_rel)\n",
        "\n",
        "                    # Store destination node IDs\n",
        "                    dst_node_dict[dtype] = rel_g.dstdata[dgl.NID]\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing {stype}-{etype}-{dtype}: {e}\")\n",
        "                    print(f\"Source features shape: {src_feat.shape}\")\n",
        "                    print(f\"Number of nodes in rel_g: src={rel_g.number_of_src_nodes()}, dst={rel_g.number_of_dst_nodes()}\")\n",
        "                    raise\n",
        "\n",
        "        # Apply semantic attention for each node type to get first layer embeddings\n",
        "        node_embeddings = {}\n",
        "        for ntype, embs in rel_embeddings.items():\n",
        "            if embs:\n",
        "                if len(embs) > 1:\n",
        "                    # If we have multiple relation types leading to this node type, use semantic attention\n",
        "                    stacked_embs = torch.stack(embs, dim=1)\n",
        "                    node_embeddings[ntype] = self.semantic_attentions[ntype](stacked_embs)\n",
        "                else:\n",
        "                    # If only one relation type, just use that\n",
        "                    node_embeddings[ntype] = embs[0]\n",
        "\n",
        "        # Process second block\n",
        "        second_block = blocks[1]\n",
        "        rel_embeddings_2 = {ntype: [] for ntype in second_block.dsttypes}\n",
        "\n",
        "        # Similar processing for second block\n",
        "        for stype, etype, dtype in second_block.canonical_etypes:\n",
        "            if second_block.number_of_edges((stype, etype, dtype)) > 0 and stype in node_embeddings:\n",
        "                rel_g = second_block[stype, etype, dtype]\n",
        "\n",
        "                # Get embeddings from first layer for source nodes\n",
        "                src_emb = node_embeddings[stype]\n",
        "                if rel_g.srcdata[dgl.NID].shape[0] < src_emb.shape[0]:\n",
        "                    src_indices = rel_g.srcdata[dgl.NID]\n",
        "                    src_emb = src_emb[src_indices]\n",
        "\n",
        "                try:\n",
        "                    h_rel = self.node_attentions[etype](rel_g, src_emb).flatten(1)\n",
        "                    if dtype not in rel_embeddings_2:\n",
        "                        rel_embeddings_2[dtype] = []\n",
        "                    rel_embeddings_2[dtype].append(h_rel)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing second layer {stype}-{etype}-{dtype}: {e}\")\n",
        "                    print(f\"Source embeddings shape: {src_emb.shape}\")\n",
        "                    print(f\"Number of nodes in rel_g: src={rel_g.number_of_src_nodes()}, dst={rel_g.number_of_dst_nodes()}\")\n",
        "                    raise\n",
        "\n",
        "        # Final embeddings with semantic attention\n",
        "        output_embeddings = {}\n",
        "        for ntype, embs in rel_embeddings_2.items():\n",
        "            if embs:\n",
        "                if len(embs) > 1:\n",
        "                    stacked_embs = torch.stack(embs, dim=1)\n",
        "                    ntype_emb = self.semantic_attentions[ntype](stacked_embs)\n",
        "                else:\n",
        "                    ntype_emb = embs[0]\n",
        "                output_embeddings[ntype] = self.norms[ntype](self.output_projections[ntype](ntype_emb))\n",
        "            elif ntype in node_embeddings:\n",
        "                # If this node type doesn't receive messages in the second layer, use the first layer embedding\n",
        "                output_embeddings[ntype] = self.norms[ntype](self.output_projections[ntype](node_embeddings[ntype]))\n",
        "\n",
        "        return output_embeddings\n",
        "\n",
        "\n",
        "class HANModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Full HAN model for recommendation task, integrating SimpleHAN with other components\n",
        "    \"\"\"\n",
        "    def __init__(self, in_sizes, hidden_size=128, out_size=128, num_heads=4, dropout=0.1):\n",
        "        super(HANModel, self).__init__()\n",
        "\n",
        "        # Feature projection\n",
        "        self.user_embedding = nn.Sequential(\n",
        "            nn.Linear(in_sizes['user'], hidden_size),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.instr_embedding = nn.Sequential(\n",
        "            nn.Linear(in_sizes['recipe'], hidden_size),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.ingredient_embedding = nn.Sequential(\n",
        "            nn.Linear(in_sizes['ingredient'], hidden_size),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # HAN encoder\n",
        "        self.han = SimpleHAN(in_sizes, hidden_size, out_size, num_heads, dropout)\n",
        "\n",
        "        # SetTransformer for ingredient aggregation\n",
        "        self.setTransformer = SetTransformer()\n",
        "\n",
        "        # Combine recipe embeddings\n",
        "        self.recipe_combine2out = nn.Sequential(\n",
        "            nn.Linear(out_size, out_size),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Score predictor\n",
        "        self.pred = ScorePredictor()\n",
        "\n",
        "    def forward(self, positive_graph, negative_graph, blocks, input_features):\n",
        "        user, instr, ingredient, ingredient_of_dst_recipe = input_features\n",
        "\n",
        "        # Project features\n",
        "        user_emb = self.user_embedding(user)\n",
        "        user_emb = norm(user_emb)\n",
        "        instr_emb = self.instr_embedding(instr)\n",
        "        instr_emb = norm(instr_emb)\n",
        "        ingredient_emb = self.ingredient_embedding(ingredient)\n",
        "        ingredient_emb = norm(ingredient_emb)\n",
        "\n",
        "        # Main HAN encoding\n",
        "        x = self.han(blocks, {'user': user_emb, 'recipe': instr_emb, 'ingredient': ingredient_emb})\n",
        "\n",
        "        # Contrastive learning part 1\n",
        "        user1 = node_drop(user, 0.1, self.training)\n",
        "        instr1 = node_drop(instr, 0.1, self.training)\n",
        "        ingredient1 = node_drop(ingredient, 0.1, self.training)\n",
        "\n",
        "        user1 = self.user_embedding(user1)\n",
        "        user1 = norm(user1)\n",
        "        instr1 = self.instr_embedding(instr1)\n",
        "        instr1 = norm(instr1)\n",
        "        ingredient1 = self.ingredient_embedding(ingredient1)\n",
        "        ingredient1 = norm(ingredient1)\n",
        "\n",
        "        x1 = self.han(blocks, {'user': user1, 'recipe': instr1, 'ingredient': ingredient1})\n",
        "\n",
        "        # Contrastive learning part 2\n",
        "        user2 = node_drop(user, 0.1, self.training)\n",
        "        instr2 = node_drop(instr, 0.1, self.training)\n",
        "        ingredient2 = node_drop(ingredient, 0.1, self.training)\n",
        "\n",
        "        user2 = self.user_embedding(user2)\n",
        "        user2 = norm(user2)\n",
        "        instr2 = self.instr_embedding(instr2)\n",
        "        instr2 = norm(instr2)\n",
        "        ingredient2 = self.ingredient_embedding(ingredient2)\n",
        "        ingredient2 = norm(ingredient2)\n",
        "\n",
        "        x2 = self.han(blocks, {'user': user2, 'recipe': instr2, 'ingredient': ingredient2})\n",
        "\n",
        "        # Set Transformer for ingredient aggregation\n",
        "        all_ingre_emb_for_each_recipe = get_ingredient_neighbors_all_embeddings(blocks, blocks[1].dstdata['_ID']['recipe'], ingredient_of_dst_recipe)\n",
        "        all_ingre_emb_for_each_recipe = norm(all_ingre_emb_for_each_recipe)\n",
        "        total_ingre_emb = self.setTransformer(all_ingre_emb_for_each_recipe)\n",
        "        total_ingre_emb = norm(total_ingre_emb)\n",
        "\n",
        "        # Combine recipe embeddings\n",
        "        x['recipe'] = self.recipe_combine2out(total_ingre_emb.add(x['recipe']))\n",
        "\n",
        "        # Calculate scores\n",
        "        pos_score = self.pred(positive_graph, x)\n",
        "        neg_score = self.pred(negative_graph, x)\n",
        "\n",
        "        return pos_score, neg_score, x1, x2\n",
        "\n",
        "class HANNetwork(nn.Module):\n",
        "    \"\"\"\n",
        "    HAN-based replacement for the GNN class.\n",
        "    This class maintains compatibility with your existing model structure.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_feats, hid_feats, out_feats, rel_names):\n",
        "        super(HANNetwork, self).__init__()\n",
        "\n",
        "        self.hidden_size = hid_feats\n",
        "        self.out_feats = out_feats\n",
        "        self.num_heads = 4\n",
        "\n",
        "        # Feature projection\n",
        "        self.projections = nn.ModuleDict({\n",
        "            'user': nn.Linear(in_feats, hid_feats),\n",
        "            'recipe': nn.Linear(in_feats, hid_feats),\n",
        "            'ingredient': nn.Linear(in_feats, hid_feats)\n",
        "        })\n",
        "\n",
        "        # First-layer attention\n",
        "        self.attention_layer1 = nn.ModuleDict()\n",
        "        for rel in rel_names:\n",
        "            self.attention_layer1[rel] = GATConv(\n",
        "                hid_feats, hid_feats // self.num_heads,\n",
        "                self.num_heads, 0.1, 0.1,\n",
        "                activation=F.elu, allow_zero_in_degree=True\n",
        "            )\n",
        "\n",
        "        # Second-layer attention\n",
        "        self.attention_layer2 = nn.ModuleDict()\n",
        "        for rel in rel_names:\n",
        "            self.attention_layer2[rel] = GATConv(\n",
        "                hid_feats, out_feats // self.num_heads,\n",
        "                self.num_heads, 0.1, 0.1,\n",
        "                activation=F.elu, allow_zero_in_degree=True\n",
        "            )\n",
        "\n",
        "        # Semantic attention for each node type\n",
        "        self.semantic_attentions = nn.ModuleDict({\n",
        "            'user': SemanticAttention(hid_feats),\n",
        "            'recipe': SemanticAttention(hid_feats),\n",
        "            'ingredient': SemanticAttention(hid_feats)\n",
        "        })\n",
        "\n",
        "        # Output semantic attention\n",
        "        self.out_semantic_attentions = nn.ModuleDict({\n",
        "            'user': SemanticAttention(out_feats),\n",
        "            'recipe': SemanticAttention(out_feats),\n",
        "            'ingredient': SemanticAttention(out_feats)\n",
        "        })\n",
        "\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, blocks, inputs, edge_weights=None, do_edge_drop=None):\n",
        "        \"\"\"\n",
        "        Forward pass compatible with your current GNN structure.\n",
        "\n",
        "        Args:\n",
        "            blocks: List of DGL blocks for mini-batch training\n",
        "            inputs: Dictionary of input features for each node type\n",
        "            edge_weights: Optional edge weights (not used in this implementation)\n",
        "            do_edge_drop: Optional flag for edge dropout (not used in this implementation)\n",
        "\n",
        "        Returns:\n",
        "            Dictionary of output embeddings for each node type\n",
        "        \"\"\"\n",
        "        # First block processing\n",
        "        first_block = blocks[0]\n",
        "        h_dict = {}\n",
        "\n",
        "        # Project features for each node type\n",
        "        for ntype, feats in inputs.items():\n",
        "            if ntype in self.projections:\n",
        "                h_dict[ntype] = F.relu(self.projections[ntype](feats))\n",
        "                h_dict[ntype] = self.dropout(h_dict[ntype])\n",
        "\n",
        "        # First layer processing\n",
        "        rel_embeddings = {ntype: [] for ntype in first_block.dsttypes}\n",
        "\n",
        "        for stype, etype, dtype in first_block.canonical_etypes:\n",
        "            if first_block.number_of_edges((stype, etype, dtype)) > 0 and stype in h_dict:\n",
        "                rel_g = first_block[stype, etype, dtype]\n",
        "\n",
        "                # Get source features for this relation\n",
        "                src_feat = h_dict[stype]\n",
        "                if rel_g.srcdata[dgl.NID].shape[0] < src_feat.shape[0]:\n",
        "                    src_indices = rel_g.srcdata[dgl.NID]\n",
        "                    src_feat = src_feat[src_indices]\n",
        "\n",
        "                # Apply attention\n",
        "                try:\n",
        "                    h_rel = self.attention_layer1[etype](rel_g, src_feat).flatten(1)\n",
        "                    if dtype not in rel_embeddings:\n",
        "                        rel_embeddings[dtype] = []\n",
        "                    rel_embeddings[dtype].append(h_rel)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error in first layer {stype}-{etype}-{dtype}: {e}\")\n",
        "                    print(f\"Source features shape: {src_feat.shape}\")\n",
        "                    print(f\"Relation graph: src={rel_g.number_of_src_nodes()}, dst={rel_g.number_of_dst_nodes()}\")\n",
        "                    raise\n",
        "\n",
        "        # Apply semantic attention to first layer\n",
        "        intermediate_dict = {}\n",
        "        for ntype, embs in rel_embeddings.items():\n",
        "            if embs:\n",
        "                if len(embs) > 1:\n",
        "                    stacked_embs = torch.stack(embs, dim=1)\n",
        "                    intermediate_dict[ntype] = self.semantic_attentions[ntype](stacked_embs)\n",
        "                else:\n",
        "                    intermediate_dict[ntype] = embs[0]\n",
        "\n",
        "        # Second block processing\n",
        "        second_block = blocks[1]\n",
        "        rel_embeddings_2 = {ntype: [] for ntype in second_block.dsttypes}\n",
        "\n",
        "        for stype, etype, dtype in second_block.canonical_etypes:\n",
        "            if second_block.number_of_edges((stype, etype, dtype)) > 0 and stype in intermediate_dict:\n",
        "                rel_g = second_block[stype, etype, dtype]\n",
        "\n",
        "                # Get intermediate embeddings\n",
        "                src_emb = intermediate_dict[stype]\n",
        "                if rel_g.srcdata[dgl.NID].shape[0] < src_emb.shape[0]:\n",
        "                    src_indices = rel_g.srcdata[dgl.NID]\n",
        "                    src_emb = src_emb[src_indices]\n",
        "\n",
        "                try:\n",
        "                    h_rel = self.attention_layer2[etype](rel_g, src_emb).flatten(1)\n",
        "                    if dtype not in rel_embeddings_2:\n",
        "                        rel_embeddings_2[dtype] = []\n",
        "                    rel_embeddings_2[dtype].append(h_rel)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error in second layer {stype}-{etype}-{dtype}: {e}\")\n",
        "                    print(f\"Source embeddings shape: {src_emb.shape}\")\n",
        "                    print(f\"Relation graph: src={rel_g.number_of_src_nodes()}, dst={rel_g.number_of_dst_nodes()}\")\n",
        "                    raise\n",
        "\n",
        "        # Final embeddings\n",
        "        output_dict = {}\n",
        "        for ntype, embs in rel_embeddings_2.items():\n",
        "            if embs:\n",
        "                if len(embs) > 1:\n",
        "                    stacked_embs = torch.stack(embs, dim=1)\n",
        "                    output_dict[ntype] = self.out_semantic_attentions[ntype](stacked_embs)\n",
        "                else:\n",
        "                    output_dict[ntype] = embs[0]\n",
        "            elif ntype in intermediate_dict:\n",
        "                # If no connections in second layer, use first layer\n",
        "                output_dict[ntype] = intermediate_dict[ntype]\n",
        "\n",
        "        return output_dict\n",
        "\n",
        "class RelationAttention(nn.Module):\n",
        "    def __init__(self, in_size, hidden_size=16):\n",
        "        super(RelationAttention, self).__init__()\n",
        "\n",
        "        self.project = nn.Sequential(\n",
        "            nn.Linear(in_size, hidden_size),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_size, 1, bias=False)\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        w = self.project(z).mean(0)                    # (M, 1)\n",
        "        beta = torch.softmax(w, dim=0)                 # (M, 1)\n",
        "        beta = beta.expand((z.shape[0],) + beta.shape) # (N, M, 1)\n",
        "        out = (beta * z).sum(1)                        # (N, D * K)\n",
        "        return out\n",
        "\n",
        "class HANBasedGNN(nn.Module):\n",
        "    \"\"\"\n",
        "    A simplified HAN implementation that directly replaces your GNN class.\n",
        "    This version focuses on compatibility and robustness rather than full HAN capabilities.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_feats, hid_feats, out_feats, rel_names):\n",
        "        super(HANBasedGNN, self).__init__()\n",
        "\n",
        "        self.num_heads = 4\n",
        "        self.hid_feats = int(hid_feats/self.num_heads)\n",
        "        self.out_feats = int(out_feats/self.num_heads)\n",
        "        self.relation_attention = RelationAttention(hid_feats)\n",
        "\n",
        "        # First layer GATConv - one for each relation type\n",
        "        self.gatconv1 = custom_HeteroGraphConv({\n",
        "            'i-r': custom_GATConv(in_feats, self.hid_feats, num_heads=self.num_heads),\n",
        "            'r-i': custom_GATConv(in_feats, self.hid_feats, num_heads=self.num_heads),\n",
        "            'r-r': custom_GATConv(in_feats, self.hid_feats, num_heads=self.num_heads),\n",
        "            'i-i': custom_GATConv(in_feats, self.hid_feats, num_heads=self.num_heads),\n",
        "            'u-r': custom_GATConv(in_feats, self.hid_feats, num_heads=self.num_heads),\n",
        "            'r-u': custom_GATConv(in_feats, self.hid_feats, num_heads=self.num_heads),\n",
        "        }, aggregate='stack')\n",
        "\n",
        "        # Second layer GATConv - one for each relation type\n",
        "        self.gatconv2 = custom_HeteroGraphConv({\n",
        "            'i-r': custom_GATConv(self.hid_feats*self.num_heads, self.out_feats, num_heads=self.num_heads),\n",
        "            'r-i': custom_GATConv(self.hid_feats*self.num_heads, self.out_feats, num_heads=self.num_heads),\n",
        "            'r-r': custom_GATConv(self.hid_feats*self.num_heads, self.out_feats, num_heads=self.num_heads),\n",
        "            'i-i': custom_GATConv(self.hid_feats*self.num_heads, self.out_feats, num_heads=self.num_heads),\n",
        "            'u-r': custom_GATConv(self.hid_feats*self.num_heads, self.out_feats, num_heads=self.num_heads),\n",
        "            'r-u': custom_GATConv(self.hid_feats*self.num_heads, self.out_feats, num_heads=self.num_heads),\n",
        "        }, aggregate='stack')\n",
        "\n",
        "        # Additional semantic attention for each node type\n",
        "        self.semantic_attention = nn.ModuleDict({\n",
        "            'user': RelationAttention(out_feats),\n",
        "            'recipe': RelationAttention(out_feats),\n",
        "            'ingredient': RelationAttention(out_feats)\n",
        "        })\n",
        "\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, blocks, inputs, edge_weight=None, do_edge_drop=None):\n",
        "        \"\"\"\n",
        "        Forward pass that maintains compatibility with your current code.\n",
        "\n",
        "        Args:\n",
        "            blocks: List of DGL blocks\n",
        "            inputs: Dictionary of input features for each node type\n",
        "            edge_weight: Edge weights (optional)\n",
        "            do_edge_drop: Flag for edge dropout (optional)\n",
        "\n",
        "        Returns:\n",
        "            Dictionary of output embeddings for each node type\n",
        "        \"\"\"\n",
        "        # Extract edge weights if provided\n",
        "        if edge_weight is not None:\n",
        "            edge_weight_0 = blocks[0].edata['weight']\n",
        "            edge_weight_1 = blocks[1].edata['weight']\n",
        "        else:\n",
        "            edge_weight_0 = None\n",
        "            edge_weight_1 = None\n",
        "\n",
        "        # Get destination node IDs for final layer\n",
        "        try:\n",
        "            num_users = blocks[-1].dstdata[dgl.NID]['user'].shape[0]\n",
        "            num_recipes = blocks[-1].dstdata[dgl.NID]['recipe'].shape[0]\n",
        "        except:\n",
        "            # Fallback if NID is not available\n",
        "            num_users = blocks[-1].number_of_dst_nodes('user')\n",
        "            num_recipes = blocks[-1].number_of_dst_nodes('recipe')\n",
        "\n",
        "        # First layer GAT + relationship attention\n",
        "        try:\n",
        "            h = self.gatconv1(blocks[0], inputs, edge_weight_0, do_edge_drop)\n",
        "\n",
        "            # Flatten the head dimension and apply relation attention\n",
        "            h = {k: F.relu(v).flatten(2) for k, v in h.items()}\n",
        "            h = {k: self.relation_attention(v) for k, v in h.items()}\n",
        "\n",
        "            # Store first layer outputs for later\n",
        "            first_layer_output = {}\n",
        "            if 'user' in h:\n",
        "                first_layer_output['user'] = h['user'][:num_users]\n",
        "            if 'recipe' in h:\n",
        "                first_layer_output['recipe'] = h['recipe'][:num_recipes]\n",
        "\n",
        "            # Apply dropout before second layer\n",
        "            h = {key: self.dropout(value) for key, value in h.items()}\n",
        "\n",
        "            # Second layer GAT\n",
        "            h = self.gatconv2(blocks[-1], h, edge_weight_1, do_edge_drop)\n",
        "\n",
        "            # Process the output embeddings\n",
        "            output_dict = {}\n",
        "            for ntype, embs in h.items():\n",
        "                # Apply semantic attention across multiple attention heads\n",
        "                flattened_embs = embs.flatten(2)\n",
        "                if ntype in self.semantic_attention:\n",
        "                    output_dict[ntype] = self.semantic_attention[ntype](flattened_embs)\n",
        "                else:\n",
        "                    # Fallback if node type doesn't have semantic attention\n",
        "                    output_dict[ntype] = torch.mean(flattened_embs, dim=1)\n",
        "\n",
        "            return output_dict\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in HANBasedGNN forward pass: {e}\")\n",
        "            # Print debugging information\n",
        "            for i, block in enumerate(blocks):\n",
        "                print(f\"Block {i} info:\")\n",
        "                for ntype in block.ntypes:\n",
        "                    print(f\"  {ntype} nodes: {block.number_of_nodes(ntype)}\")\n",
        "                for canonical_etype in block.canonical_etypes:\n",
        "                    stype, etype, dtype = canonical_etype\n",
        "                    print(f\"  {stype}-{etype}-{dtype} edges: {block.number_of_edges(canonical_etype)}\")\n",
        "\n",
        "            # Also print feature dimensions\n",
        "            print(\"Input features dimensions:\")\n",
        "            for ntype, feat in inputs.items():\n",
        "                print(f\"  {ntype}: {feat.shape}\")\n",
        "\n",
        "            raise"
      ],
      "metadata": {
        "id": "Fx5I3Hx5Tgu0"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Old Model"
      ],
      "metadata": {
        "id": "nFhvWmMyyChU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.user_embedding = nn.Sequential(\n",
        "            nn.Linear(300, 128),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.instr_embedding = nn.Sequential(\n",
        "            nn.Linear(1024, 128),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.ingredient_embedding = nn.Sequential(\n",
        "            nn.Linear(46, 128),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.recipe_combine2out = nn.Sequential(\n",
        "            nn.Linear(128, 128),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.gnn = GNN(128, 128, 128, graph.etypes)\n",
        "        self.pred = ScorePredictor()\n",
        "        self.setTransformer_ = SetTransformer()\n",
        "\n",
        "    def forward(self, positive_graph, negative_graph, blocks, input_features):\n",
        "        user, instr, ingredient, ingredient_of_dst_recipe = input_features\n",
        "\n",
        "        # major GNN\n",
        "        user_major = self.user_embedding(user)\n",
        "        user_major = norm(user_major)\n",
        "        instr_major = self.instr_embedding(instr)\n",
        "        instr_major = norm(instr_major)\n",
        "        ingredient_major = self.ingredient_embedding(ingredient)\n",
        "        ingredient_major = norm(ingredient_major)\n",
        "        x = self.gnn(blocks, {'user': user_major, 'recipe': instr_major, 'ingredient': ingredient_major}, torch.Tensor([[0]]))\n",
        "\n",
        "        # contrastive - 1\n",
        "        user1 = node_drop(user, 0.1, model.training)\n",
        "        instr1 = node_drop(instr, 0.1, model.training)\n",
        "        ingredient1 = node_drop(ingredient, 0.1, model.training)\n",
        "\n",
        "        user1 = self.user_embedding(user1)\n",
        "        user1 = norm(user1)\n",
        "        instr1 = self.instr_embedding(instr1)\n",
        "        instr1 = norm(instr1)\n",
        "        ingredient1 = self.ingredient_embedding(ingredient1)\n",
        "        ingredient1 = norm(ingredient1)\n",
        "\n",
        "        x1 = self.gnn(blocks, {'user': user1, 'recipe': instr1, 'ingredient': ingredient1}, torch.Tensor([[1]]))\n",
        "\n",
        "        # contrastive - 2\n",
        "        user2 = node_drop(user, 0.1, model.training)\n",
        "        instr2 = node_drop(instr, 0.1, model.training)\n",
        "        ingredient2 = node_drop(ingredient, 0.1, model.training)\n",
        "\n",
        "        user2 = self.user_embedding(user2)\n",
        "        user2 = norm(user2)\n",
        "        instr2 = self.instr_embedding(instr2)\n",
        "        instr2 = norm(instr2)\n",
        "        ingredient2 = self.ingredient_embedding(ingredient2)\n",
        "        ingredient2 = norm(ingredient2)\n",
        "\n",
        "        x2 = self.gnn(blocks, {'user': user2, 'recipe': instr2, 'ingredient': ingredient2}, torch.Tensor([[1]]))\n",
        "\n",
        "        # setTransformer\n",
        "        all_ingre_emb_for_each_recipe = get_ingredient_neighbors_all_embeddings(blocks, blocks[1].dstdata['_ID']['recipe'], ingredient_of_dst_recipe)\n",
        "        all_ingre_emb_for_each_recipe = norm(all_ingre_emb_for_each_recipe)\n",
        "        total_ingre_emb = self.setTransformer_(all_ingre_emb_for_each_recipe) # 1\n",
        "        total_ingre_emb = norm(total_ingre_emb)\n",
        "\n",
        "        # scores\n",
        "        x['recipe'] = self.recipe_combine2out(total_ingre_emb.add(x['recipe']))\n",
        "        pos_score = self.pred(positive_graph, x)\n",
        "        neg_score = self.pred(negative_graph, x)\n",
        "\n",
        "        return pos_score, neg_score, x1, x2"
      ],
      "metadata": {
        "id": "BE2V5Ol7x6D1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Losses and helper functions"
      ],
      "metadata": {
        "id": "o0S1h0cBY2uG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def norm(input, p=1, dim=1, eps=1e-12):\n",
        "    return input / input.norm(p, dim, keepdim=True).clamp(min=eps).expand_as(input)\n",
        "\n",
        "def get_recommendation_loss(pos_score, neg_score):\n",
        "    n = pos_score.shape[0]\n",
        "    return (neg_score.view(n, -1) - pos_score.view(n, -1) + 1).clamp(min=0).mean()\n",
        "\n",
        "def get_contrastive_loss(x1, x2):\n",
        "    temperature = 0.07\n",
        "\n",
        "    # users\n",
        "    x1_user, x2_user = F.normalize(x1['user']), F.normalize(x2['user'])\n",
        "    pos_score_user = torch.mul(x1_user, x2_user).sum(dim=1)\n",
        "    pos_score_user = torch.exp(pos_score_user/temperature)\n",
        "\n",
        "    x2_user_neg = torch.flipud(x2_user)\n",
        "    ttl_score_user = torch.mul(x1_user, x2_user_neg).sum(dim=1)\n",
        "    ttl_score_user = pos_score_user + torch.exp(ttl_score_user/temperature)\n",
        "\n",
        "    contrastive_loss_user = - torch.log(pos_score_user/ttl_score_user).mean()\n",
        "    # print('contrastive_loss_user: ', contrastive_loss_user)\n",
        "    assert not math.isnan(contrastive_loss_user)\n",
        "\n",
        "\n",
        "    # recipes\n",
        "    x1_recipe, x2_recipe = F.normalize(x1['recipe']), F.normalize(x2['recipe'])\n",
        "    pos_score_recipe = torch.mul(x1_recipe, x2_recipe).sum(dim=1)\n",
        "    pos_score_recipe = torch.exp(pos_score_recipe/temperature)\n",
        "\n",
        "    x2_recipe_neg = torch.flipud(x2_recipe)\n",
        "    ttl_score_recipe = torch.mul(x1_recipe, x2_recipe_neg).sum(dim=1)\n",
        "    ttl_score_recipe = pos_score_recipe + torch.exp(ttl_score_recipe/temperature) #.sum(dim=1)\n",
        "\n",
        "    contrastive_loss_recipe = - torch.log(pos_score_recipe/ttl_score_recipe).mean()\n",
        "    # print('contrastive_loss_recipe: ', contrastive_loss_recipe)\n",
        "\n",
        "    return contrastive_loss_user + contrastive_loss_recipe\n",
        "\n",
        "def get_emb_loss(*params):\n",
        "    out = None\n",
        "    for param in params:\n",
        "        for k,v in param.items():\n",
        "            if out == None:\n",
        "                out = (v**2/2).mean()\n",
        "            else:\n",
        "                out += (v**2/2).mean()\n",
        "    return out\n",
        "\n",
        "def print_block_info(blocks):\n",
        "    \"\"\"\n",
        "    Print detailed information about the structure of blocks for debugging.\n",
        "    \"\"\"\n",
        "    print(\"\\n----- Block Information -----\")\n",
        "    for i, block in enumerate(blocks):\n",
        "        print(f\"\\nBlock {i}:\")\n",
        "        print(f\"  Number of src nodes: {block.number_of_src_nodes()}\")\n",
        "        print(f\"  Number of dst nodes: {block.number_of_dst_nodes()}\")\n",
        "\n",
        "        print(\"  Node types:\")\n",
        "        for ntype in block.ntypes:\n",
        "            print(f\"    {ntype}: {block.number_of_nodes(ntype)} nodes\")\n",
        "            if ntype in block.srcdata:\n",
        "                for key in block.srcdata[ntype]:\n",
        "                    print(f\"      srcdata[{ntype}][{key}] shape: {block.srcdata[ntype][key].shape}\")\n",
        "            if ntype in block.dstdata:\n",
        "                for key in block.dstdata[ntype]:\n",
        "                    print(f\"      dstdata[{ntype}][{key}] shape: {block.dstdata[ntype][key].shape}\")\n",
        "\n",
        "        print(\"  Edge types:\")\n",
        "        for stype, etype, dtype in block.canonical_etypes:\n",
        "            num_edges = block.number_of_edges((stype, etype, dtype))\n",
        "            print(f\"    {stype}-{etype}-{dtype}: {num_edges} edges\")\n",
        "            if num_edges > 0:\n",
        "                print(f\"      src_nodes: {block.srcnodes(stype).shape}\")\n",
        "                print(f\"      dst_nodes: {block.dstnodes(dtype).shape}\")\n",
        "    print(\"-----------------------------\\n\")\n",
        "\n",
        "\n",
        "def print_feature_info(features_dict):\n",
        "    \"\"\"\n",
        "    Print information about feature tensors.\n",
        "    \"\"\"\n",
        "    print(\"\\n----- Feature Information -----\")\n",
        "    for ntype, features in features_dict.items():\n",
        "        print(f\"  {ntype} features: shape={features.shape}, dtype={features.dtype}\")\n",
        "        print(f\"    min={features.min().item()}, max={features.max().item()}, mean={features.mean().item()}\")\n",
        "        print(f\"    has_nan={torch.isnan(features).any().item()}, has_inf={torch.isinf(features).any().item()}\")\n",
        "    print(\"-----------------------------\\n\")"
      ],
      "metadata": {
        "id": "EETk7eIhY6HG"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Metrics and evaluation methods"
      ],
      "metadata": {
        "id": "WcqNHAdkx-Db"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ScorePredictor(nn.Module):\n",
        "    def forward(self, edge_subgraph, x):\n",
        "        with edge_subgraph.local_scope():\n",
        "            edge_subgraph.ndata['x'] = x\n",
        "            edge_subgraph.apply_edges(dgl.function.u_dot_v('x', 'x', 'score'), etype='u-r')\n",
        "            return edge_subgraph.edata['score'][('user', 'u-r', 'recipe')].squeeze()\n",
        "\n",
        "def compute_auc(pos_score, neg_score):\n",
        "    scores = torch.cat([pos_score, neg_score]).cpu().numpy()\n",
        "    labels = torch.cat([torch.ones(pos_score.shape[0]), torch.zeros(neg_score.shape[0])]).cpu().numpy()\n",
        "    return roc_auc_score(labels, scores)\n",
        "\n",
        "def precision_at_k(r, k):\n",
        "    # Relevance is binary (nonzero is relevant).\n",
        "    assert k >= 1\n",
        "    r = np.asarray(r)[:k]\n",
        "    return np.mean(r)\n",
        "\n",
        "def recall_at_k(r, k, all_pos_num):\n",
        "    r = np.asarray(r, dtype=float)[:k]  # Changed from np.asfarray\n",
        "    return np.sum(r) / all_pos_num\n",
        "\n",
        "def dcg_at_k(r, k, method=0):\n",
        "    r = np.asarray(r, dtype=float)[:k]  # Changed from np.asfarray\n",
        "    # Rest of function unchanged\n",
        "    if r.size:\n",
        "        if method == 0:\n",
        "            return r[0] + np.sum(r[1:] / np.log2(np.arange(2, r.size + 1)))\n",
        "        elif method == 1:\n",
        "            return np.sum(r / np.log2(np.arange(2, r.size + 2)))\n",
        "        else:\n",
        "            raise ValueError('method must be 0 or 1.')\n",
        "    return 0.\n",
        "\n",
        "\n",
        "def average_precision_at_k(r, Ks):\n",
        "    r = np.asarray(r) != 0\n",
        "    out = []\n",
        "    for k in Ks:\n",
        "        assert k <= len(r)\n",
        "        # print('[precision_at_k(r, i + 1) for i in range(k) if r[i]]: ', [precision_at_k(r, i + 1) for i in range(k) if r[i]])\n",
        "        all_precision_before_k = [precision_at_k(r, i + 1) for i in range(k) if r[i]]\n",
        "        if len(all_precision_before_k) == 0:\n",
        "            all_precision_before_k = [0]\n",
        "        out.append(np.mean(all_precision_before_k))\n",
        "    if not out:\n",
        "        return 0.\n",
        "    # return np.array([np.mean(out)])\n",
        "    return np.array(out)\n",
        "\n",
        "def get_map_at_k(rs, Ks):\n",
        "    # examples:\n",
        "    # average_precision_at_k([1, 1, 0, 1, 0, 1, 0, 0, 0, 1], [5,10])\n",
        "    # average_precision_at_k([0, 0, 1, 0, 0, 0, 0, 0, 0, 1], [5,10])\n",
        "    # get_map_at_k([[1, 1, 0, 1, 0, 1, 0, 0, 0, 1,1], [0, 0, 1, 0, 0, 0, 0, 0, 0, 1,1]], [5,10])\n",
        "    out = np.zeros(len(Ks))\n",
        "    for r in rs:\n",
        "        # print('average_precision_at_k: ', average_precision_at_k(r, Ks))\n",
        "        out += average_precision_at_k(r, Ks)/len(rs)\n",
        "    return out\n",
        "    # return np.mean([average_precision(r) for r in rs])\n",
        "\n",
        "def get_ranklist_for_one_user(user_poss, user_negs, Ks):\n",
        "    item_scores = {}\n",
        "    n_pos = len(user_poss)\n",
        "    n_neg = len(user_negs)\n",
        "    for i in range(n_pos):\n",
        "        item_scores[i] = user_poss[i]\n",
        "    for i in range(n_neg):\n",
        "        item_scores[i+1] = user_negs[i]\n",
        "\n",
        "    K_max = max(Ks)\n",
        "    K_max_item_score = heapq.nlargest(K_max, item_scores, key=item_scores.get)\n",
        "\n",
        "    r = []\n",
        "    for i in K_max_item_score:\n",
        "        if i < n_pos:\n",
        "            r.append(1)\n",
        "        else:\n",
        "            r.append(0)\n",
        "    return r\n",
        "\n",
        "def hit_at_k(r, k):\n",
        "    r = np.array(r)[:k]\n",
        "    if np.sum(r) > 0:\n",
        "        return 1.\n",
        "    else:\n",
        "        return 0.\n",
        "\n",
        "def get_mrr(rs):\n",
        "    rs = (np.asarray(r).nonzero()[0] for r in rs)\n",
        "    return np.mean([1. / (r[0] + 1) if r.size else 0. for r in rs])\n",
        "\n",
        "\n",
        "def get_performance_one_user(user_poss, user_negs, Ks):\n",
        "    r = get_ranklist_for_one_user(user_poss, user_negs, Ks)\n",
        "\n",
        "    precision, recall, ndcg, hit_ratio = [], [], [], []\n",
        "    for K in Ks:\n",
        "        precision.append(precision_at_k(r, K))\n",
        "        # recall.append(recall_at_k(r, K, len(user_poss)))\n",
        "        ndcg.append(ndcg_at_k(r, K))\n",
        "        hit_ratio.append(hit_at_k(r, K))\n",
        "    # return {'precision': np.array(precision), 'recall': np.array(recall),\n",
        "    #         'ndcg': np.array(ndcg), 'hit_ratio': np.array(hit_ratio)}, r\n",
        "    return {'precision': np.array(precision),\n",
        "            'ndcg': np.array(ndcg), 'hit_ratio': np.array(hit_ratio)}, r\n",
        "\n",
        "\n",
        "def get_performance_all_users(user2pos_score_dict, user2neg_score_dict, Ks):\n",
        "    # all_result = {'precision': np.zeros(len(Ks)), 'recall': np.zeros(len(Ks)), 'ndcg': np.zeros(len(Ks)),\n",
        "    #           'hit_ratio': np.zeros(len(Ks))}\n",
        "    all_result = {'hit_ratio': np.zeros(len(Ks)), 'ndcg': np.zeros(len(Ks)), 'precision': np.zeros(len(Ks))}\n",
        "\n",
        "    rs = []\n",
        "    n_test_users = len(user2pos_score_dict)\n",
        "\n",
        "    # one specific user\n",
        "    for user in user2pos_score_dict.keys():\n",
        "        user_pos_score = user2pos_score_dict[user]\n",
        "        user_neg_score = user2neg_score_dict[user]\n",
        "        one_result, one_r = get_performance_one_user(user_pos_score, user_neg_score, Ks)\n",
        "        # all_result['recall'] += one_result['recall']/n_test_users\n",
        "        all_result['hit_ratio'] += one_result['hit_ratio']/n_test_users\n",
        "        all_result['ndcg'] += one_result['ndcg']/n_test_users\n",
        "        all_result['precision'] += one_result['precision']/n_test_users\n",
        "        rs.append(one_r)\n",
        "\n",
        "    # get MRR\n",
        "    # MRR = get_mrr(rs)\n",
        "    # all_result['MRR'] = MRR\n",
        "\n",
        "    # get MAP\n",
        "    MAP = get_map_at_k(rs, Ks)\n",
        "    all_result['MAP'] = MAP\n",
        "\n",
        "    return all_result\n",
        "\n",
        "\n",
        "def evaluate(model, dataloader, multi_metrics=False):\n",
        "    # print('start evaluating ...')\n",
        "    evaluate_start = time.time()\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    epoch_contrastive_loss = 0\n",
        "    iteration_cnt = 0\n",
        "    total_pos_score = torch.tensor([]).to(device)\n",
        "    total_neg_score = torch.tensor([]).to(device)\n",
        "\n",
        "    # for evaluation\n",
        "    user2pos_score_dict = {}\n",
        "    user2neg_score_dict = {}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for input_nodes, positive_graph, negative_graph, blocks in dataloader:\n",
        "            blocks = [b.to(device) for b in blocks]\n",
        "            positive_graph = positive_graph.to(device)\n",
        "            negative_graph = negative_graph.to(device)\n",
        "\n",
        "            input_user = blocks[0].srcdata['random_feature']['user']\n",
        "            input_instr = blocks[0].srcdata['avg_instr_feature']['recipe']\n",
        "            input_ingredient = blocks[0].srcdata['nutrient_feature']['ingredient']\n",
        "            ingredient_of_dst_recipe = blocks[1].srcdata['nutrient_feature']['ingredient']\n",
        "            input_features = [input_user, input_instr, input_ingredient, ingredient_of_dst_recipe]\n",
        "\n",
        "            pos_score, neg_score, x1, x2 = model(positive_graph, negative_graph, blocks, input_features)\n",
        "            contrastive_loss = get_contrastive_loss(x1, x2)\n",
        "            total_pos_score = torch.cat([total_pos_score, pos_score])\n",
        "            total_neg_score = torch.cat([total_neg_score, neg_score])\n",
        "\n",
        "            recommendation_loss = get_recommendation_loss(pos_score, neg_score)\n",
        "            loss = recommendation_loss # + 0.01 * contrastive_loss\n",
        "            total_loss += recommendation_loss.item()\n",
        "            epoch_contrastive_loss += contrastive_loss.item()\n",
        "            iteration_cnt += 1\n",
        "\n",
        "            # for evaluation\n",
        "            global_test_users = blocks[1].dstdata['_ID']['user'] # we need to map the user id in subgraph to the whole graph\n",
        "            test_users, test_recipes = positive_graph.edges(etype='u-r')\n",
        "            test_users = test_users.tolist()\n",
        "            test_recipes = test_recipes.tolist()\n",
        "            for index in range(len(test_users)):\n",
        "                test_u = int(global_test_users[test_users[index]])\n",
        "                test_r = int(test_recipes[index])\n",
        "                test_score = float(pos_score[index])\n",
        "\n",
        "                if test_u not in user2pos_score_dict:\n",
        "                    user2pos_score_dict[test_u] = []\n",
        "                user2pos_score_dict[test_u].append(test_score)\n",
        "\n",
        "                if test_u not in user2neg_score_dict:\n",
        "                    user2neg_score_dict[test_u] = neg_score[index*n_test_negs:(index+1)*n_test_negs]\n",
        "\n",
        "            # break\n",
        "\n",
        "        total_loss /= iteration_cnt\n",
        "        epoch_contrastive_loss /= iteration_cnt\n",
        "\n",
        "        # metrics\n",
        "        auc = compute_auc(total_pos_score, total_neg_score)\n",
        "        if multi_metrics:\n",
        "            # evaluation_result = get_performance_all_users(total_pos_score, total_neg_score, Ks)\n",
        "            evaluation_result = get_performance_all_users(user2pos_score_dict, user2neg_score_dict, Ks)\n",
        "            evaluation_result['AUC'] = auc\n",
        "            print('evaluation_result: ', evaluation_result)\n",
        "            print('epoch_contrastive_loss: ', epoch_contrastive_loss)\n",
        "        else:\n",
        "            print('AUC: ', auc)\n",
        "\n",
        "        evalutate_time = time.strftime(\"%M:%S min\", time.gmtime(time.time()-evaluate_start))\n",
        "        print('evalutate_time: ', evalutate_time)\n",
        "    return total_loss"
      ],
      "metadata": {
        "id": "uvPEEjPGx8P8"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## New Model"
      ],
      "metadata": {
        "id": "wkwmA7li2PBy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import dgl\n",
        "import dgl.function as fn\n",
        "from dgl.nn.pytorch import GATConv\n",
        "from dgl.utils import expand_as_pair\n",
        "from dgl.nn.functional import edge_softmax\n",
        "\n",
        "# ====================== Custom components from your original code ======================\n",
        "\n",
        "class RelationAttention(nn.Module):\n",
        "    def __init__(self, in_size, hidden_size=16):\n",
        "        super(RelationAttention, self).__init__()\n",
        "\n",
        "        self.project = nn.Sequential(\n",
        "            nn.Linear(in_size, hidden_size),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_size, 1, bias=False)\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        w = self.project(z).mean(0)                    # (M, 1)\n",
        "        beta = torch.softmax(w, dim=0)                 # (M, 1)\n",
        "        beta = beta.expand((z.shape[0],) + beta.shape) # (N, M, 1)\n",
        "        out = (beta * z).sum(1)                        # (N, D * K)\n",
        "        return out\n",
        "\n",
        "def node_drop(feats, drop_rate, training):\n",
        "    n = feats.shape[0]\n",
        "    drop_rates = torch.FloatTensor(np.ones(n) * drop_rate)\n",
        "\n",
        "    if training:\n",
        "        masks = torch.bernoulli(1. - drop_rates).unsqueeze(1)\n",
        "        feats = masks.to(feats.device) * feats / (1. - drop_rate)\n",
        "    else:\n",
        "        feats = feats\n",
        "    return feats\n",
        "\n",
        "class custom_GATConv(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_feats,\n",
        "                 out_feats,\n",
        "                 num_heads,\n",
        "                 feat_drop=0.1,\n",
        "                 attn_drop=0.,\n",
        "                 negative_slope=0.2,\n",
        "                 edge_drop=0.1,\n",
        "                 residual=False,\n",
        "                 activation=None,\n",
        "                 allow_zero_in_degree=False,\n",
        "                 bias=True):\n",
        "        super(custom_GATConv, self).__init__()\n",
        "        self._num_heads = num_heads\n",
        "        self._in_src_feats, self._in_dst_feats = expand_as_pair(in_feats)\n",
        "        self._out_feats = out_feats\n",
        "        self._allow_zero_in_degree = allow_zero_in_degree\n",
        "        if isinstance(in_feats, tuple):\n",
        "            self.fc_src = nn.Linear(\n",
        "                self._in_src_feats, out_feats * num_heads, bias=False)\n",
        "            self.fc_dst = nn.Linear(\n",
        "                self._in_dst_feats, out_feats * num_heads, bias=False)\n",
        "            self.fc_src2 = nn.Linear(\n",
        "                self._in_src_feats, out_feats * num_heads, bias=False)\n",
        "            self.fc_dst2 = nn.Linear(\n",
        "                self._in_dst_feats, out_feats * num_heads, bias=False)\n",
        "        else:\n",
        "            self.fc = nn.Linear(\n",
        "                self._in_src_feats, out_feats * num_heads, bias=False)\n",
        "            self.fc2 = nn.Linear(\n",
        "                self._in_src_feats, out_feats * num_heads, bias=False)\n",
        "        self.attn_l = nn.Parameter(torch.FloatTensor(size=(1, num_heads, out_feats)))\n",
        "        self.attn_r = nn.Parameter(torch.FloatTensor(size=(1, num_heads, out_feats)))\n",
        "        self.attn_l2 = nn.Parameter(torch.FloatTensor(size=(1, num_heads, out_feats)))\n",
        "        self.attn_r2 = nn.Parameter(torch.FloatTensor(size=(1, num_heads, out_feats)))\n",
        "        self.feat_drop = nn.Dropout(feat_drop)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.leaky_relu = nn.LeakyReLU(negative_slope)\n",
        "        if bias:\n",
        "            self.bias = nn.Parameter(torch.FloatTensor(size=(num_heads * out_feats,)))\n",
        "        else:\n",
        "            self.register_buffer('bias', None)\n",
        "        if residual:\n",
        "            if self._in_dst_feats != out_feats:\n",
        "                self.res_fc = nn.Linear(\n",
        "                    self._in_dst_feats, num_heads * out_feats, bias=False)\n",
        "            else:\n",
        "                self.res_fc = Identity()\n",
        "        else:\n",
        "            self.register_buffer('res_fc', None)\n",
        "        self.reset_parameters()\n",
        "        self.activation = activation\n",
        "\n",
        "        self.edge_drop = edge_drop\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        gain = nn.init.calculate_gain('relu')\n",
        "        if hasattr(self, 'fc'):\n",
        "            nn.init.xavier_normal_(self.fc.weight, gain=gain)\n",
        "        else:\n",
        "            nn.init.xavier_normal_(self.fc_src.weight, gain=gain)\n",
        "            nn.init.xavier_normal_(self.fc_dst.weight, gain=gain)\n",
        "        nn.init.xavier_normal_(self.attn_l, gain=gain)\n",
        "        nn.init.xavier_normal_(self.attn_r, gain=gain)\n",
        "        nn.init.xavier_normal_(self.attn_l2, gain=gain)\n",
        "        nn.init.xavier_normal_(self.attn_r2, gain=gain)\n",
        "        nn.init.constant_(self.bias, 0)\n",
        "        if isinstance(self.res_fc, nn.Linear):\n",
        "            nn.init.xavier_normal_(self.res_fc.weight, gain=gain)\n",
        "\n",
        "    def set_allow_zero_in_degree(self, set_value):\n",
        "        self._allow_zero_in_degree = set_value\n",
        "\n",
        "    def forward(self, graph, feat, get_attention=False):\n",
        "        with graph.local_scope():\n",
        "            if not self._allow_zero_in_degree:\n",
        "                if (graph.in_degrees() == 0).any():\n",
        "                    raise DGLError('There are 0-in-degree nodes in the graph, '\n",
        "                                   'output for those nodes will be invalid. '\n",
        "                                   'This is harmful for some applications, '\n",
        "                                   'causing silent performance regression. '\n",
        "                                   'Adding self-loop on the input graph by '\n",
        "                                   'calling `g = dgl.add_self_loop(g)` will resolve '\n",
        "                                   'the issue. Setting ``allow_zero_in_degree`` '\n",
        "                                   'to be `True` when constructing this module will '\n",
        "                                   'suppress the check and let the code run.')\n",
        "\n",
        "            if isinstance(feat, tuple):\n",
        "                do_edge_drop = feat[2]\n",
        "                # print('do_edge_drop: ', do_edge_drop)\n",
        "                h_src = self.feat_drop(feat[0])\n",
        "                h_dst = self.feat_drop(feat[1])\n",
        "                h_src2 = h_src.clone()\n",
        "                h_dst2 = h_dst.clone()\n",
        "                if not hasattr(self, 'fc_src'):\n",
        "                    feat_src = self.fc(h_src).view(-1, self._num_heads, self._out_feats)\n",
        "                    feat_dst = self.fc(h_dst).view(-1, self._num_heads, self._out_feats)\n",
        "                    feat_src2 = self.fc2(h_src2).view(-1, self._num_heads, self._out_feats)\n",
        "                    feat_dst2 = self.fc2(h_dst2).view(-1, self._num_heads, self._out_feats)\n",
        "                else:\n",
        "                    feat_src = self.fc_src(h_src).view(-1, self._num_heads, self._out_feats)\n",
        "                    feat_dst = self.fc_dst(h_dst).view(-1, self._num_heads, self._out_feats)\n",
        "                    feat_src2 = self.fc_src2(h_src2).view(-1, self._num_heads, self._out_feats)\n",
        "                    feat_dst2 = self.fc_dst2(h_dst2).view(-1, self._num_heads, self._out_feats)\n",
        "            else:\n",
        "                h_src = h_dst = self.feat_drop(feat)\n",
        "                h_src2 = h_dst2 = h_src.clone() # self.feat_drop(feat)\n",
        "                feat_src = feat_dst = self.fc(h_src).view(\n",
        "                    -1, self._num_heads, self._out_feats)\n",
        "                feat_src2 = feat_dst2 = self.fc(h_src).view(\n",
        "                    -1, self._num_heads, self._out_feats)\n",
        "                if graph.is_block:\n",
        "                    feat_dst = feat_src[:graph.number_of_dst_nodes()]\n",
        "                    feat_dst2 = feat_src2[:graph.number_of_dst_nodes()]\n",
        "\n",
        "            el = (feat_src * self.attn_l).sum(dim=-1).unsqueeze(-1)\n",
        "            er = (feat_dst * self.attn_r).sum(dim=-1).unsqueeze(-1)\n",
        "\n",
        "            graph.srcdata.update({'ft': feat_src, 'el': el, 'feat_src2': feat_src2})\n",
        "            graph.dstdata.update({'er': er, 'feat_dst2': feat_dst2})\n",
        "            # compute edge attention, el and er are a_l Wh_i and a_r Wh_j respectively.\n",
        "            graph.apply_edges(fn.u_add_v('el', 'er', 'e'))\n",
        "            e = self.leaky_relu(graph.edata.pop('e'))\n",
        "\n",
        "            # compute softmax, edge dropout\n",
        "            if self.training and self.edge_drop > 0:\n",
        "                if hasattr(do_edge_drop, 'item') and do_edge_drop.item() == 1:\n",
        "                    perm = torch.randperm(graph.number_of_edges(), device=e.device)\n",
        "                    bound = int(graph.number_of_edges() * self.edge_drop)\n",
        "                    eids = perm[bound:]\n",
        "                    graph.edata[\"a\"] = torch.zeros_like(e)\n",
        "                    graph.edata[\"a\"][eids] = self.attn_drop(edge_softmax(graph, e[eids], eids=eids))\n",
        "                else:\n",
        "                    graph.edata['a'] = self.attn_drop(edge_softmax(graph, e))\n",
        "            else:\n",
        "                graph.edata['a'] = self.attn_drop(edge_softmax(graph, e))\n",
        "\n",
        "            # message passing\n",
        "            graph.update_all(fn.u_mul_e('ft', 'a', 'm'),\n",
        "                             fn.sum('m', 'initial_ft'))\n",
        "            graph.update_all(fn.u_mul_v('feat_src2', 'feat_dst2', 'm2'),\n",
        "                             fn.sum('m2', 'add_ft'))\n",
        "            rst = graph.dstdata['initial_ft'] + graph.dstdata['add_ft']\n",
        "\n",
        "            # residual\n",
        "            if self.res_fc is not None:\n",
        "                resval = self.res_fc(h_dst).view(h_dst.shape[0], self._num_heads, self._out_feats)\n",
        "                rst = rst + resval\n",
        "            # bias\n",
        "            if self.bias is not None:\n",
        "                rst = rst + self.bias.view(1, self._num_heads, self._out_feats)\n",
        "            # activation\n",
        "            if self.activation:\n",
        "                rst = self.activation(rst)\n",
        "\n",
        "            if get_attention:\n",
        "                return rst, graph.edata['a']\n",
        "            else:\n",
        "                return rst\n",
        "\n",
        "\n",
        "def _agg_func(inputs, dsttype, fn):\n",
        "    if len(inputs) == 0:\n",
        "        return None\n",
        "    stacked = torch.stack(inputs, dim=0)\n",
        "    return fn(stacked, dim=0)\n",
        "\n",
        "def _sum_reduce_func(inputs, dim):\n",
        "    return torch.sum(inputs, dim=dim)\n",
        "\n",
        "def get_aggregate_fn(agg):\n",
        "    if agg == 'sum':\n",
        "        fn = _sum_reduce_func\n",
        "    elif agg == 'max':\n",
        "        fn = lambda inputs, dim: torch.max(inputs, dim=dim)[0]\n",
        "    elif agg == 'min':\n",
        "        fn = lambda inputs, dim: torch.min(inputs, dim=dim)[0]\n",
        "    elif agg == 'mean':\n",
        "        fn = lambda inputs, dim: torch.mean(inputs, dim=dim)\n",
        "    elif agg == 'stack':\n",
        "        fn = None  # will not be called\n",
        "    else:\n",
        "        raise DGLError('Invalid cross type aggregator. Must be one of '\n",
        "                       '\"sum\", \"max\", \"min\", \"mean\" or \"stack\". But got \"%s\"' % agg)\n",
        "    if agg == 'stack':\n",
        "        return lambda inputs, dsttype: torch.stack(inputs, dim=1) if len(inputs) > 0 else None\n",
        "    else:\n",
        "        return lambda inputs, dsttype: _agg_func(inputs, dsttype, fn)\n",
        "\n",
        "\n",
        "class custom_HeteroGraphConv(nn.Module):\n",
        "    def __init__(self, mods, aggregate='sum'):\n",
        "        super(custom_HeteroGraphConv, self).__init__()\n",
        "        self.mods = nn.ModuleDict(mods)\n",
        "        # Do not break if graph has 0-in-degree nodes.\n",
        "        # Because there is no general rule to add self-loop for heterograph.\n",
        "        for _, v in self.mods.items():\n",
        "            set_allow_zero_in_degree_fn = getattr(v, 'set_allow_zero_in_degree', None)\n",
        "            if callable(set_allow_zero_in_degree_fn):\n",
        "                set_allow_zero_in_degree_fn(True)\n",
        "        if isinstance(aggregate, str):\n",
        "            self.agg_fn = get_aggregate_fn(aggregate)\n",
        "        else:\n",
        "            self.agg_fn = aggregate\n",
        "\n",
        "    def forward(self, g, inputs, mod_args=None, mod_kwargs=None):\n",
        "        if mod_args is None:\n",
        "            mod_args = {}\n",
        "        if mod_kwargs is None:\n",
        "            mod_kwargs = {}\n",
        "        outputs = {nty : [] for nty in g.dsttypes}\n",
        "        if isinstance(inputs, tuple) or g.is_block:\n",
        "            if isinstance(inputs, tuple):\n",
        "                src_inputs, dst_inputs = inputs\n",
        "            else:\n",
        "                src_inputs = inputs\n",
        "                dst_inputs = {k: v[:g.number_of_dst_nodes(k)] for k, v in inputs.items()}\n",
        "\n",
        "            for stype, etype, dtype in g.canonical_etypes:\n",
        "                rel_graph = g[stype, etype, dtype]\n",
        "                if rel_graph.number_of_edges() == 0:\n",
        "                    continue\n",
        "                if stype not in src_inputs or dtype not in dst_inputs:\n",
        "                    continue\n",
        "                dstdata = self.mods[etype](\n",
        "                    rel_graph,\n",
        "                    (src_inputs[stype], dst_inputs[dtype], mod_kwargs),\n",
        "                    *mod_args.get(etype, ())\n",
        "                    )\n",
        "                outputs[dtype].append(dstdata)\n",
        "        else:\n",
        "            for stype, etype, dtype in g.canonical_etypes:\n",
        "                rel_graph = g[stype, etype, dtype]\n",
        "                if rel_graph.number_of_edges() == 0:\n",
        "                    continue\n",
        "                if stype not in inputs:\n",
        "                    continue\n",
        "                dstdata = self.mods[etype](\n",
        "                    rel_graph,\n",
        "                    (inputs[stype], inputs[dtype], mod_kwargs),\n",
        "                    *mod_args.get(etype, ())\n",
        "                    )\n",
        "                outputs[dtype].append(dstdata)\n",
        "        rsts = {}\n",
        "        for nty, alist in outputs.items():\n",
        "            if len(alist) != 0:\n",
        "                rsts[nty] = self.agg_fn(alist, nty)\n",
        "        return rsts\n",
        "\n",
        "\n",
        "class ScorePredictor(nn.Module):\n",
        "    def forward(self, edge_subgraph, x):\n",
        "        with edge_subgraph.local_scope():\n",
        "            edge_subgraph.ndata['x'] = x\n",
        "            edge_subgraph.apply_edges(dgl.function.u_dot_v('x', 'x', 'score'), etype='u-r')\n",
        "            return edge_subgraph.edata['score'][('user', 'u-r', 'recipe')].squeeze()\n",
        "\n",
        "\n",
        "# ====================== HAN-Based GNN Implementation ======================\n",
        "\n",
        "class HANBasedGNN(nn.Module):\n",
        "    \"\"\"\n",
        "    A simplified HAN implementation that directly replaces your GNN class.\n",
        "    This version focuses on compatibility and robustness rather than full HAN capabilities.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_feats, hid_feats, out_feats, rel_names):\n",
        "        super(HANBasedGNN, self).__init__()\n",
        "\n",
        "        self.num_heads = 4\n",
        "        self.hid_feats = int(hid_feats/self.num_heads)\n",
        "        self.out_feats = int(out_feats/self.num_heads)\n",
        "        self.relation_attention = RelationAttention(hid_feats)\n",
        "\n",
        "        # First layer GATConv - one for each relation type\n",
        "        self.gatconv1 = custom_HeteroGraphConv({\n",
        "            'i-r': custom_GATConv(in_feats, self.hid_feats, num_heads=self.num_heads),\n",
        "            'r-i': custom_GATConv(in_feats, self.hid_feats, num_heads=self.num_heads),\n",
        "            'r-r': custom_GATConv(in_feats, self.hid_feats, num_heads=self.num_heads),\n",
        "            'i-i': custom_GATConv(in_feats, self.hid_feats, num_heads=self.num_heads),\n",
        "            'u-r': custom_GATConv(in_feats, self.hid_feats, num_heads=self.num_heads),\n",
        "            'r-u': custom_GATConv(in_feats, self.hid_feats, num_heads=self.num_heads),\n",
        "        }, aggregate='stack')\n",
        "\n",
        "        # Second layer GATConv - one for each relation type\n",
        "        self.gatconv2 = custom_HeteroGraphConv({\n",
        "            'i-r': custom_GATConv(self.hid_feats*self.num_heads, self.out_feats, num_heads=self.num_heads),\n",
        "            'r-i': custom_GATConv(self.hid_feats*self.num_heads, self.out_feats, num_heads=self.num_heads),\n",
        "            'r-r': custom_GATConv(self.hid_feats*self.num_heads, self.out_feats, num_heads=self.num_heads),\n",
        "            'i-i': custom_GATConv(self.hid_feats*self.num_heads, self.out_feats, num_heads=self.num_heads),\n",
        "            'u-r': custom_GATConv(self.hid_feats*self.num_heads, self.out_feats, num_heads=self.num_heads),\n",
        "            'r-u': custom_GATConv(self.hid_feats*self.num_heads, self.out_feats, num_heads=self.num_heads),\n",
        "        }, aggregate='stack')\n",
        "\n",
        "        # Additional semantic attention for each node type\n",
        "        self.semantic_attention = nn.ModuleDict({\n",
        "            'user': RelationAttention(out_feats),\n",
        "            'recipe': RelationAttention(out_feats),\n",
        "            'ingredient': RelationAttention(out_feats)\n",
        "        })\n",
        "\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, blocks, inputs, edge_weight=None, do_edge_drop=None):\n",
        "        \"\"\"\n",
        "        Forward pass that maintains compatibility with your current code.\n",
        "\n",
        "        Args:\n",
        "            blocks: List of DGL blocks\n",
        "            inputs: Dictionary of input features for each node type\n",
        "            edge_weight: Edge weights (optional)\n",
        "            do_edge_drop: Flag for edge dropout (optional)\n",
        "\n",
        "        Returns:\n",
        "            Dictionary of output embeddings for each node type\n",
        "        \"\"\"\n",
        "        # Extract edge weights if provided\n",
        "        if edge_weight is not None:\n",
        "            edge_weight_0 = blocks[0].edata['weight']\n",
        "            edge_weight_1 = blocks[1].edata['weight']\n",
        "        else:\n",
        "            edge_weight_0 = None\n",
        "            edge_weight_1 = None\n",
        "\n",
        "        # Get destination node IDs for final layer\n",
        "        try:\n",
        "            num_users = blocks[-1].dstdata[dgl.NID]['user'].shape[0]\n",
        "            num_recipes = blocks[-1].dstdata[dgl.NID]['recipe'].shape[0]\n",
        "        except:\n",
        "            # Fallback if NID is not available\n",
        "            num_users = blocks[-1].number_of_dst_nodes('user')\n",
        "            num_recipes = blocks[-1].number_of_dst_nodes('recipe')\n",
        "\n",
        "        # First layer GAT + relationship attention\n",
        "        try:\n",
        "            h = self.gatconv1(blocks[0], inputs, edge_weight_0, do_edge_drop)\n",
        "\n",
        "            # Flatten the head dimension and apply relation attention\n",
        "            h = {k: F.relu(v).flatten(2) for k, v in h.items()}\n",
        "            h = {k: self.relation_attention(v) for k, v in h.items()}\n",
        "\n",
        "            # Store first layer outputs for later\n",
        "            first_layer_output = {}\n",
        "            if 'user' in h:\n",
        "                first_layer_output['user'] = h['user'][:num_users]\n",
        "            if 'recipe' in h:\n",
        "                first_layer_output['recipe'] = h['recipe'][:num_recipes]\n",
        "\n",
        "            # Apply dropout before second layer\n",
        "            h = {key: self.dropout(value) for key, value in h.items()}\n",
        "\n",
        "            # Second layer GAT\n",
        "            h = self.gatconv2(blocks[-1], h, edge_weight_1, do_edge_drop)\n",
        "\n",
        "            # Process the output embeddings\n",
        "            output_dict = {}\n",
        "            for ntype, embs in h.items():\n",
        "                # Apply semantic attention across multiple attention heads\n",
        "                flattened_embs = embs.flatten(2)\n",
        "                if ntype in self.semantic_attention:\n",
        "                    output_dict[ntype] = self.semantic_attention[ntype](flattened_embs)\n",
        "                else:\n",
        "                    # Fallback if node type doesn't have semantic attention\n",
        "                    output_dict[ntype] = torch.mean(flattened_embs, dim=1)\n",
        "\n",
        "            return output_dict\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in HANBasedGNN forward pass: {e}\")\n",
        "            # Print debugging information\n",
        "            for i, block in enumerate(blocks):\n",
        "                print(f\"Block {i} info:\")\n",
        "                for ntype in block.ntypes:\n",
        "                    print(f\"  {ntype} nodes: {block.number_of_nodes(ntype)}\")\n",
        "                for canonical_etype in block.canonical_etypes:\n",
        "                    stype, etype, dtype = canonical_etype\n",
        "                    print(f\"  {stype}-{etype}-{dtype} edges: {block.number_of_edges(canonical_etype)}\")\n",
        "\n",
        "            # Also print feature dimensions\n",
        "            print(\"Input features dimensions:\")\n",
        "            for ntype, feat in inputs.items():\n",
        "                print(f\"  {ntype}: {feat.shape}\")\n",
        "\n",
        "            raise\n",
        "\n",
        "\n",
        "# ====================== Model Implementation Using HAN ======================\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Feature projection layers\n",
        "        self.user_embedding = nn.Sequential(\n",
        "            nn.Linear(300, 128),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.instr_embedding = nn.Sequential(\n",
        "            nn.Linear(1024, 128),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.ingredient_embedding = nn.Sequential(\n",
        "            nn.Linear(46, 128),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.recipe_combine2out = nn.Sequential(\n",
        "            nn.Linear(128, 128),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Replace GNN with HANBasedGNN\n",
        "        self.gnn = HANBasedGNN(128, 128, 128, graph.etypes)\n",
        "\n",
        "        self.pred = ScorePredictor()\n",
        "        self.setTransformer_ = SetTransformer()\n",
        "\n",
        "    def forward(self, positive_graph, negative_graph, blocks, input_features):\n",
        "        user, instr, ingredient, ingredient_of_dst_recipe = input_features\n",
        "\n",
        "        # Project input features\n",
        "        user_major = self.user_embedding(user)\n",
        "        user_major = norm(user_major)\n",
        "        instr_major = self.instr_embedding(instr)\n",
        "        instr_major = norm(instr_major)\n",
        "        ingredient_major = self.ingredient_embedding(ingredient)\n",
        "        ingredient_major = norm(ingredient_major)\n",
        "\n",
        "        # Process with HANBasedGNN - this is a direct replacement for your GNN\n",
        "        x = self.gnn(blocks, {'user': user_major, 'recipe': instr_major, 'ingredient': ingredient_major}, do_edge_drop=torch.Tensor([[0]]))\n",
        "\n",
        "        # Contrastive learning - 1\n",
        "        user1 = node_drop(user, 0.1, self.training)\n",
        "        instr1 = node_drop(instr, 0.1, self.training)\n",
        "        ingredient1 = node_drop(ingredient, 0.1, self.training)\n",
        "\n",
        "        user1 = self.user_embedding(user1)\n",
        "        user1 = norm(user1)\n",
        "        instr1 = self.instr_embedding(instr1)\n",
        "        instr1 = norm(instr1)\n",
        "        ingredient1 = self.ingredient_embedding(ingredient1)\n",
        "        ingredient1 = norm(ingredient1)\n",
        "\n",
        "        x1 = self.gnn(blocks, {'user': user1, 'recipe': instr1, 'ingredient': ingredient1}, do_edge_drop=torch.Tensor([[1]]))\n",
        "\n",
        "        # Contrastive learning - 2\n",
        "        user2 = node_drop(user, 0.1, self.training)\n",
        "        instr2 = node_drop(instr, 0.1, self.training)\n",
        "        ingredient2 = node_drop(ingredient, 0.1, self.training)\n",
        "\n",
        "        user2 = self.user_embedding(user2)\n",
        "        user2 = norm(user2)\n",
        "        instr2 = self.instr_embedding(instr2)\n",
        "        instr2 = norm(instr2)\n",
        "        ingredient2 = self.ingredient_embedding(ingredient2)\n",
        "        ingredient2 = norm(ingredient2)\n",
        "\n",
        "        x2 = self.gnn(blocks, {'user': user2, 'recipe': instr2, 'ingredient': ingredient2}, do_edge_drop=torch.Tensor([[1]]))\n",
        "\n",
        "        # Set Transformer for ingredient aggregation\n",
        "        all_ingre_emb_for_each_recipe = get_ingredient_neighbors_all_embeddings(blocks, blocks[1].dstdata['_ID']['recipe'], ingredient_of_dst_recipe)\n",
        "        all_ingre_emb_for_each_recipe = norm(all_ingre_emb_for_each_recipe)\n",
        "        total_ingre_emb = self.setTransformer_(all_ingre_emb_for_each_recipe)\n",
        "        total_ingre_emb = norm(total_ingre_emb)\n",
        "\n",
        "        # Combine recipe embeddings\n",
        "        x['recipe'] = self.recipe_combine2out(total_ingre_emb.add(x['recipe']))\n",
        "\n",
        "        # Calculate scores\n",
        "        pos_score = self.pred(positive_graph, x)\n",
        "        neg_score = self.pred(negative_graph, x)\n",
        "\n",
        "        return pos_score, neg_score, x1, x2\n",
        "\n",
        "\n",
        "# Helper function to normalize embeddings\n",
        "def norm(input, p=1, dim=1, eps=1e-12):\n",
        "    return input / input.norm(p, dim, keepdim=True).clamp(min=eps).expand_as(input)\n",
        "\n",
        "\n",
        "# ====================== Usage ======================\n",
        "\n",
        "# The following imports should already be in your code\n",
        "import numpy as np\n",
        "import time\n",
        "import math\n",
        "from functools import partial\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Initialize model, optimizer, and scheduler\n",
        "from dgl.base import DGLError\n",
        "\n",
        "# You'll need to make sure that SetTransformer and related dependencies are defined before this point\n",
        "\n",
        "# model = Model().to(device)\n",
        "# opt = torch.optim.Adam(model.parameters(), lr=0.005)\n",
        "# scheduler = torch.optim.lr_scheduler.ExponentialLR(opt, gamma=0.9)"
      ],
      "metadata": {
        "id": "CrV5Zhbevbz2"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Feature projection layers\n",
        "        self.user_embedding = nn.Sequential(\n",
        "            nn.Linear(300, 128),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.instr_embedding = nn.Sequential(\n",
        "            nn.Linear(1024, 128),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.ingredient_embedding = nn.Sequential(\n",
        "            nn.Linear(46, 128),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.recipe_combine2out = nn.Sequential(\n",
        "            nn.Linear(128, 128),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Replace GNN with HANBasedGNN\n",
        "        self.gnn = HANBasedGNN(128, 128, 128, graph.etypes)\n",
        "\n",
        "        self.pred = ScorePredictor()\n",
        "        self.setTransformer_ = SetTransformer()\n",
        "\n",
        "    def forward(self, positive_graph, negative_graph, blocks, input_features):\n",
        "        user, instr, ingredient, ingredient_of_dst_recipe = input_features\n",
        "\n",
        "        # Project input features\n",
        "        user_major = self.user_embedding(user)\n",
        "        user_major = norm(user_major)\n",
        "        instr_major = self.instr_embedding(instr)\n",
        "        instr_major = norm(instr_major)\n",
        "        ingredient_major = self.ingredient_embedding(ingredient)\n",
        "        ingredient_major = norm(ingredient_major)\n",
        "\n",
        "        # Process with HANBasedGNN - this is a direct replacement for your GNN\n",
        "        x = self.gnn(blocks, {'user': user_major, 'recipe': instr_major, 'ingredient': ingredient_major}, do_edge_drop=torch.Tensor([[0]]))\n",
        "\n",
        "        # Contrastive learning - 1\n",
        "        user1 = node_drop(user, 0.1, self.training)\n",
        "        instr1 = node_drop(instr, 0.1, self.training)\n",
        "        ingredient1 = node_drop(ingredient, 0.1, self.training)\n",
        "\n",
        "        user1 = self.user_embedding(user1)\n",
        "        user1 = norm(user1)\n",
        "        instr1 = self.instr_embedding(instr1)\n",
        "        instr1 = norm(instr1)\n",
        "        ingredient1 = self.ingredient_embedding(ingredient1)\n",
        "        ingredient1 = norm(ingredient1)\n",
        "\n",
        "        x1 = self.gnn(blocks, {'user': user1, 'recipe': instr1, 'ingredient': ingredient1}, do_edge_drop=torch.Tensor([[1]]))\n",
        "\n",
        "        # Contrastive learning - 2\n",
        "        user2 = node_drop(user, 0.1, self.training)\n",
        "        instr2 = node_drop(instr, 0.1, self.training)\n",
        "        ingredient2 = node_drop(ingredient, 0.1, self.training)\n",
        "\n",
        "        user2 = self.user_embedding(user2)\n",
        "        user2 = norm(user2)\n",
        "        instr2 = self.instr_embedding(instr2)\n",
        "        instr2 = norm(instr2)\n",
        "        ingredient2 = self.ingredient_embedding(ingredient2)\n",
        "        ingredient2 = norm(ingredient2)\n",
        "\n",
        "        x2 = self.gnn(blocks, {'user': user2, 'recipe': instr2, 'ingredient': ingredient2}, do_edge_drop=torch.Tensor([[1]]))\n",
        "\n",
        "        # Set Transformer for ingredient aggregation\n",
        "        all_ingre_emb_for_each_recipe = get_ingredient_neighbors_all_embeddings(blocks, blocks[1].dstdata['_ID']['recipe'], ingredient_of_dst_recipe)\n",
        "        all_ingre_emb_for_each_recipe = norm(all_ingre_emb_for_each_recipe)\n",
        "        total_ingre_emb = self.setTransformer_(all_ingre_emb_for_each_recipe)\n",
        "        total_ingre_emb = norm(total_ingre_emb)\n",
        "\n",
        "        # Combine recipe embeddings\n",
        "        x['recipe'] = self.recipe_combine2out(total_ingre_emb.add(x['recipe']))\n",
        "\n",
        "        # Calculate scores\n",
        "        pos_score = self.pred(positive_graph, x)\n",
        "        neg_score = self.pred(negative_graph, x)\n",
        "\n",
        "        return pos_score, neg_score, x1, x2"
      ],
      "metadata": {
        "id": "yajUWvY7oiTi"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## New Training"
      ],
      "metadata": {
        "id": "4XUL51elqxLv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dgl.base import DGLError\n",
        "\n",
        "model = Model().to(device)\n",
        "opt = torch.optim.Adam(model.parameters(), lr=0.005)\n",
        "scheduler = torch.optim.lr_scheduler.ExponentialLR(opt, gamma=0.9)\n",
        "\n",
        "print('start ... ')\n",
        "for epoch in range(50):\n",
        "    train_start = time.time()\n",
        "    epoch_loss = 0\n",
        "    epoch_contrastive_loss = 0\n",
        "    epoch_emb_loss = 0\n",
        "    iteration_cnt = 0\n",
        "\n",
        "    for input_nodes, positive_graph, negative_graph, blocks in train_edgeloader:\n",
        "        model.train()\n",
        "        blocks = [b.to(device) for b in blocks]\n",
        "        positive_graph = positive_graph.to(device)\n",
        "        negative_graph = negative_graph.to(device)\n",
        "\n",
        "        input_user = blocks[0].srcdata['random_feature']['user']\n",
        "        input_instr = blocks[0].srcdata['avg_instr_feature']['recipe']\n",
        "        input_ingredient = blocks[0].srcdata['nutrient_feature']['ingredient']\n",
        "        ingredient_of_dst_recipe = blocks[1].srcdata['nutrient_feature']['ingredient']\n",
        "        input_features = [input_user, input_instr, input_ingredient, ingredient_of_dst_recipe]\n",
        "\n",
        "        #print_block_info(blocks)\n",
        "        #print_feature_info({'user': input_user, 'recipe': input_instr,'ingredient': input_ingredient})\n",
        "\n",
        "        pos_score, neg_score, x1, x2 = model(positive_graph, negative_graph, blocks, input_features)\n",
        "        contrastive_loss = get_contrastive_loss(x1, x2)\n",
        "        # emb_loss = get_emb_loss(x1, x2)\n",
        "        assert not math.isnan(contrastive_loss)\n",
        "        recommendation_loss = get_recommendation_loss(pos_score, neg_score)\n",
        "        assert not math.isnan(recommendation_loss)\n",
        "\n",
        "        loss = recommendation_loss + 0.1 * contrastive_loss # + 1e-5 * emb_loss\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "        epoch_loss += recommendation_loss.item()\n",
        "        epoch_contrastive_loss += contrastive_loss.item()\n",
        "        # epoch_emb_loss += emb_loss.item()\n",
        "        iteration_cnt += 1\n",
        "\n",
        "        # break\n",
        "\n",
        "    epoch_loss /= iteration_cnt\n",
        "    epoch_contrastive_loss /= iteration_cnt\n",
        "    train_end = time.strftime(\"%M:%S min\", time.gmtime(time.time()-train_start))\n",
        "\n",
        "    print('Epoch: {0},  Loss: {l:.4f}, Contrastive: {cl:.4f}, Emb: {el:.4f},  Time: {t}, LR: {lr:.6f}'\n",
        "          .format(epoch, l=epoch_loss, cl=epoch_contrastive_loss, el=epoch_emb_loss, t=train_end, lr=opt.param_groups[0]['lr']))\n",
        "    scheduler.step()\n",
        "\n",
        "    # Evaluation\n",
        "    # For demonstration purpose, only test set result is reported here. Please use val_dataloader for comprehensiveness.\n",
        "    if epoch >= 4 and epoch % 1 == 0:\n",
        "        print('testing: ')\n",
        "        evaluate(model, test_edgeloader, multi_metrics=True)\n",
        "        print()\n",
        "        print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wwRSMvqB2OM2",
        "outputId": "282f2af5-a0dc-4927-e0c4-2c81f6e57234"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start ... \n",
            "Epoch: 0,  Loss: 0.6318, Contrastive: 0.2187, Emb: 0.0000,  Time: 03:43 min, LR: 0.005000\n",
            "Epoch: 1,  Loss: 0.4827, Contrastive: 0.0579, Emb: 0.0000,  Time: 03:43 min, LR: 0.004500\n",
            "Epoch: 2,  Loss: 0.4432, Contrastive: 0.0389, Emb: 0.0000,  Time: 03:42 min, LR: 0.004050\n",
            "Epoch: 3,  Loss: 0.4159, Contrastive: 0.0309, Emb: 0.0000,  Time: 03:42 min, LR: 0.003645\n",
            "Epoch: 4,  Loss: 0.3992, Contrastive: 0.0267, Emb: 0.0000,  Time: 03:41 min, LR: 0.003281\n",
            "testing: \n",
            "evaluation_result:  {'hit_ratio': array([0.18017339, 0.21924865, 0.25530846, 0.28872974, 0.31725091,\n",
            "       0.3412489 , 0.36637769, 0.38761151, 0.41223772, 0.43309461]), 'ndcg': array([0.18017339, 0.21924865, 0.24199985, 0.2587105 , 0.2709939 ,\n",
            "       0.28027759, 0.28922864, 0.29630658, 0.30407528, 0.31035383]), 'precision': array([0.18017339, 0.10962432, 0.08510282, 0.07218243, 0.06345018,\n",
            "       0.05687482, 0.05233967, 0.04845144, 0.04580419, 0.04330946]), 'MAP': array([0.18017339, 0.19971102, 0.21173095, 0.22008628, 0.22579051,\n",
            "       0.22979017, 0.23338   , 0.23603423, 0.23877047, 0.24085616]), 'AUC': np.float64(0.7535584426821459)}\n",
            "epoch_contrastive_loss:  0.027905467838521988\n",
            "evalutate_time:  04:29 min\n",
            "\n",
            "\n",
            "Epoch: 5,  Loss: 0.3867, Contrastive: 0.0237, Emb: 0.0000,  Time: 03:41 min, LR: 0.002952\n",
            "testing: \n",
            "evaluation_result:  {'hit_ratio': array([0.17188089, 0.21862043, 0.26221887, 0.29362985, 0.31863299,\n",
            "       0.34589773, 0.36989572, 0.39251162, 0.41575575, 0.43962809]), 'ndcg': array([0.17188089, 0.21862043, 0.24612798, 0.26183347, 0.27260174,\n",
            "       0.28314918, 0.29169744, 0.29923607, 0.30656878, 0.31375507]), 'precision': array([0.17188089, 0.10931021, 0.08740629, 0.07340746, 0.0637266 ,\n",
            "       0.05764962, 0.05284225, 0.04906395, 0.04619508, 0.04396281]), 'MAP': array([0.17188089, 0.19525066, 0.20978347, 0.21763622, 0.22263685,\n",
            "       0.22718097, 0.23060925, 0.23343624, 0.23601892, 0.23840616]), 'AUC': np.float64(0.7489354710702376)}\n",
            "epoch_contrastive_loss:  0.024502329232673797\n",
            "evalutate_time:  04:28 min\n",
            "\n",
            "\n",
            "Epoch: 6,  Loss: 0.3763, Contrastive: 0.0204, Emb: 0.0000,  Time: 03:41 min, LR: 0.002657\n",
            "testing: \n",
            "evaluation_result:  {'hit_ratio': array([0.1696193 , 0.21573062, 0.25995728, 0.29362985, 0.32365875,\n",
            "       0.35104913, 0.37919337, 0.4013067 , 0.42040457, 0.44276919]), 'ndcg': array([0.1696193 , 0.21573062, 0.24363454, 0.26047082, 0.27340356,\n",
            "       0.28399961, 0.29402479, 0.3013959 , 0.30742061, 0.31415303]), 'precision': array([0.1696193 , 0.10786531, 0.08665243, 0.07340746, 0.06473175,\n",
            "       0.05850819, 0.05417048, 0.05016334, 0.04671162, 0.04427692]), 'MAP': array([0.1696193 , 0.19267496, 0.20741718, 0.21583532, 0.2218411 ,\n",
            "       0.22640616, 0.23042677, 0.23319094, 0.23531292, 0.23754939]), 'AUC': np.float64(0.7595874649291402)}\n",
            "epoch_contrastive_loss:  0.02337057814593353\n",
            "evalutate_time:  04:28 min\n",
            "\n",
            "\n",
            "Epoch: 7,  Loss: 0.3684, Contrastive: 0.0191, Emb: 0.0000,  Time: 03:41 min, LR: 0.002391\n",
            "testing: \n",
            "evaluation_result:  {'hit_ratio': array([0.1747707 , 0.2202538 , 0.25480588, 0.29011182, 0.31900993,\n",
            "       0.34539515, 0.36826234, 0.3918834 , 0.41349416, 0.43623571]), 'ndcg': array([0.1747707 , 0.2202538 , 0.24205374, 0.25970671, 0.27215244,\n",
            "       0.28235964, 0.2905051 , 0.29837879, 0.30519622, 0.31204211]), 'precision': array([0.1747707 , 0.1101269 , 0.08493529, 0.07252796, 0.06380199,\n",
            "       0.05756586, 0.05260891, 0.04898543, 0.0459438 , 0.04362357]), 'MAP': array([0.1747707 , 0.19751225, 0.20902961, 0.2178561 , 0.22363572,\n",
            "       0.22803325, 0.2313    , 0.23425263, 0.23665382, 0.23892798]), 'AUC': np.float64(0.7497106870632584)}\n",
            "epoch_contrastive_loss:  0.01855675686180355\n",
            "evalutate_time:  04:28 min\n",
            "\n",
            "\n",
            "Epoch: 8,  Loss: 0.3595, Contrastive: 0.0187, Emb: 0.0000,  Time: 03:42 min, LR: 0.002152\n",
            "testing: \n",
            "evaluation_result:  {'hit_ratio': array([0.17087574, 0.22113331, 0.26121372, 0.29275035, 0.32252796,\n",
            "       0.35042091, 0.37303681, 0.3961553 , 0.41801734, 0.43837165]), 'ndcg': array([0.17087574, 0.22113331, 0.24642123, 0.26218955, 0.27501406,\n",
            "       0.28580453, 0.29386048, 0.30156664, 0.30846335, 0.31459061]), 'precision': array([0.17087574, 0.11056665, 0.08707124, 0.07318759, 0.06450559,\n",
            "       0.05840348, 0.05329097, 0.04951941, 0.04644637, 0.04383717]), 'MAP': array([0.17087574, 0.19600452, 0.20936466, 0.21724882, 0.22320434,\n",
            "       0.22785316, 0.23108401, 0.23397382, 0.23640293, 0.23843837]), 'AUC': np.float64(0.7522825046272689)}\n",
            "epoch_contrastive_loss:  0.020780573998178755\n",
            "evalutate_time:  04:28 min\n",
            "\n",
            "\n",
            "Epoch: 9,  Loss: 0.3550, Contrastive: 0.0174, Emb: 0.0000,  Time: 03:41 min, LR: 0.001937\n",
            "testing: \n",
            "evaluation_result:  {'hit_ratio': array([0.16949365, 0.22100766, 0.25907777, 0.29237341, 0.32102023,\n",
            "       0.3475311 , 0.3744189 , 0.39665787, 0.419902  , 0.44063325]), 'ndcg': array([0.16949365, 0.22100766, 0.24502723, 0.26167505, 0.27401256,\n",
            "       0.28426836, 0.29384599, 0.30125898, 0.30859169, 0.31483242]), 'precision': array([0.16949365, 0.11050383, 0.08635926, 0.07309335, 0.06420405,\n",
            "       0.05792185, 0.05348841, 0.04958223, 0.04665578, 0.04406332]), 'MAP': array([0.16949365, 0.19525066, 0.2079407 , 0.21626461, 0.22199397,\n",
            "       0.22641245, 0.23025356, 0.23303343, 0.23561611, 0.23768924]), 'AUC': np.float64(0.7579738497088697)}\n",
            "epoch_contrastive_loss:  0.0171642644360425\n",
            "evalutate_time:  04:29 min\n",
            "\n",
            "\n",
            "Epoch: 10,  Loss: 0.3486, Contrastive: 0.0167, Emb: 0.0000,  Time: 03:41 min, LR: 0.001743\n",
            "testing: \n",
            "evaluation_result:  {'hit_ratio': array([0.16873979, 0.21962558, 0.25782133, 0.29111697, 0.32164845,\n",
            "       0.34991833, 0.37504712, 0.39942204, 0.42216359, 0.44088453]), 'ndcg': array([0.16873979, 0.21962558, 0.24372442, 0.26037224, 0.27352143,\n",
            "       0.28445771, 0.29340877, 0.30153374, 0.3087079 , 0.31434347]), 'precision': array([0.16873979, 0.10981279, 0.08594044, 0.07277924, 0.06432969,\n",
            "       0.05831972, 0.05357816, 0.04992775, 0.04690707, 0.04408845]), 'MAP': array([0.16873979, 0.19418269, 0.2069146 , 0.21523851, 0.22134481,\n",
            "       0.22605646, 0.22964628, 0.23269315, 0.23521999, 0.23709208]), 'AUC': np.float64(0.7615993147977997)}\n",
            "epoch_contrastive_loss:  0.01549589585158087\n",
            "evalutate_time:  04:27 min\n",
            "\n",
            "\n",
            "Epoch: 11,  Loss: 0.3449, Contrastive: 0.0169, Emb: 0.0000,  Time: 03:41 min, LR: 0.001569\n",
            "testing: \n",
            "evaluation_result:  {'hit_ratio': array([0.16773464, 0.21975123, 0.25958035, 0.29438372, 0.32453826,\n",
            "       0.35217992, 0.3764292 , 0.39917075, 0.42103279, 0.44151275]), 'ndcg': array([0.16773464, 0.21975123, 0.2448806 , 0.26228229, 0.27526914,\n",
            "       0.2859624 , 0.29460016, 0.30218068, 0.30907739, 0.31524247]), 'precision': array([0.16773464, 0.10987561, 0.08652678, 0.07359593, 0.06490765,\n",
            "       0.05869665, 0.0537756 , 0.04989634, 0.04678142, 0.04415128]), 'MAP': array([0.16773464, 0.19374293, 0.20701931, 0.21572015, 0.22175106,\n",
            "       0.226358  , 0.22982218, 0.23266488, 0.23509399, 0.23714199]), 'AUC': np.float64(0.7655303890410461)}\n",
            "epoch_contrastive_loss:  0.014413774220479859\n",
            "evalutate_time:  04:28 min\n",
            "\n",
            "\n",
            "Epoch: 12,  Loss: 0.3408, Contrastive: 0.0157, Emb: 0.0000,  Time: 03:41 min, LR: 0.001412\n",
            "testing: \n",
            "evaluation_result:  {'hit_ratio': array([0.15919085, 0.21095615, 0.24965448, 0.28382963, 0.31674834,\n",
            "       0.34225405, 0.36763412, 0.39050132, 0.4104787 , 0.43271768]), 'ndcg': array([0.15919085, 0.21095615, 0.23537208, 0.25245965, 0.26663697,\n",
            "       0.27650393, 0.28554449, 0.29316689, 0.29946905, 0.30616365]), 'precision': array([0.15919085, 0.10547808, 0.08321816, 0.07095741, 0.06334967,\n",
            "       0.05704234, 0.05251916, 0.04881266, 0.04560874, 0.04327177]), 'MAP': array([0.15919085, 0.1850735 , 0.19797294, 0.20651673, 0.21310047,\n",
            "       0.21735143, 0.22097715, 0.22383555, 0.22605526, 0.22827916]), 'AUC': np.float64(0.7607784271038147)}\n",
            "epoch_contrastive_loss:  0.013645726063894846\n",
            "evalutate_time:  04:28 min\n",
            "\n",
            "\n",
            "Epoch: 13,  Loss: 0.3363, Contrastive: 0.0162, Emb: 0.0000,  Time: 03:41 min, LR: 0.001271\n",
            "testing: \n",
            "evaluation_result:  {'hit_ratio': array([0.16786028, 0.21761528, 0.25669054, 0.29187084, 0.32227667,\n",
            "       0.34778238, 0.37316246, 0.39854253, 0.42065586, 0.44377434]), 'ndcg': array([0.16786028, 0.21761528, 0.24226902, 0.25985917, 0.27295425,\n",
            "       0.28282121, 0.29186177, 0.3003218 , 0.30729778, 0.31425713]), 'precision': array([0.16786028, 0.10880764, 0.08556351, 0.07296771, 0.06445533,\n",
            "       0.05796373, 0.05330892, 0.04981782, 0.04673954, 0.04437743]), 'MAP': array([0.16786028, 0.19273778, 0.20576287, 0.21455794, 0.22063911,\n",
            "       0.22489006, 0.22851579, 0.2316883 , 0.23414533, 0.23645718]), 'AUC': np.float64(0.7600549421830354)}\n",
            "epoch_contrastive_loss:  0.01348686023127465\n",
            "evalutate_time:  04:29 min\n",
            "\n",
            "\n",
            "Epoch: 14,  Loss: 0.3316, Contrastive: 0.0164, Emb: 0.0000,  Time: 03:41 min, LR: 0.001144\n",
            "testing: \n",
            "evaluation_result:  {'hit_ratio': array([0.16547305, 0.21535369, 0.25631361, 0.28986054, 0.31926121,\n",
            "       0.34677723, 0.37090087, 0.39439628, 0.41688654, 0.43611006]), 'ndcg': array([0.16547305, 0.21535369, 0.24119652, 0.25796998, 0.27063217,\n",
            "       0.28127682, 0.28986983, 0.29770163, 0.30479652, 0.31058338]), 'precision': array([0.16547305, 0.10767684, 0.08543787, 0.07246513, 0.06385224,\n",
            "       0.05779621, 0.05298584, 0.04929954, 0.04632073, 0.04361101]), 'MAP': array([0.16547305, 0.19041337, 0.20406668, 0.21245341, 0.21833354,\n",
            "       0.22291955, 0.22636578, 0.22930271, 0.23180162, 0.23372398]), 'AUC': np.float64(0.7591285843150064)}\n",
            "epoch_contrastive_loss:  0.013763490960829787\n",
            "evalutate_time:  04:28 min\n",
            "\n",
            "\n",
            "Epoch: 15,  Loss: 0.3280, Contrastive: 0.0159, Emb: 0.0000,  Time: 03:42 min, LR: 0.001029\n",
            "testing: \n",
            "evaluation_result:  {'hit_ratio': array([0.16584998, 0.21849479, 0.25505717, 0.29237341, 0.32353311,\n",
            "       0.35343636, 0.37919337, 0.40168363, 0.42467647, 0.44641287]), 'ndcg': array([0.16584998, 0.21849479, 0.24156308, 0.2602212 , 0.27364095,\n",
            "       0.28520911, 0.29438394, 0.3018807 , 0.30913413, 0.31567744]), 'precision': array([0.16584998, 0.10924739, 0.08501906, 0.07309335, 0.06470662,\n",
            "       0.05890606, 0.05417048, 0.05021045, 0.04718627, 0.04464129]), 'MAP': array([0.16584998, 0.19217238, 0.20435984, 0.21368891, 0.21992084,\n",
            "       0.22490472, 0.22858429, 0.23139557, 0.23395033, 0.23612397]), 'AUC': np.float64(0.7678122991526446)}\n",
            "epoch_contrastive_loss:  0.012493233048608379\n",
            "evalutate_time:  04:28 min\n",
            "\n",
            "\n",
            "Epoch: 16,  Loss: 0.3247, Contrastive: 0.0152, Emb: 0.0000,  Time: 03:41 min, LR: 0.000927\n",
            "testing: \n",
            "evaluation_result:  {'hit_ratio': array([0.16007036, 0.21258952, 0.25078527, 0.28684508, 0.31800477,\n",
            "       0.34677723, 0.37605227, 0.39464757, 0.41826863, 0.44000503]), 'ndcg': array([0.16007036, 0.21258952, 0.23668836, 0.25471826, 0.26813801,\n",
            "       0.27926872, 0.2896967 , 0.29589513, 0.30334674, 0.30989005]), 'precision': array([0.16007036, 0.10629476, 0.08359509, 0.07171127, 0.06360095,\n",
            "       0.05779621, 0.05372175, 0.04933095, 0.04647429, 0.0440005 ]), 'MAP': array([0.16007036, 0.18632994, 0.19906186, 0.20807681, 0.21430875,\n",
            "       0.21910416, 0.22328631, 0.22561072, 0.22823528, 0.23040892]), 'AUC': np.float64(0.7638993659252001)}\n",
            "epoch_contrastive_loss:  0.012456675200530934\n",
            "evalutate_time:  04:27 min\n",
            "\n",
            "\n",
            "Epoch: 17,  Loss: 0.3211, Contrastive: 0.0151, Emb: 0.0000,  Time: 03:42 min, LR: 0.000834\n",
            "testing: \n",
            "evaluation_result:  {'hit_ratio': array([0.16195502, 0.21422289, 0.25555974, 0.28998618, 0.31888428,\n",
            "       0.34853625, 0.37341375, 0.39716045, 0.41839427, 0.43824601]), 'ndcg': array([0.16195502, 0.21422289, 0.24030354, 0.25751676, 0.26996249,\n",
            "       0.28143344, 0.29029498, 0.29821055, 0.30490908, 0.31088505]), 'precision': array([0.16195502, 0.10711145, 0.08518658, 0.07249654, 0.06377686,\n",
            "       0.05808937, 0.05334482, 0.04964506, 0.04648825, 0.0438246 ]), 'MAP': array([0.16195502, 0.18808896, 0.20186791, 0.21047452, 0.21625414,\n",
            "       0.22119613, 0.22475006, 0.2277184 , 0.23007771, 0.23206288]), 'AUC': np.float64(0.7657701803695188)}\n",
            "epoch_contrastive_loss:  0.012521702374908187\n",
            "evalutate_time:  04:27 min\n",
            "\n",
            "\n",
            "Epoch: 18,  Loss: 0.3177, Contrastive: 0.0149, Emb: 0.0000,  Time: 03:42 min, LR: 0.000750\n",
            "testing: \n",
            "evaluation_result:  {'hit_ratio': array([0.16597563, 0.2162332 , 0.25669054, 0.29011182, 0.32039201,\n",
            "       0.35205428, 0.37567534, 0.40005026, 0.42316874, 0.44427692]), 'ndcg': array([0.16597563, 0.2162332 , 0.24175894, 0.25846958, 0.27151055,\n",
            "       0.28375918, 0.29217317, 0.30029815, 0.30759122, 0.31394541]), 'precision': array([0.16597563, 0.1081166 , 0.08556351, 0.07252796, 0.0640784 ,\n",
            "       0.05867571, 0.05366791, 0.05000628, 0.04701875, 0.04442769]), 'MAP': array([0.16597563, 0.19110441, 0.20459019, 0.21294551, 0.21900155,\n",
            "       0.22427859, 0.22765303, 0.2306999 , 0.23326862, 0.23537943]), 'AUC': np.float64(0.7648854721445019)}\n",
            "epoch_contrastive_loss:  0.01118102129400959\n",
            "evalutate_time:  04:28 min\n",
            "\n",
            "\n",
            "Epoch: 19,  Loss: 0.3154, Contrastive: 0.0145, Emb: 0.0000,  Time: 03:43 min, LR: 0.000675\n",
            "testing: \n",
            "evaluation_result:  {'hit_ratio': array([0.16396532, 0.21321774, 0.25241865, 0.28923232, 0.32039201,\n",
            "       0.34929011, 0.37077522, 0.39753738, 0.42040457, 0.44302048]), 'ndcg': array([0.16396532, 0.21321774, 0.23795076, 0.25635759, 0.26977734,\n",
            "       0.28095665, 0.28860981, 0.29753052, 0.30474432, 0.31155239]), 'precision': array([0.16396532, 0.10660887, 0.08413955, 0.07230808, 0.0640784 ,\n",
            "       0.05821502, 0.05296789, 0.04969217, 0.04671162, 0.04430205]), 'MAP': array([0.16396532, 0.18859153, 0.2016585 , 0.21086192, 0.21709386,\n",
            "       0.22191021, 0.22497951, 0.22832478, 0.23086558, 0.23312717]), 'AUC': np.float64(0.7617796300113973)}\n",
            "epoch_contrastive_loss:  0.01132545391068099\n",
            "evalutate_time:  04:28 min\n",
            "\n",
            "\n",
            "Epoch: 20,  Loss: 0.3150, Contrastive: 0.0144, Emb: 0.0000,  Time: 03:42 min, LR: 0.000608\n",
            "testing: \n",
            "evaluation_result:  {'hit_ratio': array([0.16409097, 0.21384596, 0.25442895, 0.29111697, 0.31926121,\n",
            "       0.3484106 , 0.37316246, 0.39716045, 0.41789169, 0.43912552]), 'ndcg': array([0.16409097, 0.21384596, 0.23945098, 0.25779499, 0.26991605,\n",
            "       0.28119258, 0.29000936, 0.29800869, 0.30454867, 0.31094069]), 'precision': array([0.16409097, 0.10692298, 0.08480965, 0.07277924, 0.06385224,\n",
            "       0.05806843, 0.05330892, 0.04964506, 0.04643241, 0.04391255]), 'MAP': array([0.16409097, 0.18896846, 0.20249613, 0.21166813, 0.21729698,\n",
            "       0.22215521, 0.22569119, 0.22869094, 0.23099441, 0.23311779]), 'AUC': np.float64(0.7627640882098972)}\n",
            "epoch_contrastive_loss:  0.011288801829020182\n",
            "evalutate_time:  04:28 min\n",
            "\n",
            "\n",
            "Epoch: 21,  Loss: 0.3121, Contrastive: 0.0145, Emb: 0.0000,  Time: 03:42 min, LR: 0.000547\n",
            "testing: \n",
            "evaluation_result:  {'hit_ratio': array([0.15919085, 0.21083051, 0.25153914, 0.28634251, 0.31762784,\n",
            "       0.34727981, 0.37228295, 0.39477321, 0.41537882, 0.4373665 ]), 'ndcg': array([0.15919085, 0.21083051, 0.23651479, 0.25391648, 0.26739034,\n",
            "       0.27886128, 0.28776758, 0.29526434, 0.30176468, 0.30838364]), 'precision': array([0.15919085, 0.10541525, 0.08384638, 0.07158563, 0.06352557,\n",
            "       0.05787997, 0.05318328, 0.04934665, 0.0461532 , 0.04373665]), 'MAP': array([0.15919085, 0.18501068, 0.19858022, 0.20728107, 0.21353813,\n",
            "       0.21848013, 0.222052  , 0.22486329, 0.2271528 , 0.22935157]), 'AUC': np.float64(0.7641430474952191)}\n",
            "epoch_contrastive_loss:  0.010702519352355647\n",
            "evalutate_time:  04:28 min\n",
            "\n",
            "\n",
            "Epoch: 22,  Loss: 0.3109, Contrastive: 0.0145, Emb: 0.0000,  Time: 03:42 min, LR: 0.000492\n",
            "testing: \n",
            "evaluation_result:  {'hit_ratio': array([0.160196  , 0.21296645, 0.25040834, 0.28596557, 0.31762784,\n",
            "       0.34740545, 0.37391632, 0.39640658, 0.419902  , 0.43962809]), 'ndcg': array([0.160196  , 0.21296645, 0.23658966, 0.25436827, 0.26800447,\n",
            "       0.27952402, 0.28896738, 0.29646414, 0.30387611, 0.30981426]), 'precision': array([0.160196  , 0.10648323, 0.08346945, 0.07149139, 0.06352557,\n",
            "       0.05790091, 0.05341662, 0.04955082, 0.04665578, 0.04396281]), 'MAP': array([0.160196  , 0.18658123, 0.19906186, 0.20795117, 0.21428362,\n",
            "       0.21924656, 0.22303382, 0.2258451 , 0.22845571, 0.23042832]), 'AUC': np.float64(0.7627497252732983)}\n",
            "epoch_contrastive_loss:  0.01075184585467454\n",
            "evalutate_time:  04:28 min\n",
            "\n",
            "\n",
            "Epoch: 23,  Loss: 0.3101, Contrastive: 0.0146, Emb: 0.0000,  Time: 03:42 min, LR: 0.000443\n",
            "testing: \n",
            "evaluation_result:  {'hit_ratio': array([0.16296017, 0.21698706, 0.25354944, 0.28822716, 0.31800477,\n",
            "       0.34526951, 0.37115215, 0.39364242, 0.41688654, 0.44000503]), 'ndcg': array([0.16296017, 0.21698706, 0.24005535, 0.25739422, 0.27021873,\n",
            "       0.28076617, 0.28998576, 0.29748251, 0.30481522, 0.31177457]), 'precision': array([0.16296017, 0.10849353, 0.08451648, 0.07205679, 0.06360095,\n",
            "       0.05754492, 0.05302174, 0.0492053 , 0.04632073, 0.0440005 ]), 'MAP': array([0.16296017, 0.18997361, 0.20216108, 0.21083051, 0.21678603,\n",
            "       0.22133015, 0.22502767, 0.22783895, 0.23042164, 0.23273348]), 'AUC': np.float64(0.7618638966088311)}\n",
            "epoch_contrastive_loss:  0.010891112373284405\n",
            "evalutate_time:  04:28 min\n",
            "\n",
            "\n",
            "Epoch: 24,  Loss: 0.3077, Contrastive: 0.0140, Emb: 0.0000,  Time: 03:42 min, LR: 0.000399\n",
            "testing: \n",
            "evaluation_result:  {'hit_ratio': array([0.15667797, 0.20932278, 0.24978012, 0.28282448, 0.31524061,\n",
            "       0.34501822, 0.37353939, 0.39829124, 0.42090715, 0.44226662]), 'ndcg': array([0.15667797, 0.20932278, 0.23484852, 0.2513707 , 0.26533157,\n",
            "       0.27685112, 0.28701056, 0.29526118, 0.30239571, 0.30882555]), 'precision': array([0.15667797, 0.10466139, 0.08326004, 0.07070612, 0.06304812,\n",
            "       0.05750304, 0.05336277, 0.04978641, 0.04676746, 0.04422666]), 'MAP': array([0.15667797, 0.18300038, 0.19648616, 0.20474725, 0.21123047,\n",
            "       0.21619341, 0.22026786, 0.22336184, 0.22587472, 0.22801067]), 'AUC': np.float64(0.7614672035809356)}\n",
            "epoch_contrastive_loss:  0.010771797208618077\n",
            "evalutate_time:  04:27 min\n",
            "\n",
            "\n",
            "Epoch: 25,  Loss: 0.3076, Contrastive: 0.0139, Emb: 0.0000,  Time: 03:43 min, LR: 0.000359\n",
            "testing: \n",
            "evaluation_result:  {'hit_ratio': array([0.16497047, 0.21887172, 0.25782133, 0.29036311, 0.32164845,\n",
            "       0.35004398, 0.3744189 , 0.40080412, 0.42128408, 0.44226662]), 'ndcg': array([0.16497047, 0.21887172, 0.24344619, 0.25971708, 0.27319094,\n",
            "       0.28417583, 0.29285835, 0.30165343, 0.30811413, 0.31443051]), 'precision': array([0.16497047, 0.10943586, 0.08594044, 0.07259078, 0.06432969,\n",
            "       0.05834066, 0.05348841, 0.05010052, 0.04680934, 0.04422666]), 'MAP': array([0.16497047, 0.1919211 , 0.2049043 , 0.21303975, 0.21929681,\n",
            "       0.2240294 , 0.22751153, 0.23080969, 0.23308524, 0.23518349]), 'AUC': np.float64(0.766393638028771)}\n",
            "epoch_contrastive_loss:  0.010451516713060084\n",
            "evalutate_time:  04:28 min\n",
            "\n",
            "\n",
            "Epoch: 26,  Loss: 0.3058, Contrastive: 0.0136, Emb: 0.0000,  Time: 03:42 min, LR: 0.000323\n",
            "testing: \n",
            "evaluation_result:  {'hit_ratio': array([0.160196  , 0.21083051, 0.25179043, 0.28370398, 0.31624576,\n",
            "       0.34413871, 0.36838799, 0.39364242, 0.41939942, 0.43899987]), 'ndcg': array([0.160196  , 0.21083051, 0.23667334, 0.25263012, 0.2666451 ,\n",
            "       0.27743556, 0.28607333, 0.29449147, 0.3026169 , 0.30851723]), 'precision': array([0.160196  , 0.10541525, 0.08393014, 0.070926  , 0.06324915,\n",
            "       0.05735645, 0.05262686, 0.0492053 , 0.04659994, 0.04389999]), 'MAP': array([0.160196  , 0.18551326, 0.19916656, 0.20714495, 0.21365331,\n",
            "       0.21830213, 0.22176631, 0.22492312, 0.22778501, 0.22974505]), 'AUC': np.float64(0.7617151050124475)}\n",
            "epoch_contrastive_loss:  0.010984762889584379\n",
            "evalutate_time:  04:30 min\n",
            "\n",
            "\n",
            "Epoch: 27,  Loss: 0.3043, Contrastive: 0.0142, Emb: 0.0000,  Time: 03:43 min, LR: 0.000291\n",
            "testing: \n",
            "evaluation_result:  {'hit_ratio': array([0.16044729, 0.21171001, 0.25266993, 0.2874733 , 0.31624576,\n",
            "       0.34665159, 0.37203166, 0.3970348 , 0.419902  , 0.44377434]), 'ndcg': array([0.16044729, 0.21171001, 0.23755285, 0.25495453, 0.26734615,\n",
            "       0.27910873, 0.2881493 , 0.29648368, 0.30369747, 0.31088377]), 'precision': array([0.16044729, 0.10585501, 0.08422331, 0.07186833, 0.06324915,\n",
            "       0.05777526, 0.05314738, 0.04962935, 0.04665578, 0.04437743]), 'MAP': array([0.16044729, 0.18607865, 0.19973196, 0.2084328 , 0.21418729,\n",
            "       0.21925493, 0.22288066, 0.22600605, 0.22854685, 0.23093408]), 'AUC': np.float64(0.7649054836114241)}\n",
            "epoch_contrastive_loss:  0.010152584344651255\n",
            "evalutate_time:  04:28 min\n",
            "\n",
            "\n",
            "Epoch: 28,  Loss: 0.3029, Contrastive: 0.0139, Emb: 0.0000,  Time: 03:43 min, LR: 0.000262\n",
            "testing: \n",
            "evaluation_result:  {'hit_ratio': array([0.1624576 , 0.21321774, 0.2554341 , 0.28797588, 0.31461239,\n",
            "       0.3432592 , 0.37291117, 0.39778867, 0.42078151, 0.44126147]), 'ndcg': array([0.1624576 , 0.21321774, 0.2398533 , 0.25612419, 0.26759591,\n",
            "       0.27867801, 0.28924025, 0.29753275, 0.30478618, 0.31095127]), 'precision': array([0.1624576 , 0.10660887, 0.0851447 , 0.07199397, 0.06292248,\n",
            "       0.05720987, 0.05327302, 0.04972358, 0.0467535 , 0.04412615]), 'MAP': array([0.1624576 , 0.18783767, 0.20190979, 0.21004523, 0.21537253,\n",
            "       0.220147  , 0.224383  , 0.22749269, 0.23004745, 0.23209544]), 'AUC': np.float64(0.7652234249403681)}\n",
            "epoch_contrastive_loss:  0.010123094972518702\n",
            "evalutate_time:  04:28 min\n",
            "\n",
            "\n",
            "Epoch: 29,  Loss: 0.3022, Contrastive: 0.0144, Emb: 0.0000,  Time: 03:41 min, LR: 0.000236\n",
            "testing: \n",
            "evaluation_result:  {'hit_ratio': array([0.16145244, 0.21422289, 0.25405202, 0.28847845, 0.31624576,\n",
            "       0.34288227, 0.37027265, 0.39716045, 0.41939942, 0.4405076 ]), 'ndcg': array([0.16145244, 0.21422289, 0.23935227, 0.25656549, 0.26852422,\n",
            "       0.27882863, 0.28858528, 0.29754788, 0.30456349, 0.31091769]), 'precision': array([0.16145244, 0.10711145, 0.08468401, 0.07211961, 0.06324915,\n",
            "       0.05714705, 0.05289609, 0.04964506, 0.04659994, 0.04405076]), 'MAP': array([0.16145244, 0.18783767, 0.20111404, 0.20972065, 0.21527411,\n",
            "       0.21971353, 0.22362644, 0.22698742, 0.22945841, 0.23156923]), 'AUC': np.float64(0.7642267157882476)}\n",
            "epoch_contrastive_loss:  0.010228935961744614\n",
            "evalutate_time:  04:27 min\n",
            "\n",
            "\n",
            "Epoch: 30,  Loss: 0.3015, Contrastive: 0.0139, Emb: 0.0000,  Time: 03:42 min, LR: 0.000212\n",
            "testing: \n",
            "evaluation_result:  {'hit_ratio': array([0.16484483, 0.21372032, 0.25216736, 0.29023747, 0.31850735,\n",
            "       0.34614901, 0.37265988, 0.39866817, 0.42203794, 0.44465385]), 'ndcg': array([0.16484483, 0.21372032, 0.2379777 , 0.25701275, 0.26918793,\n",
            "       0.27988118, 0.28932455, 0.29799398, 0.30536632, 0.31217439]), 'precision': array([0.16484483, 0.10686016, 0.08405579, 0.07255937, 0.06370147,\n",
            "       0.0576915 , 0.05323713, 0.04983352, 0.0468931 , 0.04446539]), 'MAP': array([0.16484483, 0.18928257, 0.20209825, 0.21161578, 0.21726976,\n",
            "       0.2218767 , 0.22566397, 0.228915  , 0.23151165, 0.23377324]), 'AUC': np.float64(0.7651119935864292)}\n",
            "epoch_contrastive_loss:  0.010070444882980414\n",
            "evalutate_time:  04:28 min\n",
            "\n",
            "\n",
            "Epoch: 31,  Loss: 0.2997, Contrastive: 0.0138, Emb: 0.0000,  Time: 03:42 min, LR: 0.000191\n",
            "testing: \n",
            "evaluation_result:  {'hit_ratio': array([0.1624576 , 0.21547933, 0.25455459, 0.28734766, 0.31599447,\n",
            "       0.34300792, 0.36939314, 0.39376806, 0.41814298, 0.43899987]), 'ndcg': array([0.1624576 , 0.21547933, 0.24013308, 0.25652961, 0.26886712,\n",
            "       0.27931735, 0.28871595, 0.29684093, 0.30453036, 0.31080891]), 'precision': array([0.1624576 , 0.10773967, 0.08485153, 0.07183691, 0.06319889,\n",
            "       0.05716799, 0.05277045, 0.04922101, 0.04646033, 0.04389999]), 'MAP': array([0.1624576 , 0.18896846, 0.20199355, 0.21019182, 0.21592118,\n",
            "       0.22042342, 0.22419274, 0.2272396 , 0.22994793, 0.23203362]), 'AUC': np.float64(0.7642063039783249)}\n",
            "epoch_contrastive_loss:  0.010090126530341213\n",
            "evalutate_time:  04:27 min\n",
            "\n",
            "\n",
            "Epoch: 32,  Loss: 0.2975, Contrastive: 0.0137, Emb: 0.0000,  Time: 03:42 min, LR: 0.000172\n",
            "testing: \n",
            "evaluation_result:  {'hit_ratio': array([0.16522176, 0.21573062, 0.2534238 , 0.28709637, 0.31498932,\n",
            "       0.34263098, 0.37190602, 0.39477321, 0.41688654, 0.43937681]), 'ndcg': array([0.16522176, 0.21573062, 0.23951237, 0.25634865, 0.26836149,\n",
            "       0.27905475, 0.28948273, 0.29710512, 0.3040811 , 0.31085135]), 'precision': array([0.16522176, 0.10786531, 0.0844746 , 0.07177409, 0.06299786,\n",
            "       0.05710516, 0.05312943, 0.04934665, 0.04632073, 0.04393768]), 'MAP': array([0.16522176, 0.19047619, 0.20304058, 0.21145873, 0.21703732,\n",
            "       0.22164426, 0.22582641, 0.22868481, 0.23114184, 0.23339087]), 'AUC': np.float64(0.7654800436986383)}\n",
            "epoch_contrastive_loss:  0.01013235439590755\n",
            "evalutate_time:  04:27 min\n",
            "\n",
            "\n",
            "Epoch: 33,  Loss: 0.2987, Contrastive: 0.0141, Emb: 0.0000,  Time: 03:42 min, LR: 0.000155\n",
            "testing: \n",
            "evaluation_result:  {'hit_ratio': array([0.16220631, 0.21472547, 0.25417766, 0.28709637, 0.31385852,\n",
            "       0.34175148, 0.36876492, 0.39339113, 0.41952507, 0.44000503]), 'ndcg': array([0.16220631, 0.21472547, 0.23961703, 0.25607638, 0.26760222,\n",
            "       0.27839268, 0.28801507, 0.2962238 , 0.30446814, 0.31063322]), 'precision': array([0.16220631, 0.10736273, 0.08472589, 0.07177409, 0.0627717 ,\n",
            "       0.05695858, 0.0526807 , 0.04917389, 0.0466139 , 0.0440005 ]), 'MAP': array([0.16220631, 0.18846589, 0.20161662, 0.2098463 , 0.21519873,\n",
            "       0.21984755, 0.22370662, 0.22678489, 0.22968866, 0.23173666]), 'AUC': np.float64(0.7632238606764682)}\n",
            "epoch_contrastive_loss:  0.01002927797861279\n",
            "evalutate_time:  04:27 min\n",
            "\n",
            "\n",
            "Epoch: 34,  Loss: 0.2976, Contrastive: 0.0137, Emb: 0.0000,  Time: 03:42 min, LR: 0.000139\n",
            "testing: \n",
            "evaluation_result:  {'hit_ratio': array([0.16396532, 0.21598191, 0.25279558, 0.285463  , 0.31348159,\n",
            "       0.34363614, 0.37115215, 0.3930142 , 0.41575575, 0.43711522]), 'ndcg': array([0.16396532, 0.21598191, 0.23920875, 0.25554246, 0.26760941,\n",
            "       0.27927478, 0.28907618, 0.29636353, 0.30353769, 0.30996753]), 'precision': array([0.16396532, 0.10799095, 0.08426519, 0.07136575, 0.06269632,\n",
            "       0.05727269, 0.05302174, 0.04912677, 0.04619508, 0.04371152]), 'MAP': array([0.16396532, 0.18997361, 0.20224484, 0.21041169, 0.21601541,\n",
            "       0.22104117, 0.22497203, 0.22770478, 0.23023162, 0.23236757]), 'AUC': np.float64(0.7639484028911142)}\n",
            "epoch_contrastive_loss:  0.009870491337977232\n",
            "evalutate_time:  04:27 min\n",
            "\n",
            "\n",
            "Epoch: 35,  Loss: 0.2992, Contrastive: 0.0140, Emb: 0.0000,  Time: 03:42 min, LR: 0.000125\n",
            "testing: \n",
            "evaluation_result:  {'hit_ratio': array([0.16145244, 0.21459982, 0.25279558, 0.28785023, 0.31360724,\n",
            "       0.3432592 , 0.36901621, 0.39351677, 0.41701219, 0.43774344]), 'ndcg': array([0.16145244, 0.21459982, 0.23869866, 0.25622599, 0.26731893,\n",
            "       0.27878987, 0.2879647 , 0.29613156, 0.30354354, 0.30978426]), 'precision': array([0.16145244, 0.10729991, 0.08426519, 0.07196256, 0.06272145,\n",
            "       0.05720987, 0.0527166 , 0.0491896 , 0.04633469, 0.04377434]), 'MAP': array([0.16145244, 0.18802613, 0.20075805, 0.20952172, 0.21467312,\n",
            "       0.21961511, 0.22329468, 0.22635725, 0.22896786, 0.23104098]), 'AUC': np.float64(0.7643008812234571)}\n",
            "epoch_contrastive_loss:  0.00982505260848455\n",
            "evalutate_time:  04:27 min\n",
            "\n",
            "\n",
            "Epoch: 36,  Loss: 0.2965, Contrastive: 0.0136, Emb: 0.0000,  Time: 03:42 min, LR: 0.000113\n",
            "testing: \n",
            "evaluation_result:  {'hit_ratio': array([0.1613268 , 0.21158437, 0.25392637, 0.28496042, 0.31498932,\n",
            "       0.3432592 , 0.37102651, 0.39515014, 0.41889685, 0.43774344]), 'ndcg': array([0.1613268 , 0.21158437, 0.2382992 , 0.25381622, 0.26674897,\n",
            "       0.27768525, 0.28757616, 0.29561738, 0.30310863, 0.30878201]), 'precision': array([0.1613268 , 0.10579218, 0.08464212, 0.07124011, 0.06299786,\n",
            "       0.05720987, 0.05300379, 0.04939377, 0.04654409, 0.04377434]), 'MAP': array([0.1613268 , 0.18645558, 0.20056959, 0.2083281 , 0.21433388,\n",
            "       0.21904552, 0.22301228, 0.22602774, 0.22866626, 0.23055092]), 'AUC': np.float64(0.7645509910928261)}\n",
            "epoch_contrastive_loss:  0.009950703590191783\n",
            "evalutate_time:  04:28 min\n",
            "\n",
            "\n",
            "Epoch: 37,  Loss: 0.2959, Contrastive: 0.0137, Emb: 0.0000,  Time: 03:43 min, LR: 0.000101\n",
            "testing: \n",
            "evaluation_result:  {'hit_ratio': array([0.1613268 , 0.21485111, 0.252293  , 0.28571429, 0.31285337,\n",
            "       0.34175148, 0.37152909, 0.39263727, 0.41650961, 0.43899987]), 'ndcg': array([0.1613268 , 0.21485111, 0.23847431, 0.25518496, 0.26687313,\n",
            "       0.27805244, 0.28865944, 0.2956955 , 0.30322638, 0.30999663]), 'precision': array([0.1613268 , 0.10742556, 0.08409767, 0.07142857, 0.06257067,\n",
            "       0.05695858, 0.05307558, 0.04907966, 0.04627885, 0.04389999]), 'MAP': array([0.1613268 , 0.18808896, 0.20056959, 0.20892491, 0.21435272,\n",
            "       0.21916907, 0.22342302, 0.22606154, 0.22871402, 0.23096305]), 'AUC': np.float64(0.7642586294873047)}\n",
            "epoch_contrastive_loss:  0.009960799034507502\n",
            "evalutate_time:  04:27 min\n",
            "\n",
            "\n",
            "Epoch: 38,  Loss: 0.2966, Contrastive: 0.0138, Emb: 0.0000,  Time: 03:41 min, LR: 0.000091\n",
            "testing: \n",
            "evaluation_result:  {'hit_ratio': array([0.16170373, 0.21409725, 0.25216736, 0.28194497, 0.31209951,\n",
            "       0.34288227, 0.37140344, 0.39414499, 0.41726348, 0.43862294]), 'ndcg': array([0.16170373, 0.21409725, 0.23811681, 0.25300562, 0.26599247,\n",
            "       0.27790087, 0.28806032, 0.29564083, 0.3029339 , 0.30936374]), 'precision': array([0.16170373, 0.10704862, 0.08405579, 0.07048624, 0.0624199 ,\n",
            "       0.05714705, 0.05305763, 0.04926812, 0.04636261, 0.04386229]), 'MAP': array([0.16170373, 0.18790049, 0.20059053, 0.20803493, 0.21406584,\n",
            "       0.2191963 , 0.22327075, 0.22611344, 0.22868216, 0.23081811]), 'AUC': np.float64(0.763062774871739)}\n",
            "epoch_contrastive_loss:  0.009842491679130093\n",
            "evalutate_time:  04:27 min\n",
            "\n",
            "\n",
            "Epoch: 39,  Loss: 0.2967, Contrastive: 0.0132, Emb: 0.0000,  Time: 03:42 min, LR: 0.000082\n",
            "testing: \n",
            "evaluation_result:  {'hit_ratio': array([0.16157809, 0.21472547, 0.25078527, 0.28508607, 0.31310466,\n",
            "       0.34388742, 0.36775977, 0.3950245 , 0.41826863, 0.43837165]), 'ndcg': array([0.16157809, 0.21472547, 0.23747667, 0.25462707, 0.26669402,\n",
            "       0.27860242, 0.28710592, 0.29619416, 0.30352687, 0.30957848]), 'precision': array([0.16157809, 0.10736273, 0.08359509, 0.07127152, 0.06262093,\n",
            "       0.05731457, 0.05253711, 0.04937806, 0.04647429, 0.04383717]), 'MAP': array([0.16157809, 0.18815178, 0.20017171, 0.20874691, 0.21435063,\n",
            "       0.21948109, 0.22289143, 0.22629952, 0.2288822 , 0.2308925 ]), 'AUC': np.float64(0.7621987279606324)}\n",
            "epoch_contrastive_loss:  0.009817278500468958\n",
            "evalutate_time:  04:30 min\n",
            "\n",
            "\n",
            "Epoch: 40,  Loss: 0.2967, Contrastive: 0.0136, Emb: 0.0000,  Time: 03:44 min, LR: 0.000074\n",
            "testing: \n",
            "evaluation_result:  {'hit_ratio': array([0.16308581, 0.21359467, 0.25468024, 0.28684508, 0.31172258,\n",
            "       0.34162583, 0.36675462, 0.39213469, 0.4136198 , 0.43749215]), 'ndcg': array([0.16308581, 0.21359467, 0.23951678, 0.2555992 , 0.26631335,\n",
            "       0.27788151, 0.28683257, 0.29529259, 0.30207039, 0.30925668]), 'precision': array([0.16308581, 0.10679734, 0.08489341, 0.07171127, 0.06234452,\n",
            "       0.05693764, 0.05239352, 0.04901684, 0.04595776, 0.04374921]), 'MAP': array([0.16308581, 0.18834024, 0.20203543, 0.21007664, 0.21505214,\n",
            "       0.22003602, 0.22362584, 0.22679835, 0.22918559, 0.23157282]), 'AUC': np.float64(0.7630325402926839)}\n",
            "epoch_contrastive_loss:  0.00985216116163111\n",
            "evalutate_time:  04:30 min\n",
            "\n",
            "\n",
            "Epoch: 41,  Loss: 0.2957, Contrastive: 0.0140, Emb: 0.0000,  Time: 03:44 min, LR: 0.000067\n",
            "testing: \n",
            "evaluation_result:  {'hit_ratio': array([0.15831135, 0.21284081, 0.25116221, 0.2843322 , 0.31134565,\n",
            "       0.34074632, 0.36587511, 0.39112954, 0.41613268, 0.43636135]), 'ndcg': array([0.15831135, 0.21284081, 0.23701892, 0.25360392, 0.26523797,\n",
            "       0.27661171, 0.28556276, 0.29398091, 0.30186852, 0.30795796]), 'precision': array([0.15831135, 0.1064204 , 0.08372074, 0.07108305, 0.06226913,\n",
            "       0.05679105, 0.05226787, 0.04889119, 0.04623696, 0.04363614]), 'MAP': array([0.15831135, 0.18557608, 0.19834988, 0.20664238, 0.21204506,\n",
            "       0.21694518, 0.220535  , 0.22369181, 0.22646993, 0.2284928 ]), 'AUC': np.float64(0.763013355006792)}\n",
            "epoch_contrastive_loss:  0.009679070274744715\n",
            "evalutate_time:  04:29 min\n",
            "\n",
            "\n",
            "Epoch: 42,  Loss: 0.2946, Contrastive: 0.0139, Emb: 0.0000,  Time: 03:43 min, LR: 0.000060\n",
            "testing: \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-ecebd3704ee7>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m4\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'testing: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_edgeloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-41-b1b7765fe89e>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(model, dataloader, multi_metrics)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0mpos_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m             \u001b[0mcontrastive_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_contrastive_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m             \u001b[0mtotal_pos_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtotal_pos_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_score\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0mtotal_neg_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtotal_neg_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg_score\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-30-c50a652b6681>\u001b[0m in \u001b[0;36mget_contrastive_loss\u001b[0;34m(x1, x2)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mcontrastive_loss_user\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_score_user\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mttl_score_user\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# print('contrastive_loss_user: ', contrastive_loss_user)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontrastive_loss_user\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}