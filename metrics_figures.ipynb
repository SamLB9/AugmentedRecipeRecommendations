{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faiss-cpu in /Users/jakedugan/miniconda3/envs/recipemag/lib/python3.12/site-packages (1.10.0)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in /Users/jakedugan/miniconda3/envs/recipemag/lib/python3.12/site-packages (from faiss-cpu) (2.2.2)\n",
      "Requirement already satisfied: packaging in /Users/jakedugan/miniconda3/envs/recipemag/lib/python3.12/site-packages (from faiss-cpu) (24.2)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'interaction_matrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 85\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m negative_samples\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# Extract ground truth user interactions\u001b[39;00m\n\u001b[0;32m---> 85\u001b[0m ground_truth \u001b[38;5;241m=\u001b[39m extract_ground_truth(interaction_matrix)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# Apply Leave-One-Out Cross Validation\u001b[39;00m\n\u001b[1;32m     88\u001b[0m train_gt, test_gt \u001b[38;5;241m=\u001b[39m leave_one_out_cross_validation(ground_truth)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'interaction_matrix' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# Ensure FAISS is installed\n",
    "!pip install faiss-cpu\n",
    "\n",
    "def extract_ground_truth(interaction_matrix):\n",
    "    \"\"\"\n",
    "    Extracts ground truth user-recipe interactions from the interaction matrix.\n",
    "    \n",
    "    Parameters:\n",
    "    - interaction_matrix (csr_matrix): User-recipe interaction matrix.\n",
    "\n",
    "    Returns:\n",
    "    - ground_truth (dict): {user_id: [list of actual recipe interactions]}\n",
    "    \"\"\"\n",
    "    ground_truth = {}\n",
    "    for user_id in range(interaction_matrix.shape[0]):\n",
    "        actual_recipes = interaction_matrix[user_id].nonzero()[1].tolist()\n",
    "        ground_truth[user_id] = actual_recipes\n",
    "    return ground_truth\n",
    "\n",
    "def generate_recommendations(interaction_matrix, num_users, k=5):\n",
    "    \"\"\"\n",
    "    Generates top-K recommendations for each user using the recommendation method.\n",
    "\n",
    "    Parameters:\n",
    "    - interaction_matrix (csr_matrix): User-recipe interaction matrix.\n",
    "    - num_users (int): Number of users.\n",
    "    - k (int): Number of recommendations per user.\n",
    "\n",
    "    Returns:\n",
    "    - recommendations (dict): {user_id: [list of recommended recipes]}\n",
    "    \"\"\"\n",
    "    recommendations = {}\n",
    "    for user_id in range(num_users):\n",
    "        recommendations[user_id] = recommend_recipes(user_id, k=k)\n",
    "    return recommendations\n",
    "\n",
    "def leave_one_out_cross_validation(ground_truth):\n",
    "    \"\"\"\n",
    "    Performs Leave-One-Out Cross-Validation (LOO) by holding out one interaction per user.\n",
    "\n",
    "    Parameters:\n",
    "    - ground_truth (dict): User interactions.\n",
    "\n",
    "    Returns:\n",
    "    - train_gt (dict): Training set interactions.\n",
    "    - test_gt (dict): Held-out interactions per user.\n",
    "    \"\"\"\n",
    "    train_gt, test_gt = {}, {}\n",
    "    for user, items in ground_truth.items():\n",
    "        if len(items) > 1:\n",
    "            test_gt[user] = [random.choice(items)]  # Hold out one interaction\n",
    "            train_gt[user] = list(set(items) - set(test_gt[user]))  # Remaining items\n",
    "        else:\n",
    "            train_gt[user] = items\n",
    "            test_gt[user] = []\n",
    "    return train_gt, test_gt\n",
    "\n",
    "def random_negative_sampling(ground_truth, num_recipes, neg_samples=100):\n",
    "    \"\"\"\n",
    "    Generates random negative samples for each user.\n",
    "\n",
    "    Parameters:\n",
    "    - ground_truth (dict): User interactions.\n",
    "    - num_recipes (int): Total number of recipes.\n",
    "    - neg_samples (int): Number of negative samples per user.\n",
    "\n",
    "    Returns:\n",
    "    - negative_samples (dict): {user_id: [list of negative recipe samples]}\n",
    "    \"\"\"\n",
    "    negative_samples = {}\n",
    "    for user, pos_items in ground_truth.items():\n",
    "        all_items = set(range(num_recipes))\n",
    "        neg_items = list(all_items - set(pos_items))\n",
    "        negative_samples[user] = random.sample(neg_items, min(len(neg_items), neg_samples))\n",
    "    return negative_samples\n",
    "\n",
    "# Extract ground truth user interactions\n",
    "ground_truth = extract_ground_truth(interaction_matrix)\n",
    "\n",
    "# Apply Leave-One-Out Cross Validation\n",
    "train_gt, test_gt = leave_one_out_cross_validation(ground_truth)\n",
    "\n",
    "# Apply Random Negative Sampling\n",
    "negative_samples = random_negative_sampling(ground_truth, num_recipes)\n",
    "\n",
    "# Generate recommendations using the existing recommend_recipes function\n",
    "recommendations = generate_recommendations(interaction_matrix, num_users, k=10)\n",
    "\n",
    "def compute_evaluation_metrics(recommendations, ground_truth, k_values):\n",
    "    \"\"\"\n",
    "    Computes Precision@K, Hit Rate@K, NDCG@K, and MAP@K.\n",
    "\n",
    "    Parameters:\n",
    "    - recommendations (dict): {user_id: [recommended_item_ids]}\n",
    "    - ground_truth (dict): {user_id: [actual_item_ids]}\n",
    "    - k_values (list): List of K values.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrames for each metric.\n",
    "    \"\"\"\n",
    "    precision_scores = {k: [] for k in k_values}\n",
    "    hit_rate_scores = {k: [] for k in k_values}\n",
    "    ndcg_scores = {k: [] for k in k_values}\n",
    "    map_scores = {k: [] for k in k_values}\n",
    "\n",
    "    for user, recs in recommendations.items():\n",
    "        actual_items = set(ground_truth.get(user, []))\n",
    "        for k in k_values:\n",
    "            top_k_recs = recs[:k]\n",
    "            relevant_items = set(top_k_recs) & actual_items\n",
    "\n",
    "            precision_scores[k].append(len(relevant_items) / k if k > 0 else 0)\n",
    "            hit_rate_scores[k].append(1 if relevant_items else 0)\n",
    "\n",
    "            dcg = sum((1 / np.log2(idx + 2)) for idx, item in enumerate(top_k_recs) if item in actual_items)\n",
    "            idcg = sum((1 / np.log2(idx + 2)) for idx in range(min(len(actual_items), k)))\n",
    "            ndcg_scores[k].append(dcg / idcg if idcg > 0 else 0)\n",
    "\n",
    "            ap = sum((len(set(top_k_recs[:i+1]) & actual_items) / (i+1)) for i in range(k) if top_k_recs[i] in actual_items)\n",
    "            map_scores[k].append(ap / min(k, len(actual_items)) if actual_items else 0)\n",
    "\n",
    "    df_precision = pd.DataFrame(precision_scores).mean()\n",
    "    df_hit_rate = pd.DataFrame(hit_rate_scores).mean()\n",
    "    df_ndcg = pd.DataFrame(ndcg_scores).mean()\n",
    "    df_map = pd.DataFrame(map_scores).mean()\n",
    "\n",
    "    return df_precision, df_hit_rate, df_ndcg, df_map\n",
    "\n",
    "# Define the K values\n",
    "k_values = np.arange(1, 11)\n",
    "\n",
    "# Compute evaluation metrics\n",
    "df_precision, df_hit_rate, df_ndcg, df_map = compute_evaluation_metrics(recommendations, test_gt, k_values)\n",
    "\n",
    "# Combine results into one DataFrame\n",
    "df_results = pd.concat([df_precision, df_hit_rate, df_ndcg, df_map], axis=1)\n",
    "df_results.columns = [\"Precision@K\", \"Hit Rate@K\", \"NDCG@K\", \"MAP@K\"]\n",
    "df_results.index.name = \"K\"\n",
    "\n",
    "# Display table in notebook\n",
    "from IPython.display import display\n",
    "display(df_results)\n",
    "\n",
    "# Generate Bar Chart for Precision@K\n",
    "plt.figure(figsize=(10, 6))\n",
    "df_precision.plot(kind='bar', color='b', alpha=0.7)\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('Mean Precision@K')\n",
    "plt.title('Average Precision@K')\n",
    "plt.xticks(rotation=0)\n",
    "plt.grid(axis='y')\n",
    "plt.savefig(\"precision_bar_chart.pdf\", format=\"pdf\")  # Save as vector graphic\n",
    "plt.show()\n",
    "\n",
    "# Generate Line Chart for All Metrics\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_values, df_precision, marker='o', linestyle='-', label='Precision@K')\n",
    "plt.plot(k_values, df_hit_rate, marker='s', linestyle='-', label='Hit Rate@K')\n",
    "plt.plot(k_values, df_ndcg, marker='^', linestyle='-', label='NDCG@K')\n",
    "plt.plot(k_values, df_map, marker='d', linestyle='-', label='MAP@K')\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('Metric Value')\n",
    "plt.title('Evaluation Metrics Across K')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.savefig(\"evaluation_metrics_graph.pdf\", format=\"pdf\")  # Save as vector graphic\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recipemag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
