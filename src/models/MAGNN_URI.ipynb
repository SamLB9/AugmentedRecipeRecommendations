{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MDztydWHbJPA",
        "outputId": "c2b1a542-4d1a-41a7-e39b-530538046ba3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Type of r2r_data: <class 'list'>\n",
            "Length of r2r_data: 3\n",
            "Type of r2r_data[0]: <class 'list'>\n",
            "Length of r2r_data[0]: 647146\n",
            "First few entries: [0, 0, 36111, 36111, 36111, 36111, 36111, 36111, 36111, 36111]\n",
            "Type of r2r_data: <class 'list'>\n",
            "Length of r2r_data: 3\n",
            "Each part:\n",
            "  r2r_data[0] length: 647146\n",
            "  r2r_data[1] length: 647146\n",
            "  r2r_data[2] length: 647146\n",
            "Users: 7959 Recipes: 68794\n",
            "Train data size: 94747\n",
            "Loaded pretrained model weights from 'final_model_weights.pth'\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "A more 'real' MAGNN implementation on the URI dataset.\n",
        "\n",
        "1) Loads user->recipe edges + other edges.\n",
        "2) Creates real adjacency structures for (U-R), (R-I), etc.\n",
        "3) Defines metapath expansions: e.g. U->R and U->R->I->R.\n",
        "4) Does link prediction (train/val/test with negative sampling).\n",
        "5) Evaluates with both AUC/AP (pairwise) and ranking-based metrics (Hit@K, NDCG@K, Precision@K, MAP@K) using Leave-One-Out.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from collections import defaultdict, Counter\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score\n",
        "from torch.optim import Adam\n",
        "\n",
        "###############################################################################\n",
        "# 1. DATA LOADER\n",
        "###############################################################################\n",
        "class URIDataLoader(object):\n",
        "    \"\"\"\n",
        "    Loads the URI heterogeneous graph dataset. This includes:\n",
        "      - Node types: [User, Recipe, Ingredient]\n",
        "      - Edges: U-R, R-R, R-I, I-I\n",
        "      - Also constructs adjacency lists for each node type, so we can do\n",
        "        real metapath expansions (U->R->I->R, etc.)\n",
        "      - Train/Val/Test splits for user->recipe edges with negative sampling.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data_dir=\"data\", device=\"cpu\", seed=42, neg_ratio=1):\n",
        "        super(URIDataLoader, self).__init__()\n",
        "        self.data_dir = data_dir\n",
        "        self.device = device\n",
        "        self.seed = seed\n",
        "        self.neg_ratio = neg_ratio\n",
        "\n",
        "        np.random.seed(seed)\n",
        "        random.seed(seed)\n",
        "        torch.manual_seed(seed)\n",
        "\n",
        "        # Basic stats\n",
        "        self.num_users = 0\n",
        "        self.num_recipes = 0\n",
        "        self.num_ingredients = 0\n",
        "\n",
        "        # Edges\n",
        "        self.user_recipe_edges = []           # (user, recipe, rating)\n",
        "        self.recipe_recipe_edges = []         # (r1, r2, sim)\n",
        "        self.recipe_ingredient_edges = []     # (recipe, ingredient, usage_wt)\n",
        "        self.ingredient_ingredient_edges = [] # (i1, i2, cooccur)\n",
        "\n",
        "        # Train / val / test edges (pos + neg)\n",
        "        self.train_pos = []\n",
        "        self.train_neg = []\n",
        "        # self.train_pos = self.train_pos[:10000]  # only 10k\n",
        "        # self.train_neg = self.train_neg[:10000] # only 10k\n",
        "        self.val_pos = []\n",
        "        self.val_neg = []\n",
        "        self.test_pos = []\n",
        "        self.test_neg = []\n",
        "\n",
        "        # Adjacency lists: We'll store them for real aggregator calls\n",
        "        #   user->recipes adjacency (train-based)\n",
        "        self.user2recipes = defaultdict(list)       # user-> [recipes user has in train]\n",
        "        self.recipe2users = defaultdict(list)       # recipe-> [users in train]\n",
        "        self.recipe2ingredients = defaultdict(list) # recipe-> [ingredients]\n",
        "        self.ingredient2recipes = defaultdict(list) # ingredient-> [recipes]\n",
        "\n",
        "        self._load_data()\n",
        "        self._split_user_recipe_edges()\n",
        "\n",
        "        # Build adjacency from train edges\n",
        "        self._build_train_adjacency()\n",
        "\n",
        "    def _load_data(self):\n",
        "        \"\"\"\n",
        "        Loads from .pt files. Adjust as needed for your actual layout.\n",
        "        \"\"\"\n",
        "        # 1) user->recipe edges\n",
        "        user_recipe_data = torch.load(\n",
        "            os.path.join(self.data_dir, \"all_train_val_test_edge_u_rate_r_src_and_dst_and_weight.pt\")\n",
        "        )\n",
        "        ur_src, ur_dst, ur_wt = user_recipe_data[0]\n",
        "        ur_src = np.array(ur_src, dtype=np.int32)\n",
        "        ur_dst = np.array(ur_dst, dtype=np.int32)\n",
        "        ur_wt  = np.array(ur_wt,  dtype=np.float32)\n",
        "        self.user_recipe_edges = list(zip(ur_src, ur_dst, ur_wt))\n",
        "\n",
        "        # 2) recipe->recipe\n",
        "        r2r_data = torch.load(\n",
        "            os.path.join(self.data_dir, \"edge_r2r_src_and_dst_and_weight.pt\")\n",
        "        )\n",
        "        print(\"Type of r2r_data:\", type(r2r_data))\n",
        "        print(\"Length of r2r_data:\", len(r2r_data))\n",
        "        print(\"Type of r2r_data[0]:\", type(r2r_data[0]))\n",
        "\n",
        "        if isinstance(r2r_data[0], (list, tuple)):\n",
        "            print(\"Length of r2r_data[0]:\", len(r2r_data[0]))\n",
        "            print(\"First few entries:\", r2r_data[0][:10])  # Print first 10 elements\n",
        "\n",
        "        print(\"Type of r2r_data:\", type(r2r_data))\n",
        "        print(\"Length of r2r_data:\", len(r2r_data))\n",
        "\n",
        "        if isinstance(r2r_data, list) and len(r2r_data) == 3:\n",
        "            print(\"Each part:\")\n",
        "            print(\"  r2r_data[0] length:\", len(r2r_data[0]))\n",
        "            print(\"  r2r_data[1] length:\", len(r2r_data[1]))\n",
        "            print(\"  r2r_data[2] length:\", len(r2r_data[2]))\n",
        "        else:\n",
        "            print(\"Unexpected format:\", r2r_data)\n",
        "        if len(r2r_data) == 3:\n",
        "            r2r_src, r2r_dst, r2r_wt = r2r_data\n",
        "        else:\n",
        "            raise ValueError(f\"Expected r2r_data to have exactly 3 elements, but got {len(r2r_data)}\")\n",
        "        # r2r_src, r2r_dst, r2r_wt = r2r_data[0]\n",
        "        r2r_src, r2r_dst, r2r_wt = r2r_data\n",
        "        r2r_src = np.array(r2r_src, dtype=np.int32)\n",
        "        r2r_dst = np.array(r2r_dst, dtype=np.int32)\n",
        "        r2r_wt  = np.array(r2r_wt,  dtype=np.float32)\n",
        "        self.recipe_recipe_edges = list(zip(r2r_src, r2r_dst, r2r_wt))\n",
        "\n",
        "        # 3) recipe->ingredient\n",
        "        r2i_data = torch.load(\n",
        "            os.path.join(self.data_dir, \"edge_r2i_src_dst_weight.pt\")\n",
        "        )\n",
        "        # r2i_src, r2i_dst, r2i_wt = r2i_data[0]\n",
        "        r2i_src, r2i_dst, r2i_wt = r2i_data\n",
        "        r2i_src = np.array(r2i_src, dtype=np.int32)\n",
        "        r2i_dst = np.array(r2i_dst, dtype=np.int32)\n",
        "        r2i_wt  = np.array(r2i_wt,  dtype=np.float32)\n",
        "        self.recipe_ingredient_edges = list(zip(r2i_src, r2i_dst, r2i_wt))\n",
        "\n",
        "        # 4) ingredient->ingredient\n",
        "        i2i_data = torch.load(\n",
        "            os.path.join(self.data_dir, \"edge_i2i_src_and_dst_and_weight.pt\")\n",
        "        )\n",
        "        # i2i_src, i2i_dst, i2i_wt = i2i_data[0]\n",
        "        i2i_src, i2i_dst, i2i_wt = i2i_data\n",
        "        i2i_src = np.array(i2i_src, dtype=np.int32)\n",
        "        i2i_dst = np.array(i2i_dst, dtype=np.int32)\n",
        "        i2i_wt  = np.array(i2i_wt,  dtype=np.float32)\n",
        "        self.ingredient_ingredient_edges = list(zip(i2i_src, i2i_dst, i2i_wt))\n",
        "\n",
        "        # Suppose the dataset is known:\n",
        "        self.num_users      = 7959\n",
        "        self.num_recipes    = 68794\n",
        "        self.num_ingredients= 8847\n",
        "\n",
        "    def _split_user_recipe_edges(self, train_ratio=0.7, val_ratio=0.1):\n",
        "        \"\"\"\n",
        "        Splits the user->recipe edges into train, val, test sets (pos only).\n",
        "        Then negative sampling for each set.\n",
        "        \"\"\"\n",
        "        edges = self.user_recipe_edges\n",
        "        random.shuffle(edges)\n",
        "        n_total = len(edges)\n",
        "        n_train = int(n_total * train_ratio)\n",
        "        n_val   = int(n_total * val_ratio)\n",
        "        train_edges = edges[:n_train]\n",
        "        val_edges   = edges[n_train : n_train + n_val]\n",
        "        test_edges  = edges[n_train + n_val : ]\n",
        "\n",
        "        # Build set for membership\n",
        "        all_ur_set = set()\n",
        "        for (u,r,w) in edges:\n",
        "            all_ur_set.add((u,r))\n",
        "\n",
        "        self.train_pos = train_edges\n",
        "        self.val_pos   = val_edges\n",
        "        self.test_pos  = test_edges\n",
        "\n",
        "        self.train_neg = self._build_neg_pairs(train_edges, len(train_edges), all_ur_set, neg_ratio=self.neg_ratio)  # Modified line\n",
        "        self.val_neg   = self._build_neg_pairs(val_edges,   len(val_edges),   all_ur_set, neg_ratio=self.neg_ratio)  # Modified line\n",
        "        self.test_neg  = self._build_neg_pairs(test_edges,  len(test_edges),  all_ur_set, neg_ratio=self.neg_ratio)  # Modified line\n",
        "\n",
        "    def _build_neg_pairs(self, pos_edges, n_samples, ur_set, neg_ratio=1):\n",
        "        \"\"\"\n",
        "        For each pos edge, sample 1 negative. Or any ratio you want.\n",
        "        \"\"\"\n",
        "        # Simpler negative sampling approach (like random pairs that definitely do not overlap).\n",
        "        neg_edges = []\n",
        "        for (u, r, w) in pos_edges[:n_samples]:\n",
        "            # sample exactly 1 negative\n",
        "            count = 0\n",
        "            while count < neg_ratio:\n",
        "                rand_r = random.randint(0, self.num_recipes - 1)\n",
        "                if (u, rand_r) not in ur_set:\n",
        "                    neg_edges.append((u, rand_r, 0.0))\n",
        "                    count += 1\n",
        "        return neg_edges\n",
        "\n",
        "    def refresh_negatives(self):  # New method to rebuild negative samples\n",
        "        self._split_user_recipe_edges()\n",
        "\n",
        "    def _build_train_adjacency(self):\n",
        "        \"\"\"\n",
        "        Build adjacency dictionaries for user->recipes (train only),\n",
        "        recipe->ingredients, ingredient->recipes, etc.\n",
        "        We'll need these for real metapath expansions in aggregator calls.\n",
        "        \"\"\"\n",
        "        # user->recipes from train\n",
        "        for (u, r, w) in self.train_pos:\n",
        "            self.user2recipes[u].append(r)\n",
        "            self.recipe2users[r].append(u)\n",
        "\n",
        "        # recipe->ingredient\n",
        "        for (r, i, w) in self.recipe_ingredient_edges:\n",
        "            self.recipe2ingredients[r].append(i)\n",
        "            self.ingredient2recipes[i].append(r)\n",
        "\n",
        "    def get_train_data(self):\n",
        "        return self.train_pos, self.train_neg\n",
        "\n",
        "    def get_val_data(self):\n",
        "        return self.val_pos, self.val_neg\n",
        "\n",
        "    def get_test_data(self):\n",
        "        return self.test_pos, self.test_neg\n",
        "\n",
        "    def to(self, device):\n",
        "        self.device = device\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "# 2. MODEL COMPONENTS (REAL NEIGHBOR SAMPLING)\n",
        "###############################################################################\n",
        "class MetapathInstanceEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Encodes a single metapath instance by aggregating the node features along that path.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, method=\"mean\"):\n",
        "        super().__init__()\n",
        "        self.method = method\n",
        "        self.input_dim = input_dim\n",
        "        if method == \"linear\":\n",
        "            self.fc = nn.Linear(input_dim, input_dim)\n",
        "            nn.init.xavier_normal_(self.fc.weight, gain=1.414)\n",
        "        elif method.startswith(\"rotate\"):\n",
        "            pass  # omitted for brevity\n",
        "\n",
        "    def forward(self, node_feats):\n",
        "        \"\"\"\n",
        "        node_feats: (L x d) or a list of node feature vectors\n",
        "        \"\"\"\n",
        "        if isinstance(node_feats, list):\n",
        "            node_feats = torch.stack(node_feats, dim=0)  # (L, d)\n",
        "        if self.method == \"mean\":\n",
        "            return node_feats.mean(dim=0)\n",
        "        elif self.method == \"linear\":\n",
        "            return self.fc(node_feats.mean(dim=0))\n",
        "        elif self.method.startswith(\"rotate\"):\n",
        "            return node_feats.mean(dim=0)  # placeholder\n",
        "        else:\n",
        "            return node_feats.mean(dim=0)\n",
        "\n",
        "\n",
        "class MAGNNIntraMetapathAggregator(nn.Module):\n",
        "    \"\"\"\n",
        "    Given a target node's embedding + the expansions along a certain metapath,\n",
        "    encodes each \"instance\" (a path from target->...->neighbor) with a MetapathInstanceEncoder,\n",
        "    then aggregates them by multi-head attention.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, num_heads=4, encoder_method=\"mean\"):\n",
        "        super().__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        self.instance_encoder = MetapathInstanceEncoder(input_dim, method=encoder_method)\n",
        "\n",
        "        # For multi-head attention\n",
        "        self.attn_fc = nn.Parameter(torch.Tensor(num_heads, 2 * input_dim))\n",
        "        nn.init.xavier_uniform_(self.attn_fc, gain=1.414)\n",
        "        self.leaky_relu = nn.LeakyReLU(0.2)\n",
        "\n",
        "    def forward(self, target_feat, list_of_paths):\n",
        "        \"\"\"\n",
        "        list_of_paths: each path is a list/tensor of node features (including target?)\n",
        "        \"\"\"\n",
        "        # encode each path\n",
        "        instance_reps = []\n",
        "        for path_feats in list_of_paths:\n",
        "            rep = self.instance_encoder(path_feats)\n",
        "            instance_reps.append(rep)\n",
        "\n",
        "        if len(instance_reps) == 0:\n",
        "            # no neighbors\n",
        "            return torch.zeros(self.num_heads*self.input_dim, device=target_feat.device)\n",
        "\n",
        "        instance_reps = torch.stack(instance_reps, dim=0)  # (N, d)\n",
        "        N = instance_reps.size(0)\n",
        "\n",
        "        # multi-head attention\n",
        "        # e = a_k^T [ target || instance_rep ]\n",
        "        target_expand = target_feat.unsqueeze(0).expand(N, -1)      # (N, d)\n",
        "        cat = torch.cat([target_expand, instance_reps], dim=1)      # (N, 2d)\n",
        "        # replicate for heads\n",
        "        cat_expand = cat.unsqueeze(0).expand(self.num_heads, N, 2*self.input_dim)  # (num_heads, N, 2d)\n",
        "        # attn_fc: (num_heads, 2d)\n",
        "        e = torch.bmm(cat_expand, self.attn_fc.unsqueeze(2)).squeeze(-1)  # (num_heads, N)\n",
        "        e = self.leaky_relu(e)\n",
        "        alpha = F.softmax(e, dim=1)  # (num_heads, N)\n",
        "\n",
        "        # Weighted sum\n",
        "        instance_reps_expand = instance_reps.unsqueeze(0).expand(self.num_heads, N, self.input_dim)\n",
        "        out_per_head = torch.bmm(alpha.unsqueeze(1), instance_reps_expand).squeeze(1)  # (num_heads, d)\n",
        "\n",
        "        out = out_per_head.view(self.num_heads * self.input_dim)\n",
        "        return out\n",
        "\n",
        "\n",
        "class MAGNNInterMetapathAggregator(nn.Module):\n",
        "    \"\"\"\n",
        "    Combines multiple metapath embeddings with an attention mechanism.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, num_heads, attn_vec_dim=128):\n",
        "        super().__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attn_vec_dim = attn_vec_dim\n",
        "\n",
        "        # after intra-agg we have (num_heads*input_dim)\n",
        "        self.fc = nn.Linear(num_heads*input_dim, attn_vec_dim)\n",
        "        nn.init.xavier_normal_(self.fc.weight, gain=1.414)\n",
        "\n",
        "        self.attn_vec = nn.Linear(attn_vec_dim, 1, bias=False)\n",
        "        nn.init.xavier_normal_(self.attn_vec.weight, gain=1.414)\n",
        "\n",
        "    def forward(self, list_of_metapath_embs):\n",
        "        \"\"\"\n",
        "        Each element: (num_heads*input_dim)\n",
        "        We'll do alpha_i = softmax( q^T tanh( W * h_i ) )\n",
        "        \"\"\"\n",
        "        if len(list_of_metapath_embs) == 0:\n",
        "            return torch.zeros(self.num_heads*self.input_dim, device=self.fc.weight.device)\n",
        "\n",
        "        H = torch.stack(list_of_metapath_embs, dim=0)  # (M, num_heads*input_dim)\n",
        "        # shape (M, attn_vec_dim)\n",
        "        wh = torch.tanh(self.fc(H))\n",
        "        # shape (M, 1)\n",
        "        alphas = self.attn_vec(wh)\n",
        "        alphas = F.softmax(alphas, dim=0)\n",
        "        out = (H * alphas).sum(dim=0)  # (num_heads*input_dim)\n",
        "        return out\n",
        "\n",
        "\n",
        "class MAGNNModel(nn.Module):\n",
        "    \"\"\"\n",
        "    A more realistic MAGNN-based link-prediction model, with:\n",
        "      - Real expansions for U->R and U->R->I->R on user side,\n",
        "        and R->U / R->I->R on recipe side, etc.\n",
        "      - We store an embedding matrix for users/recipes (and optionally ingredients).\n",
        "      - We do aggregator calls with actual neighbors from the adjacency dictionary.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_user, num_recipe,\n",
        "                 user2recipes, recipe2ingredients, recipe2users,\n",
        "                 input_dim=64, num_heads=4, encoder_method=\"mean\",\n",
        "                 attn_vec_dim=128, neighbor_recipe_limit=20, neighbor_ingredient_limit=5,\n",
        "                 device=\"cpu\"):\n",
        "        super().__init__()\n",
        "        self.num_user = num_user\n",
        "        self.num_recipe = num_recipe\n",
        "        self.user2recipes = user2recipes\n",
        "        self.recipe2ingredients = recipe2ingredients\n",
        "        self.recipe2users = recipe2users  # Newly added mapping\n",
        "        self.device = device\n",
        "        self.input_dim = input_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attn_vec_dim = attn_vec_dim                         # New parameter\n",
        "        self.neighbor_recipe_limit = neighbor_recipe_limit       # New parameter\n",
        "        self.neighbor_ingredient_limit = neighbor_ingredient_limit   # New parameter\n",
        "\n",
        "        # Define the \"intra\" aggregators for user side:\n",
        "        self.agg_UR   = MAGNNIntraMetapathAggregator(input_dim, num_heads, encoder_method)  # user->recipe\n",
        "        self.agg_URIR = MAGNNIntraMetapathAggregator(input_dim, num_heads, encoder_method)  # user->recipe->ingredient->recipe\n",
        "\n",
        "        # \"inter\" aggregator\n",
        "        self.inter_agg_user = MAGNNInterMetapathAggregator(input_dim, num_heads, attn_vec_dim)\n",
        "        self.agg_RU   = MAGNNIntraMetapathAggregator(input_dim, num_heads, encoder_method)\n",
        "        self.agg_RIR  = MAGNNIntraMetapathAggregator(input_dim, num_heads, encoder_method)\n",
        "        self.inter_agg_recipe = MAGNNInterMetapathAggregator(input_dim, num_heads, attn_vec_dim)\n",
        "\n",
        "        # Embeddings\n",
        "        self.user_emb = nn.Embedding(num_user, input_dim)\n",
        "        self.recipe_emb = nn.Embedding(num_recipe, input_dim)\n",
        "        nn.init.xavier_normal_(self.user_emb.weight, gain=1.414)\n",
        "        nn.init.xavier_normal_(self.recipe_emb.weight, gain=1.414)\n",
        "\n",
        "        self.num_ingredient = 8847  # (or pass it in from data_loader)\n",
        "        self.ingredient_emb = nn.Embedding(self.num_ingredient, input_dim)\n",
        "        nn.init.xavier_normal_(self.ingredient_emb.weight, gain=1.414)\n",
        "\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "        self.to(device)\n",
        "\n",
        "    def forward_user(self, u_id):\n",
        "        user_feat = self.user_emb.weight[u_id]\n",
        "        ur_paths = []\n",
        "        recipes = self.user2recipes[u_id] if u_id in self.user2recipes else []\n",
        "        if recipes:\n",
        "            recipe_tensor = torch.tensor(recipes, device=self.device)\n",
        "            recipe_feats = self.recipe_emb(recipe_tensor)\n",
        "            for r_feat in recipe_feats:\n",
        "                ur_paths.append(torch.stack([user_feat, r_feat], dim=0))\n",
        "        UR_out = self.agg_UR(user_feat, ur_paths)\n",
        "        urir_paths = []\n",
        "        for r in recipes[:self.neighbor_recipe_limit]:  # Changed limit\n",
        "            r_feat = self.recipe_emb.weight[r]\n",
        "            ingredients = self.recipe2ingredients[r] if r in self.recipe2ingredients else []\n",
        "            for i_id in ingredients[:self.neighbor_ingredient_limit]:  # Changed limit\n",
        "                i_feat = self.ingredient_emb.weight[i_id]\n",
        "                r2_feat = self.recipe_emb.weight[r]\n",
        "                path_feats = [user_feat, r_feat, i_feat, r2_feat]\n",
        "                urir_paths.append(torch.stack(path_feats, dim=0))\n",
        "        URIR_out = self.agg_URIR(user_feat, urir_paths)\n",
        "        user_final = self.inter_agg_user([UR_out, URIR_out])\n",
        "        user_final = self.dropout(user_final)\n",
        "        return user_final\n",
        "\n",
        "    def forward_recipe(self, r_id):\n",
        "        recipe_feat = self.recipe_emb.weight[r_id]\n",
        "        RU_paths = []\n",
        "        users = self.recipe2users.get(r_id, [])\n",
        "        if users:\n",
        "            user_tensor = torch.tensor(users, device=self.device)\n",
        "            user_feats = self.user_emb(user_tensor)\n",
        "            for u_feat in user_feats:\n",
        "                RU_paths.append(torch.stack([recipe_feat, u_feat], dim=0))\n",
        "        RU_out = self.agg_RU(recipe_feat, RU_paths)\n",
        "        RIR_paths = []\n",
        "        ings = self.recipe2ingredients.get(r_id, [])\n",
        "        for i_id in ings[:self.neighbor_ingredient_limit]:\n",
        "            i_feat = self.ingredient_emb.weight[i_id]\n",
        "            r2_feat = self.recipe_emb.weight[r_id]\n",
        "            RIR_paths.append(torch.stack([recipe_feat, i_feat, r2_feat], dim=0))\n",
        "        RIR_out = self.agg_RIR(recipe_feat, RIR_paths)\n",
        "        recipe_final = self.inter_agg_recipe([RU_out, RIR_out])\n",
        "        recipe_final = self.dropout(recipe_final)\n",
        "        return recipe_final\n",
        "\n",
        "    def predict_score(self, u_id, r_id):\n",
        "        hu = self.forward_user(u_id)\n",
        "        hr = self.forward_recipe(r_id)\n",
        "        return torch.dot(hu, hr)\n",
        "\n",
        "    def forward(self, user_ids, recipe_ids):\n",
        "        out = []\n",
        "        for u, r in zip(user_ids, recipe_ids):\n",
        "            s = self.predict_score(int(u), int(r))\n",
        "            out.append(s)\n",
        "        return torch.stack(out, dim=0)\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "# 3. TRAINING/INFERENCE WITH RANK-BASED METRICS\n",
        "###############################################################################\n",
        "def precision_at_k(recommended, ground_truth, k=10):\n",
        "    if not recommended:\n",
        "        return 0.0\n",
        "    rec_k = recommended[:k]\n",
        "    hits = len(set(rec_k).intersection(set(ground_truth)))\n",
        "    return hits / float(k)\n",
        "\n",
        "def hit_rate_at_k(recommended, ground_truth, k=10):\n",
        "    rec_k = recommended[:k]\n",
        "    hits = set(rec_k).intersection(set(ground_truth))\n",
        "    return 1.0 if len(hits) > 0 else 0.0\n",
        "\n",
        "def _dcg_at_k(recommended, ground_truth, k=10):\n",
        "    dcg = 0.0\n",
        "    for i, item in enumerate(recommended[:k]):\n",
        "        if item in ground_truth:\n",
        "            dcg += 1.0 / math.log2(i+2)\n",
        "    return dcg\n",
        "\n",
        "def ndcg_at_k(recommended, ground_truth, k=10):\n",
        "    dcg = _dcg_at_k(recommended, ground_truth, k)\n",
        "    # best DCG is if the ground_truth is ranked at top\n",
        "    ideal_list = list(ground_truth)\n",
        "    idcg = _dcg_at_k(ideal_list, ground_truth, min(k, len(ground_truth)))\n",
        "    if idcg == 0.0:\n",
        "        return 0.0\n",
        "    return dcg / idcg\n",
        "\n",
        "def map_at_k(recommended, ground_truth, k=10):\n",
        "    # average precision\n",
        "    # if multiple ground-truth items, we sum precision at each relevant item rank, then average\n",
        "    hits = 0\n",
        "    sum_precisions = 0.0\n",
        "    for i, item in enumerate(recommended[:k]):\n",
        "        if item in ground_truth:\n",
        "            hits += 1\n",
        "            sum_precisions += hits / (i+1.0)\n",
        "    if hits == 0:\n",
        "        return 0.0\n",
        "    return sum_precisions / hits\n",
        "\n",
        "\n",
        "class Trainer(object):\n",
        "    \"\"\"\n",
        "    Trainer with negative sampling (binary cross-entropy) and also\n",
        "    advanced ranking evaluation with leave-one-out if desired.\n",
        "    \"\"\"\n",
        "    def __init__(self, model, data_loader, device=\"cpu\", lr=0.001, weight_decay=1e-4):\n",
        "        self.model = model\n",
        "        self.loader = data_loader\n",
        "        self.device = device\n",
        "        self.optimizer = Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max=100)\n",
        "\n",
        "    @staticmethod\n",
        "    def stratified_sample_edges(edges, target_count=10000):\n",
        "        \"\"\"\n",
        "        Samples a representative subset of edges based on user groups.\n",
        "        Each edge is assumed to be a tuple (user, recipe, weight).\n",
        "        \"\"\"\n",
        "        from collections import defaultdict\n",
        "        grouped = defaultdict(list)\n",
        "        for edge in edges:\n",
        "            grouped[edge[0]].append(edge)\n",
        "        sampled = []\n",
        "        total_edges = len(edges)\n",
        "        if total_edges == 0:\n",
        "            return sampled\n",
        "        # Determine the fraction of edges to sample\n",
        "        fraction = target_count / total_edges\n",
        "        for user, user_edges in grouped.items():\n",
        "            k = max(1, int(len(user_edges) * fraction))\n",
        "            # Ensure we don't request more than available\n",
        "            k = min(k, len(user_edges))\n",
        "            sampled.extend(random.sample(user_edges, k))\n",
        "        # If we have overshot the target count, randomly pick target_count edges\n",
        "        if len(sampled) > target_count:\n",
        "            sampled = random.sample(sampled, target_count)\n",
        "        return sampled\n",
        "\n",
        "    def train_epoch(self, train_pos, train_neg, batch_size=512):\n",
        "        self.model.train()\n",
        "        random.shuffle(train_pos)\n",
        "        random.shuffle(train_neg)\n",
        "        # Use stratified sampling to ensure representativeness\n",
        "        pairs_pos = Trainer.stratified_sample_edges(train_pos, target_count=10000)\n",
        "        pairs_neg = Trainer.stratified_sample_edges(train_neg, target_count=10000)\n",
        "        # pairs_pos = train_pos[:10000]  # limit to 10k for speed\n",
        "        # pairs_neg = train_neg[:10000]  # limit to 10k for speed\n",
        "        # Use the full training set without sampling\n",
        "        # pairs_pos = train_pos\n",
        "        # pairs_neg = train_neg\n",
        "        random.shuffle(pairs_pos)\n",
        "        random.shuffle(pairs_neg)\n",
        "\n",
        "        def bce_loss_fn(logits, labels):\n",
        "            offset = 0.01\n",
        "            return F.binary_cross_entropy_with_logits(logits + offset, labels)\n",
        "\n",
        "\n",
        "        n = len(pairs_pos)\n",
        "        n_batches = math.ceil(n / batch_size)\n",
        "        total_loss = 0.0\n",
        "\n",
        "        for i in range(n_batches):\n",
        "            start = i*batch_size\n",
        "            end = min(start+batch_size, n)\n",
        "            pos_batch = pairs_pos[start:end]\n",
        "            neg_batch = pairs_neg[start:end]\n",
        "\n",
        "            user_ids, recipe_ids, labels = [], [], []\n",
        "            for (u, r, w) in pos_batch:\n",
        "                user_ids.append(u)\n",
        "                recipe_ids.append(r)\n",
        "                labels.append(1.0)\n",
        "            for (u, r, w) in neg_batch:\n",
        "                user_ids.append(u)\n",
        "                recipe_ids.append(r)\n",
        "                labels.append(0.0)\n",
        "\n",
        "            user_ids = torch.tensor(user_ids, device=self.device)\n",
        "            recipe_ids = torch.tensor(recipe_ids, device=self.device)\n",
        "            labels = torch.tensor(labels, dtype=torch.float32, device=self.device)\n",
        "\n",
        "            # DEBUG PRINT 1: Inspect data for each batch\n",
        "            # print(f\"\\n[DEBUG] Batch {i+1}/{n_batches}\")\n",
        "            # print(f\"  user_ids[:5]: {user_ids[:5].cpu().numpy()}\")\n",
        "            # print(f\"  recipe_ids[:5]: {recipe_ids[:5].cpu().numpy()}\")\n",
        "            # print(f\"  labels[:10]: {labels[:10].cpu().numpy()}\")  # first 10 labels\n",
        "            # print(f\"  labels[-10:]: {labels[-10:].cpu().numpy()}\")  # last 10 labels\n",
        "\n",
        "            # DEBUG PRINT 2: Print model parameter snippet before update\n",
        "            # Example: user_emb (first row, first 5 dims)\n",
        "            # print(f\"  model.user_emb.weight[0, :5] BEFORE step: {self.model.user_emb.weight[0, :5].detach().cpu().numpy()}\")\n",
        "\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            logits = self.model.forward(user_ids, recipe_ids)\n",
        "\n",
        "            # DEBUG PRINT 3: Check logits for the first few samples\n",
        "            # print(f\"  logits[:5]: {logits[:5].detach().cpu().numpy()}\")\n",
        "\n",
        "            loss = bce_loss_fn(logits, labels)\n",
        "            loss.backward()\n",
        "            # grad_norm = self.model.user_emb.weight.grad.norm().item()\n",
        "            # print(f\"  Gradient norm for user_emb.weight: {grad_norm:.6f}\")\n",
        "            self.optimizer.step()\n",
        "\n",
        "            # DEBUG PRINT 4: Print same model parameter snippet after update\n",
        "            # print(f\"  model.user_emb.weight[0, :5] AFTER step: {self.model.user_emb.weight[0, :5].detach().cpu().numpy()}\")\n",
        "\n",
        "            # DEBUG PRINT 5: Print batch loss with more decimals\n",
        "            print(f\"  Batch Loss: {loss.item():.6f}\")\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Print batch progress (flush immediately)\n",
        "            print(f\"Batch {i+1}/{n_batches} - Loss: {loss.item():.4f}  Running Avg: {total_loss/(i+1):.4f}\")\n",
        "\n",
        "        avg_loss = total_loss / n_batches\n",
        "        print(f\"\\n[DEBUG] End of Epoch - Average Loss: {avg_loss:.6f}\")\n",
        "        return avg_loss\n",
        "\n",
        "    def evaluate_auc_ap(self, pos_edges, neg_edges, batch_size=512):\n",
        "        \"\"\"\n",
        "        Returns (auc, ap). Pairwise classification approach.\n",
        "        \"\"\"\n",
        "        self.model.eval()\n",
        "        user_pos = [p[0] for p in pos_edges]\n",
        "        item_pos = [p[1] for p in pos_edges]\n",
        "        user_neg = [p[0] for p in neg_edges]\n",
        "        item_neg = [p[1] for p in neg_edges]\n",
        "\n",
        "        labels = np.array([1]*len(pos_edges) + [0]*len(neg_edges), dtype=np.float32)\n",
        "\n",
        "        # compute scores\n",
        "        scores = []\n",
        "        def batched_eval(u_list, r_list):\n",
        "            local_scores = []\n",
        "            ndata = len(u_list)\n",
        "            nb = math.ceil(ndata / batch_size)\n",
        "            with torch.no_grad():\n",
        "                for bi in range(nb):\n",
        "                    st = bi*batch_size\n",
        "                    en = min(st+batch_size, ndata)\n",
        "                    uu = torch.tensor(u_list[st:en], device=self.device)\n",
        "                    rr = torch.tensor(r_list[st:en], device=self.device)\n",
        "                    s = self.model.forward(uu, rr).cpu().numpy()\n",
        "                    local_scores.append(s)\n",
        "            return np.concatenate(local_scores, axis=0)\n",
        "\n",
        "        scores_pos = batched_eval(user_pos, item_pos)\n",
        "        scores_neg = batched_eval(user_neg, item_neg)\n",
        "        all_scores = np.concatenate([scores_pos, scores_neg], axis=0)\n",
        "        auc = roc_auc_score(labels, all_scores)\n",
        "        ap  = average_precision_score(labels, all_scores)\n",
        "        return auc, ap\n",
        "\n",
        "    def evaluate_ranking_loo(self, test_pos, K=10, num_neg=100):\n",
        "        \"\"\"\n",
        "        Leave-One-Out approach: For each user in test_pos, pick exactly one test edge\n",
        "        (user->item) as the ground-truth. Then sample 'num_neg' negative items for that user,\n",
        "        rank them all, and compute Hit@K, NDCG@K, etc. A standard approach in recommender eval.\n",
        "\n",
        "        test_pos: list of (u, r, w). If user has multiple test items, we handle each item individually.\n",
        "        \"\"\"\n",
        "        self.model.eval()\n",
        "        # Group test items by user\n",
        "        user2items = defaultdict(list)\n",
        "        for (u, r, w) in test_pos:\n",
        "            user2items[u].append(r)\n",
        "\n",
        "        hr_list, ndcg_list, prec_list, map_list = [], [], [], []\n",
        "\n",
        "        all_users = sorted(user2items.keys())\n",
        "        # We'll do a single LOO per test item if the user has multiple test edges\n",
        "        # (some do a single item, others do multiple). We'll handle them in a small loop.\n",
        "\n",
        "        # We'll need a global set of train edges so we don't sample positives as negatives\n",
        "        train_UR = set()\n",
        "        for (u,r,_) in self.loader.train_pos:\n",
        "            train_UR.add((u,r))\n",
        "\n",
        "        for user in all_users:\n",
        "            test_recipes = user2items[user]  # may be multiple\n",
        "            for gt_item in test_recipes:\n",
        "                # Sample num_neg negative\n",
        "                neg_items = []\n",
        "                tried = 0\n",
        "                while len(neg_items) < num_neg and tried < num_neg*100:\n",
        "                    candidate = random.randint(0, self.loader.num_recipes-1)\n",
        "                    if (user, candidate) not in train_UR and candidate not in test_recipes:\n",
        "                        neg_items.append(candidate)\n",
        "                    tried += 1\n",
        "\n",
        "                # now we have 1 positive (gt_item) + num_neg negatives\n",
        "                # let's compute scores\n",
        "                candidates = [gt_item] + neg_items\n",
        "                user_ids = torch.tensor([user]*len(candidates), device=self.device)\n",
        "                item_ids = torch.tensor(candidates, device=self.device)\n",
        "                with torch.no_grad():\n",
        "                    scores = self.model.forward(user_ids, item_ids).cpu().numpy()\n",
        "\n",
        "                # sort candidates by score descending\n",
        "                sorted_idx = np.argsort(scores)[::-1]\n",
        "                ranked_items = [candidates[i] for i in sorted_idx]\n",
        "\n",
        "                # compute metrics\n",
        "                ground_truth = [gt_item]\n",
        "                hr = hit_rate_at_k(ranked_items, ground_truth, K)\n",
        "                nd = ndcg_at_k(ranked_items, ground_truth, K)\n",
        "                pc = precision_at_k(ranked_items, ground_truth, K)\n",
        "                mp = map_at_k(ranked_items, ground_truth, K)\n",
        "\n",
        "                hr_list.append(hr)\n",
        "                ndcg_list.append(nd)\n",
        "                prec_list.append(pc)\n",
        "                map_list.append(mp)\n",
        "\n",
        "        # average\n",
        "        hr_ = np.mean(hr_list)\n",
        "        nd_ = np.mean(ndcg_list)\n",
        "        pr_ = np.mean(prec_list)\n",
        "        mp_ = np.mean(map_list)\n",
        "        return hr_, nd_, pr_, mp_\n",
        "\n",
        "    def train_model(self, train_pos, train_neg, val_pos, val_neg, batch_size=256, max_epochs=100, patience=30):\n",
        "        import copy\n",
        "        best_val_auc = 0.0\n",
        "        cur_patience = 0\n",
        "        best_state = None\n",
        "        for ep in range(max_epochs):\n",
        "            epoch_start = time.time()\n",
        "            print(f\"\\nEpoch {ep} start:\")\n",
        "            trn_loss = self.train_epoch(train_pos, train_neg, batch_size=batch_size)\n",
        "            self.scheduler.step()  # same scheduler step as in gridsearch\n",
        "            val_auc, _ = self.evaluate_auc_ap(val_pos, val_neg)\n",
        "            epoch_time = time.time() - epoch_start\n",
        "            print(f\"Epoch {ep} finished: Loss={trn_loss:.4f}, Val AUC={val_auc:.4f}, Time={epoch_time:.2f}s\")\n",
        "            if val_auc > best_val_auc:\n",
        "                best_val_auc = val_auc\n",
        "                cur_patience = 0\n",
        "                best_state = copy.deepcopy(self.model.state_dict())\n",
        "                print(f\"New best model found at epoch {ep} with Val AUC: {val_auc:.4f}\")\n",
        "            else:\n",
        "                cur_patience += 1\n",
        "                if cur_patience >= patience:\n",
        "                    print(\"Early stopping triggered.\")\n",
        "                    break\n",
        "        if best_state is not None:\n",
        "            self.model.load_state_dict(best_state)\n",
        "        return best_state, best_val_auc\n",
        "\n",
        "###############################################################################\n",
        "# 4.5 GRID SEARCH\n",
        "###############################################################################\n",
        "class GridSearch:\n",
        "    def __init__(self, data_loader, device=\"cuda\"):\n",
        "        self.data_loader = data_loader\n",
        "        self.device = device\n",
        "        self.param_grid = {\n",
        "            \"input_dim\": [64], # 128\n",
        "            \"num_heads\": [4], # 8\n",
        "            \"dropout\": [0.3], # 0.5\n",
        "            \"lr\": [0.005], # 0.01\n",
        "            \"weight_decay\": [1e-6], # 1e-4\n",
        "            \"encoder_method\": [\"mean\"], # linear\n",
        "            \"neg_ratio\": [3],      # 1, 5         # New parameter\n",
        "            \"attn_vec_dim\": [128], # 64 not 256?               # New parameter\n",
        "            \"neighbor_recipe_limit\": [30],            # New parameter\n",
        "            \"neighbor_ingredient_limit\": [5],     # 10    # New parameter\n",
        "            \"batch_size\": [256]    # 512                  # New parameter\n",
        "        }\n",
        "        self.results = []\n",
        "\n",
        "    def run(self, epochs=3, batch_size=512):\n",
        "        import copy\n",
        "        from itertools import product\n",
        "\n",
        "        overall_best_auc = 0.0\n",
        "        overall_best_config = None\n",
        "        overall_best_model_state = None\n",
        "\n",
        "        keys = list(self.param_grid.keys())\n",
        "        for values in product(*[self.param_grid[k] for k in keys]):\n",
        "            config = dict(zip(keys, values))\n",
        "            print(\"Testing configuration:\", config)\n",
        "            # Update the data loader with new negative sampling ratio and refresh negatives\n",
        "            self.data_loader.neg_ratio = config[\"neg_ratio\"]\n",
        "            self.data_loader.refresh_negatives()\n",
        "            # Build model with current configuration\n",
        "            model = MAGNNModel(\n",
        "                num_user=self.data_loader.num_users,\n",
        "                num_recipe=self.data_loader.num_recipes,\n",
        "                user2recipes=self.data_loader.user2recipes,\n",
        "                recipe2ingredients=self.data_loader.recipe2ingredients,\n",
        "                recipe2users=self.data_loader.recipe2users,\n",
        "                input_dim=config[\"input_dim\"],\n",
        "                num_heads=config[\"num_heads\"],\n",
        "                encoder_method=config[\"encoder_method\"],\n",
        "                attn_vec_dim=config[\"attn_vec_dim\"],\n",
        "                neighbor_recipe_limit=config[\"neighbor_recipe_limit\"],\n",
        "                neighbor_ingredient_limit=config[\"neighbor_ingredient_limit\"],\n",
        "                device=self.device\n",
        "            )\n",
        "            model.dropout = nn.Dropout(config[\"dropout\"])\n",
        "            trainer = Trainer(model, self.data_loader, device=self.device, lr=config[\"lr\"], weight_decay=config[\"weight_decay\"])\n",
        "            best_val_auc = 0.0\n",
        "            config_best_model_state = None  # to store the best weights for this configuration\n",
        "            for ep in range(epochs):\n",
        "                trainer.train_epoch(*self.data_loader.get_train_data(), batch_size=config[\"batch_size\"])\n",
        "                val_auc, _ = trainer.evaluate_auc_ap(*self.data_loader.get_val_data())\n",
        "                print(f\"Epoch {ep} - Val AUC: {val_auc:.4f}\")\n",
        "                # Save model state if current epoch improved for this config\n",
        "                if val_auc > best_val_auc:\n",
        "                    best_val_auc = val_auc\n",
        "                    config_best_model_state = copy.deepcopy(model.state_dict())\n",
        "\n",
        "\n",
        "                trainer.scheduler.step()\n",
        "            self.results.append((config, best_val_auc))\n",
        "            # Update overall best model if this configuration is better\n",
        "            if best_val_auc > overall_best_auc:\n",
        "                overall_best_auc = best_val_auc\n",
        "                overall_best_config = config\n",
        "                overall_best_model_state = config_best_model_state\n",
        "\n",
        "            print(f\"Finished config {config} with best Val AUC: {best_val_auc:.4f}\\n\")\n",
        "\n",
        "        # Save overall best model weights to disk\n",
        "        if overall_best_model_state is not None:\n",
        "            torch.save(overall_best_model_state, \"best_model_weights.pth\")\n",
        "            print(\"Saved best model weights to 'best_model_weights.pth'\")\n",
        "\n",
        "        print(\"Best configuration:\", overall_best_config, \"with Val AUC:\", overall_best_auc)\n",
        "        return overall_best_config\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "# 4. MAIN\n",
        "###############################################################################\n",
        "def main():\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(\"Using device:\", device)\n",
        "    sys.stdout.flush()\n",
        "\n",
        "    # Load data\n",
        "    data_loader = URIDataLoader(data_dir=\"/content/drive/MyDrive/RecipeMAG/data/\", device=device, seed=42)\n",
        "    print(\"Users:\", data_loader.num_users, \"Recipes:\", data_loader.num_recipes)\n",
        "    print(\"Train data size:\", len(data_loader.train_pos))\n",
        "    sys.stdout.flush()\n",
        "\n",
        "    train_pos, train_neg = data_loader.get_train_data()\n",
        "    val_pos, val_neg = data_loader.get_val_data()\n",
        "    test_pos, test_neg = data_loader.get_test_data()\n",
        "\n",
        "    # ***** USE FIXED CONFIGURATION (same as gridsearch best config) *****\n",
        "    data_loader.neg_ratio = 3\n",
        "    data_loader.refresh_negatives()\n",
        "    best_config = {\n",
        "        \"input_dim\": 64,\n",
        "        \"num_heads\": 4,\n",
        "        \"dropout\": 0.3,\n",
        "        \"lr\": 0.005,\n",
        "        \"weight_decay\": 1e-6,\n",
        "        \"encoder_method\": \"mean\",\n",
        "        \"attn_vec_dim\": 128,\n",
        "        \"neighbor_recipe_limit\": 30,\n",
        "        \"neighbor_ingredient_limit\": 5,\n",
        "        \"batch_size\": 256\n",
        "    }\n",
        "\n",
        "    # Build the model and trainer\n",
        "    model = MAGNNModel(\n",
        "        num_user=data_loader.num_users,\n",
        "        num_recipe=data_loader.num_recipes,\n",
        "        user2recipes=data_loader.user2recipes,\n",
        "        recipe2ingredients=data_loader.recipe2ingredients,\n",
        "        recipe2users=data_loader.recipe2users,\n",
        "        input_dim=best_config[\"input_dim\"],\n",
        "        num_heads=best_config[\"num_heads\"],\n",
        "        encoder_method=best_config[\"encoder_method\"],\n",
        "        attn_vec_dim=best_config[\"attn_vec_dim\"],\n",
        "        neighbor_recipe_limit=best_config[\"neighbor_recipe_limit\"],\n",
        "        neighbor_ingredient_limit=best_config[\"neighbor_ingredient_limit\"],\n",
        "        device=device\n",
        "    )\n",
        "    model.dropout = nn.Dropout(best_config[\"dropout\"])\n",
        "    trainer = Trainer(model, data_loader, device=device, lr=best_config[\"lr\"], weight_decay=best_config[\"weight_decay\"])\n",
        "\n",
        "    # Set to True to load a pretrained model and evaluate, or False to train a new one.\n",
        "    load_pretrained = True\n",
        "\n",
        "    if load_pretrained:\n",
        "        # Load the pretrained model weights from disk\n",
        "        model.load_state_dict(torch.load(\"/content/best_model_weights_MAGNN.pth\"))\n",
        "        print(\"Loaded pretrained model weights from '_________.pth'\")\n",
        "    else:\n",
        "        # ***** TRAINING PROCEDURE (using the new train_model method) *****\n",
        "        best_state, best_val_auc = trainer.train_model(\n",
        "            train_pos, train_neg, val_pos, val_neg,\n",
        "            batch_size=best_config[\"batch_size\"],\n",
        "            max_epochs=100,    # you can adjust max epochs here\n",
        "            patience=30        # early stopping patience\n",
        "        )\n",
        "        print(f\"Training completed. Best Validation AUC: {best_val_auc:.4f}\")\n",
        "\n",
        "        # Save final best model weights for later use\n",
        "        torch.save(model.state_dict(), \"final_model_weights.pth\")\n",
        "        print(\"Final model weights saved to 'final_model_weights.pth'\")\n",
        "        print(\"To load the model later, use: model.load_state_dict(torch.load('final_model_weights.pth'))\")\n",
        "\n",
        "    # Evaluate on test data\n",
        "    test_auc, test_ap = trainer.evaluate_auc_ap(test_pos, test_neg)\n",
        "    print(f\"[Test Pairwise Classification] AUC={test_auc:.4f}, AP={test_ap:.4f}\")\n",
        "\n",
        "    hr, ndcg, prec, map_ = trainer.evaluate_ranking_loo(test_pos, K=5, num_neg=100)\n",
        "    print(f\"[Test Ranking LOO @5] HitRate={hr:.4f}, NDCG={ndcg:.4f}, Precision={prec:.4f}, MAP={map_:.4f}\")\n",
        "\n",
        "    # Example: testing the model with sample inputs\n",
        "    model.eval()\n",
        "    user_ids = torch.tensor([0, 1, 2], device=device)\n",
        "    recipe_ids = torch.tensor([10, 20, 30], device=device)\n",
        "    scores = model.forward(user_ids, recipe_ids)\n",
        "    print(\"Sample predictions:\", scores)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "# 4. MAIN (With Gridsearch)\n",
        "###############################################################################\n",
        "# def main():\n",
        "#     device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "#     print(\"Using device:\", device)\n",
        "#     sys.stdout.flush()\n",
        "\n",
        "#     # Load data\n",
        "#     data_loader = URIDataLoader(data_dir=\"/content/drive/MyDrive/RecipeMAG/data/\", device=device, seed=42)\n",
        "#     print(\"Users:\", data_loader.num_users, \"Recipes:\", data_loader.num_recipes)\n",
        "#     print(\"Train data size:\", len(data_loader.train_pos))\n",
        "#     sys.stdout.flush()\n",
        "\n",
        "#     train_pos, train_neg = data_loader.get_train_data()\n",
        "#     val_pos, val_neg = data_loader.get_val_data()\n",
        "#     test_pos, test_neg = data_loader.get_test_data()\n",
        "\n",
        "#     # ***** BEGIN GRID SEARCH HOOK *****\n",
        "#     run_grid = True  # Set to True to perform grid search\n",
        "#     if run_grid:\n",
        "#         gs = GridSearch(data_loader, device=device)\n",
        "#         best_config = gs.run(epochs=18)  # grid search now controls batch size from config\n",
        "#         print(\"Grid search completed. Best config:\", best_config)\n",
        "#         model = MAGNNModel(\n",
        "#             num_user=data_loader.num_users,\n",
        "#             num_recipe=data_loader.num_recipes,\n",
        "#             user2recipes=data_loader.user2recipes,\n",
        "#             recipe2ingredients=data_loader.recipe2ingredients,\n",
        "#             recipe2users=data_loader.recipe2users,\n",
        "#             input_dim=best_config[\"input_dim\"],\n",
        "#             num_heads=best_config[\"num_heads\"],\n",
        "#             encoder_method=best_config[\"encoder_method\"],\n",
        "#             attn_vec_dim=best_config[\"attn_vec_dim\"],\n",
        "#             neighbor_recipe_limit=best_config[\"neighbor_recipe_limit\"],\n",
        "#             neighbor_ingredient_limit=best_config[\"neighbor_ingredient_limit\"],\n",
        "#             device=device\n",
        "#         )\n",
        "#         model.dropout = nn.Dropout(best_config[\"dropout\"])\n",
        "#         # Load the best weights saved during grid search\n",
        "#         model.load_state_dict(torch.load(\"best_model_weights.pth\"))\n",
        "#         print(\"Loaded best model weights from 'best_model_weights.pth'\")\n",
        "\n",
        "#         trainer = Trainer(model, data_loader, device=device, lr=best_config[\"lr\"], weight_decay=best_config[\"weight_decay\"])\n",
        "#     else:\n",
        "#         # Update the data loader's negative sampling ratio for the fixed configuration\n",
        "#         data_loader.neg_ratio = 3\n",
        "#         data_loader.refresh_negatives()\n",
        "\n",
        "#         model = MAGNNModel(\n",
        "#             num_user=data_loader.num_users,\n",
        "#             num_recipe=data_loader.num_recipes,\n",
        "#             user2recipes=data_loader.user2recipes,\n",
        "#             recipe2ingredients=data_loader.recipe2ingredients,\n",
        "#             recipe2users=data_loader.recipe2users,\n",
        "#             input_dim=64,\n",
        "#             num_heads=4,\n",
        "#             encoder_method=\"mean\",\n",
        "#             attn_vec_dim=128,\n",
        "#             neighbor_recipe_limit=30,   # updated hyperparameter\n",
        "#             neighbor_ingredient_limit=5,\n",
        "#             device=device\n",
        "#         )\n",
        "#         model.dropout = nn.Dropout(0.3)\n",
        "#         trainer = Trainer(model, data_loader, device=device, lr=0.005, weight_decay=1e-6)\n",
        "\n",
        "\n",
        "#     # 0.001, 0.05, 1e-4, 1e-5 #decresease lr\n",
        "\n",
        "#     # Training loop\n",
        "#     best_val_auc = 0.0\n",
        "#     patience = 30\n",
        "#     cur_patience = 0\n",
        "#     best_state = None\n",
        "#     max_epochs = 100 #10\n",
        "\n",
        "\n",
        "#     for ep in range(max_epochs):\n",
        "#         epoch_start = time.time()\n",
        "#         print(f\"\\nEpoch {ep} start:\")\n",
        "#         sys.stdout.flush()\n",
        "\n",
        "#         trn_loss = trainer.train_epoch(train_pos, train_neg, batch_size=(best_config[\"batch_size\"] if run_grid else 256))\n",
        "#         trainer.scheduler.step()  # Scheduler step added\n",
        "#         val_auc, val_ap = trainer.evaluate_auc_ap(val_pos, val_neg)\n",
        "#         epoch_time = time.time() - epoch_start\n",
        "#         print(f\"Epoch {ep} finished: Loss={trn_loss:.4f}, Val AUC={val_auc:.4f}, AP={val_ap:.4f}, Time={epoch_time:.2f}s\")\n",
        "#         sys.stdout.flush()\n",
        "\n",
        "#         if val_auc > best_val_auc:\n",
        "#             best_val_auc = val_auc\n",
        "#             cur_patience = 0\n",
        "#             best_state = {k: v.cpu() for k, v in model.state_dict().items()}\n",
        "#             checkpoint_path = f\"/content/drive/MyDrive/BaseMAGNN_checkpoint_epoch_{ep}.pt\"\n",
        "#             torch.save(best_state, checkpoint_path)\n",
        "#             print(f\"Checkpoint saved to {checkpoint_path}\")\n",
        "#             sys.stdout.flush()\n",
        "#         else:\n",
        "#             cur_patience += 1\n",
        "#             if cur_patience >= patience:\n",
        "#                 print(\"Early stopping triggered.\")\n",
        "#                 sys.stdout.flush()\n",
        "#                 break\n",
        "\n",
        "#     if best_state:\n",
        "#         model.load_state_dict(best_state)\n",
        "\n",
        "#     test_auc, test_ap = trainer.evaluate_auc_ap(test_pos, test_neg)\n",
        "#     print(f\"[Test Pairwise Classification] AUC={test_auc:.4f}, AP={test_ap:.4f}\")\n",
        "\n",
        "#     hr, ndcg, prec, map_ = trainer.evaluate_ranking_loo(test_pos, K=5, num_neg=100)\n",
        "#     print(f\"[Test Ranking LOO @5] HitRate={hr:.4f}, NDCG={ndcg:.4f}, Precision={prec:.4f}, MAP={map_:.4f}\")\n",
        "\n",
        "#     model.eval()\n",
        "#     user_ids = torch.tensor([0, 1, 2], device=device)\n",
        "#     recipe_ids = torch.tensor([10, 20, 30], device=device)\n",
        "#     scores = model.forward(user_ids, recipe_ids)\n",
        "#     print(\"Sample predictions:\", scores)\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     main()"
      ]
    }
  ]
}